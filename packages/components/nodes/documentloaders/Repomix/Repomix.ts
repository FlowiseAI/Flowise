import { omit } from 'lodash'
import { ICommonObject, IDocument, INode, INodeData, INodeOutputsValue, INodeParams } from '../../../src/Interface'
import { TextSplitter } from 'langchain/text_splitter'
import { TextLoader } from 'langchain/document_loaders/fs/text'
import { getFileFromStorage, handleEscapeCharacters } from '../../../src'
import { Document } from '@langchain/core/documents'

class Repomix_DocumentLoaders implements INode {
    label: string
    name: string
    version: number
    description: string
    type: string
    icon: string
    category: string
    baseClasses: string[]
    inputs: INodeParams[]
    outputs: INodeOutputsValue[]

    constructor() {
        this.label = 'Repomix'
        this.name = 'repomix'
        this.version = 1.0
        this.type = 'Document'
        this.icon = 'cube.svg'
        this.category = 'Document Loaders'
        this.description = `Load and process documentation from GitHub repositories or Repomix XML files with cleanup and URL generation`
        this.baseClasses = [this.type]
        this.inputs = [
            {
                label: 'GitHub URLs',
                name: 'githubUrls',
                type: 'string',
                description: 'Comma-separated GitHub URLs (public repositories only). Can be files or directories.',
                placeholder: 'https://github.com/owner/repo/blob/main/docs/file.md, https://github.com/owner/repo/tree/main/docs',
                optional: true
            },
            {
                label: 'XML File',
                name: 'xmlFile',
                type: 'file',
                fileType: '.xml',
                description: 'Upload XML files generated by Repomix (alternative to GitHub URLs)',
                optional: true
            },
            {
                label: 'URL Prefix',
                name: 'urlPrefix',
                type: 'string',
                description: 'Optional URL prefix for generating public URLs (e.g., https://answeragent.ai/docs)',
                placeholder: 'https://answeragent.ai/docs',
                optional: true,
                additionalParams: true
            },
            {
                label: 'Extensions to Remove',
                name: 'extensionsToRemove',
                type: 'string',
                description: 'Comma-separated file extensions to remove from public URLs',
                default: '.md,.tsx,.mdx',
                optional: true,
                additionalParams: true
            },
            {
                label: 'Clean Content',
                name: 'cleanContent',
                type: 'boolean',
                description: 'Enable content cleaning to remove unwanted patterns',
                default: false,
                optional: true,
                additionalParams: true
            },
            {
                label: 'Exclusion Patterns',
                name: 'exclusionPatterns',
                type: 'string',
                description: 'Regex patterns to exclude from content (one per line). Default removes lines starting with "api:"',
                placeholder: '^\\s*api:\\s*\n^\\s*debug:\\s*',
                rows: 4,
                default: '^\\s*api:\\s*',
                optional: true,
                additionalParams: true
            },
            {
                label: 'Text Splitter',
                name: 'textSplitter',
                type: 'TextSplitter',
                optional: true
            },
            {
                label: 'Additional Metadata',
                name: 'metadata',
                type: 'json',
                description: 'Additional metadata to be added to the extracted documents',
                optional: true,
                additionalParams: true
            },
            {
                label: 'Omit Metadata Keys',
                name: 'omitMetadataKeys',
                type: 'string',
                rows: 4,
                description:
                    'Each document loader comes with a default set of metadata keys that are extracted from the document. You can use this field to omit some of the default metadata keys. The value should be a list of keys, seperated by comma. Use * to omit all metadata keys execept the ones you specify in the Additional Metadata field',
                placeholder: 'key1, key2, key3.nestedKey1',
                optional: true,
                additionalParams: true
            }
        ]
        this.outputs = [
            {
                label: 'Document',
                name: 'document',
                description: 'Array of document objects containing metadata and pageContent',
                baseClasses: [...this.baseClasses, 'json']
            },
            {
                label: 'Text',
                name: 'text',
                description: 'Concatenated string from pageContent of documents',
                baseClasses: ['string', 'json']
            }
        ]
    }

    async init(nodeData: INodeData, _: string, options: ICommonObject): Promise<any> {
        const githubUrls = nodeData.inputs?.githubUrls as string
        const xmlFileBase64 = nodeData.inputs?.xmlFile as string
        const urlPrefix = nodeData.inputs?.urlPrefix as string
        const extensionsToRemove = (nodeData.inputs?.extensionsToRemove as string) || '.md,.tsx,.mdx'
        const cleanContent = nodeData.inputs?.cleanContent as boolean
        const exclusionPatterns = (nodeData.inputs?.exclusionPatterns as string) || '^\\s*api:\\s*'
        const textSplitter = nodeData.inputs?.textSplitter as TextSplitter
        const metadata = nodeData.inputs?.metadata
        const output = nodeData.outputs?.output as string
        const _omitMetadataKeys = nodeData.inputs?.omitMetadataKeys as string

        let omitMetadataKeys: string[] = []
        if (_omitMetadataKeys) {
            omitMetadataKeys = _omitMetadataKeys.split(',').map((key) => key.trim())
        }

        // Parse extensions to remove
        const extensionsArray = extensionsToRemove.split(',').map((ext) => ext.trim())

        // Parse exclusion patterns for content cleaning
        const patterns = exclusionPatterns
            .split('\n')
            .map((pattern) => pattern.trim())
            .filter((pattern) => pattern)

        let docs: IDocument[] = []

        // Validate that either GitHub URLs or XML file is provided, but not both
        if (!githubUrls && !xmlFileBase64) {
            throw new Error('Either GitHub URLs or XML file must be provided')
        }
        if (githubUrls && xmlFileBase64) {
            throw new Error('Provide either GitHub URLs or XML file, not both')
        }

        if (githubUrls) {
            // Handle GitHub URLs
            docs = await this.processGitHubUrls(githubUrls, urlPrefix, extensionsArray, cleanContent, patterns, textSplitter)
        } else if (xmlFileBase64) {
            // Handle XML file upload
            docs = await this.processXmlFile(xmlFileBase64, urlPrefix, extensionsArray, cleanContent, patterns, textSplitter, options)
        }

        // Apply additional metadata and omit keys
        if (metadata) {
            const parsedMetadata = typeof metadata === 'object' ? metadata : JSON.parse(metadata)
            docs = docs.map((doc) => ({
                ...doc,
                metadata:
                    _omitMetadataKeys === '*'
                        ? {
                              ...parsedMetadata
                          }
                        : omit(
                              {
                                  ...doc.metadata,
                                  ...parsedMetadata
                              },
                              omitMetadataKeys
                          )
            }))
        } else {
            docs = docs.map((doc) => ({
                ...doc,
                metadata:
                    _omitMetadataKeys === '*'
                        ? {}
                        : omit(
                              {
                                  ...doc.metadata
                              },
                              omitMetadataKeys
                          )
            }))
        }

        if (output === 'document') {
            return docs
        } else {
            let finaltext = ''
            for (const doc of docs) {
                finaltext += `${doc.pageContent}\n`
            }
            return handleEscapeCharacters(finaltext, false)
        }
    }

    private async processGitHubUrls(
        githubUrls: string,
        urlPrefix: string,
        extensionsArray: string[],
        cleanContent: boolean,
        patterns: string[],
        textSplitter?: TextSplitter
    ): Promise<IDocument[]> {
        const urls = githubUrls
            .split(',')
            .map((url) => url.trim())
            .filter((url) => url)
        const docs: IDocument[] = []

        for (const url of urls) {
            const urlDocs = await this.fetchFromGitHub(url, urlPrefix, extensionsArray, cleanContent, patterns)

            if (textSplitter) {
                const splittedDocs = await textSplitter.splitDocuments(urlDocs)
                docs.push(...splittedDocs)
            } else {
                docs.push(...urlDocs)
            }
        }

        return docs
    }

    private async processXmlFile(
        xmlFileBase64: string,
        urlPrefix: string,
        extensionsArray: string[],
        cleanContent: boolean,
        patterns: string[],
        textSplitter?: TextSplitter,
        options?: ICommonObject
    ): Promise<IDocument[]> {
        let docs: IDocument[] = []
        let files: string[] = []

        // Handle file storage vs direct upload
        if (xmlFileBase64.startsWith('FILE-STORAGE::')) {
            const fileName = xmlFileBase64.replace('FILE-STORAGE::', '')
            if (fileName.startsWith('[') && fileName.endsWith(']')) {
                files = JSON.parse(fileName)
            } else {
                files = [fileName]
            }
            const chatflowid = options?.chatflowid

            for (const file of files) {
                if (!file) continue
                const fileData = await getFileFromStorage(file, chatflowid)
                const xmlContent = fileData.toString()
                const xmlDocs = await this.parseXmlContent(xmlContent, urlPrefix, extensionsArray, cleanContent, patterns)
                docs.push(...xmlDocs)
            }
        } else {
            if (xmlFileBase64.startsWith('[') && xmlFileBase64.endsWith(']')) {
                files = JSON.parse(xmlFileBase64)
            } else {
                files = [xmlFileBase64]
            }

            for (const file of files) {
                if (!file) continue
                const splitDataURI = file.split(',')
                splitDataURI.pop()
                const bf = Buffer.from(splitDataURI.pop() || '', 'base64')
                const xmlContent = bf.toString()
                const xmlDocs = await this.parseXmlContent(xmlContent, urlPrefix, extensionsArray, cleanContent, patterns)
                docs.push(...xmlDocs)
            }
        }

        // Apply text splitting if provided
        if (textSplitter && docs.length > 0) {
            docs = await textSplitter.splitDocuments(docs)
        }

        return docs
    }

    private async fetchFromGitHub(
        url: string,
        urlPrefix: string,
        extensionsArray: string[],
        cleanContent: boolean,
        patterns: string[]
    ): Promise<IDocument[]> {
        // Parse GitHub URL to get raw content URL
        const rawUrl = this.convertToRawUrl(url)

        try {
            const response = await fetch(rawUrl)
            if (!response.ok) {
                throw new Error(`Failed to fetch ${url}: ${response.statusText}`)
            }

            let content = await response.text()

            // Apply content cleaning if enabled
            if (cleanContent) {
                content = this.cleanContent(content, patterns)
            }

            const path = this.extractPathFromGitHubUrl(url)
            const publicUrl = this.generatePublicUrl(path, urlPrefix, extensionsArray)

            const doc = new Document({
                pageContent: content,
                metadata: {
                    githubUrl: url,
                    originalPath: path,
                    publicUrl: publicUrl || undefined,
                    cleaned: cleanContent
                }
            })

            return [doc]
        } catch (error) {
            throw new Error(`Error fetching GitHub content from ${url}: ${error}`)
        }
    }

    private parseXmlContent(
        xmlContent: string,
        urlPrefix: string,
        extensionsArray: string[],
        cleanContent: boolean,
        patterns: string[]
    ): IDocument[] {
        const docs: IDocument[] = []

        // Parse XML content to extract individual files
        const fileRegex = /<file path="([^"]+)">([\s\S]*?)<\/file>/g
        let match

        while ((match = fileRegex.exec(xmlContent)) !== null) {
            const path = match[1]
            let content = match[2].trim()

            // Apply content cleaning if enabled
            if (cleanContent) {
                content = this.cleanContent(content, patterns)
            }

            const publicUrl = this.generatePublicUrl(path, urlPrefix, extensionsArray)

            const doc = new Document({
                pageContent: content,
                metadata: {
                    originalPath: path,
                    publicUrl: publicUrl || undefined,
                    source: 'xml_upload',
                    cleaned: cleanContent
                }
            })

            docs.push(doc)
        }

        return docs
    }

    private cleanContent(content: string, patterns: string[]): string {
        if (!patterns.length) return content

        const lines = content.split(/\r?\n/)
        let cleanedLines = lines

        for (const pattern of patterns) {
            try {
                const regex = new RegExp(pattern)
                cleanedLines = cleanedLines.filter((line) => !regex.test(line))
            } catch (error) {
                console.warn(`Invalid regex pattern: ${pattern}`, error)
            }
        }

        return cleanedLines.join('\n')
    }

    private convertToRawUrl(githubUrl: string): string {
        // Convert GitHub URL to raw content URL
        if (githubUrl.includes('/blob/')) {
            return githubUrl.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')
        } else if (githubUrl.includes('/tree/')) {
            // For directory URLs, we need to handle this differently
            // For now, throw an error asking for individual file URLs
            throw new Error('Directory URLs are not yet supported. Please provide individual file URLs.')
        }
        return githubUrl
    }

    private extractPathFromGitHubUrl(githubUrl: string): string {
        // Extract file path from GitHub URL
        const match = githubUrl.match(/\/blob\/[^/]+\/(.+)$/)
        return match ? match[1] : githubUrl.split('/').pop() || ''
    }

    private generatePublicUrl(path: string, urlPrefix: string, extensionsArray: string[]): string | null {
        if (!urlPrefix) return null

        let cleanPath = path

        // Remove file extensions if specified
        // Sort extensions by length (longest first) to handle compound extensions like .api.mdx
        const sortedExtensions = [...extensionsArray].sort((a, b) => b.length - a.length)
        for (const ext of sortedExtensions) {
            if (cleanPath.endsWith(ext)) {
                cleanPath = cleanPath.slice(0, -ext.length)
                break
            }
        }

        // Ensure urlPrefix doesn't end with slash and path doesn't start with slash
        const cleanPrefix = urlPrefix.endsWith('/') ? urlPrefix.slice(0, -1) : urlPrefix
        const cleanPathStart = cleanPath.startsWith('/') ? cleanPath.slice(1) : cleanPath

        return `${cleanPrefix}/${cleanPathStart}`
    }
}

module.exports = { nodeClass: Repomix_DocumentLoaders }
