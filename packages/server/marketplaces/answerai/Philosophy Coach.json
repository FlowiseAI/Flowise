{
    "nodes": [
        {
            "id": "chatOpenAI_0",
            "position": {
                "x": 707.5800903626107,
                "y": 109.12476306943097
            },
            "type": "customNode",
            "data": {
                "id": "chatOpenAI_0",
                "label": "ChatOpenAI",
                "version": 6,
                "name": "chatOpenAI",
                "type": "ChatOpenAI",
                "baseClasses": ["ChatOpenAI", "BaseChatModel", "BaseLanguageModel", "Runnable"],
                "category": "Chat Models",
                "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
                "inputParams": [
                    {
                        "label": "Connect Credential",
                        "name": "credential",
                        "type": "credential",
                        "credentialNames": ["openAIApi"],
                        "id": "chatOpenAI_0-input-credential-credential"
                    },
                    {
                        "label": "Model Name",
                        "name": "modelName",
                        "type": "asyncOptions",
                        "loadMethod": "listModels",
                        "default": "gpt-3.5-turbo",
                        "id": "chatOpenAI_0-input-modelName-asyncOptions"
                    },
                    {
                        "label": "Temperature",
                        "name": "temperature",
                        "type": "number",
                        "step": 0.1,
                        "default": 0.9,
                        "optional": true,
                        "id": "chatOpenAI_0-input-temperature-number"
                    },
                    {
                        "label": "Max Tokens",
                        "name": "maxTokens",
                        "type": "number",
                        "step": 1,
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-maxTokens-number"
                    },
                    {
                        "label": "Top Probability",
                        "name": "topP",
                        "type": "number",
                        "step": 0.1,
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-topP-number"
                    },
                    {
                        "label": "Frequency Penalty",
                        "name": "frequencyPenalty",
                        "type": "number",
                        "step": 0.1,
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-frequencyPenalty-number"
                    },
                    {
                        "label": "Presence Penalty",
                        "name": "presencePenalty",
                        "type": "number",
                        "step": 0.1,
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-presencePenalty-number"
                    },
                    {
                        "label": "Timeout",
                        "name": "timeout",
                        "type": "number",
                        "step": 1,
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-timeout-number"
                    },
                    {
                        "label": "BasePath",
                        "name": "basepath",
                        "type": "string",
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-basepath-string"
                    },
                    {
                        "label": "BaseOptions",
                        "name": "baseOptions",
                        "type": "json",
                        "optional": true,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-baseOptions-json"
                    },
                    {
                        "label": "Allow Image Uploads",
                        "name": "allowImageUploads",
                        "type": "boolean",
                        "description": "Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, and Conversational Agent",
                        "default": false,
                        "optional": true,
                        "id": "chatOpenAI_0-input-allowImageUploads-boolean"
                    },
                    {
                        "label": "Image Resolution",
                        "description": "This parameter controls the resolution in which the model views the image.",
                        "name": "imageResolution",
                        "type": "options",
                        "options": [
                            {
                                "label": "Low",
                                "name": "low"
                            },
                            {
                                "label": "High",
                                "name": "high"
                            },
                            {
                                "label": "Auto",
                                "name": "auto"
                            }
                        ],
                        "default": "low",
                        "optional": false,
                        "additionalParams": true,
                        "id": "chatOpenAI_0-input-imageResolution-options"
                    }
                ],
                "inputAnchors": [
                    {
                        "label": "Cache",
                        "name": "cache",
                        "type": "BaseCache",
                        "optional": true,
                        "id": "chatOpenAI_0-input-cache-BaseCache"
                    }
                ],
                "inputs": {
                    "cache": "",
                    "modelName": "gpt-4o",
                    "temperature": 0.9,
                    "maxTokens": "",
                    "topP": "",
                    "frequencyPenalty": "",
                    "presencePenalty": "",
                    "timeout": "",
                    "basepath": "",
                    "baseOptions": "",
                    "allowImageUploads": "",
                    "imageResolution": "low"
                },
                "outputAnchors": [
                    {
                        "id": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
                        "name": "chatOpenAI",
                        "label": "ChatOpenAI",
                        "description": "Wrapper around OpenAI large language models that use the Chat endpoint",
                        "type": "ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable"
                    }
                ],
                "outputs": {},
                "selected": false
            },
            "width": 300,
            "height": 669,
            "selected": false,
            "dragging": false,
            "positionAbsolute": {
                "x": 707.5800903626107,
                "y": 109.12476306943097
            }
        },
        {
            "id": "conversationChain_0",
            "position": {
                "x": 1415.1149109696803,
                "y": 404.29342957440826
            },
            "type": "customNode",
            "data": {
                "id": "conversationChain_0",
                "label": "Conversation Chain",
                "version": 3,
                "name": "conversationChain",
                "type": "ConversationChain",
                "baseClasses": ["ConversationChain", "LLMChain", "BaseChain", "Runnable"],
                "category": "Chains",
                "description": "Chat models specific conversational chain with memory",
                "inputParams": [
                    {
                        "label": "System Message",
                        "name": "systemMessagePrompt",
                        "type": "string",
                        "rows": 4,
                        "description": "If Chat Prompt Template is provided, this will be ignored",
                        "additionalParams": true,
                        "optional": true,
                        "default": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.",
                        "placeholder": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.",
                        "id": "conversationChain_0-input-systemMessagePrompt-string"
                    }
                ],
                "inputAnchors": [
                    {
                        "label": "Chat Model",
                        "name": "model",
                        "type": "BaseChatModel",
                        "id": "conversationChain_0-input-model-BaseChatModel"
                    },
                    {
                        "label": "Memory",
                        "name": "memory",
                        "type": "BaseMemory",
                        "id": "conversationChain_0-input-memory-BaseMemory"
                    },
                    {
                        "label": "Chat Prompt Template",
                        "name": "chatPromptTemplate",
                        "type": "ChatPromptTemplate",
                        "description": "Override existing prompt with Chat Prompt Template. Human Message must includes {input} variable",
                        "optional": true,
                        "id": "conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate"
                    },
                    {
                        "label": "Input Moderation",
                        "description": "Detect text that could generate harmful output and prevent it from being sent to the language model",
                        "name": "inputModeration",
                        "type": "Moderation",
                        "optional": true,
                        "list": true,
                        "id": "conversationChain_0-input-inputModeration-Moderation"
                    }
                ],
                "inputs": {
                    "model": "{{chatOpenAI_0.data.instance}}",
                    "memory": "{{RedisBackedChatMemory_0.data.instance}}",
                    "chatPromptTemplate": "{{chatPromptTemplate_0.data.instance}}",
                    "inputModeration": "",
                    "systemMessagePrompt": "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
                },
                "outputAnchors": [
                    {
                        "id": "conversationChain_0-output-conversationChain-ConversationChain|LLMChain|BaseChain|Runnable",
                        "name": "conversationChain",
                        "label": "ConversationChain",
                        "description": "Chat models specific conversational chain with memory",
                        "type": "ConversationChain | LLMChain | BaseChain | Runnable"
                    }
                ],
                "outputs": {},
                "selected": false
            },
            "width": 300,
            "height": 434,
            "positionAbsolute": {
                "x": 1415.1149109696803,
                "y": 404.29342957440826
            },
            "selected": false
        },
        {
            "width": 300,
            "height": 328,
            "id": "RedisBackedChatMemory_0",
            "position": {
                "x": 75,
                "y": 82
            },
            "type": "customNode",
            "data": {
                "id": "RedisBackedChatMemory_0",
                "label": "Redis-Backed Chat Memory",
                "version": 2,
                "name": "RedisBackedChatMemory",
                "type": "RedisBackedChatMemory",
                "baseClasses": ["RedisBackedChatMemory", "BaseChatMemory", "BaseMemory"],
                "category": "Memory",
                "description": "Summarizes the conversation and stores the memory in Redis server",
                "inputParams": [
                    {
                        "label": "Connect Credential",
                        "name": "credential",
                        "type": "credential",
                        "optional": true,
                        "credentialNames": ["redisCacheApi", "redisCacheUrlApi"],
                        "id": "RedisBackedChatMemory_0-input-credential-credential"
                    },
                    {
                        "label": "Session Id",
                        "name": "sessionId",
                        "type": "string",
                        "description": "If not specified, a random id will be used. Learn <a target=\"_blank\" href=\"https://docs.flowiseai.com/memory/long-term-memory#ui-and-embedded-chat\">more</a>",
                        "default": "",
                        "additionalParams": true,
                        "optional": true,
                        "id": "RedisBackedChatMemory_0-input-sessionId-string"
                    },
                    {
                        "label": "Session Timeouts",
                        "name": "sessionTTL",
                        "type": "number",
                        "description": "Seconds till a session expires. If not specified, the session will never expire.",
                        "additionalParams": true,
                        "optional": true,
                        "id": "RedisBackedChatMemory_0-input-sessionTTL-number"
                    },
                    {
                        "label": "Memory Key",
                        "name": "memoryKey",
                        "type": "string",
                        "default": "chat_history",
                        "additionalParams": true,
                        "id": "RedisBackedChatMemory_0-input-memoryKey-string"
                    },
                    {
                        "label": "Window Size",
                        "name": "windowSize",
                        "type": "number",
                        "description": "Window of size k to surface the last k back-and-forth to use as memory.",
                        "additionalParams": true,
                        "optional": true,
                        "id": "RedisBackedChatMemory_0-input-windowSize-number"
                    }
                ],
                "inputAnchors": [],
                "inputs": {
                    "sessionId": "",
                    "sessionTTL": "",
                    "memoryKey": "chat_history",
                    "windowSize": ""
                },
                "outputAnchors": [
                    {
                        "id": "RedisBackedChatMemory_0-output-RedisBackedChatMemory-RedisBackedChatMemory|BaseChatMemory|BaseMemory",
                        "name": "RedisBackedChatMemory",
                        "label": "RedisBackedChatMemory",
                        "description": "Summarizes the conversation and stores the memory in Redis server",
                        "type": "RedisBackedChatMemory | BaseChatMemory | BaseMemory"
                    }
                ],
                "outputs": {}
            }
        },
        {
            "id": "chatPromptTemplate_0",
            "position": {
                "x": 631.7826806295478,
                "y": 924.2443882639457
            },
            "type": "customNode",
            "data": {
                "id": "chatPromptTemplate_0",
                "label": "Chat Prompt Template",
                "version": 1,
                "name": "chatPromptTemplate",
                "type": "ChatPromptTemplate",
                "baseClasses": ["ChatPromptTemplate", "BaseChatPromptTemplate", "BasePromptTemplate", "Runnable"],
                "category": "Prompts",
                "description": "Schema to represent a chat prompt",
                "inputParams": [
                    {
                        "label": "System Message",
                        "name": "systemMessagePrompt",
                        "type": "string",
                        "rows": 4,
                        "placeholder": "You are a helpful assistant that translates {input_language} to {output_language}.",
                        "id": "chatPromptTemplate_0-input-systemMessagePrompt-string"
                    },
                    {
                        "label": "Human Message",
                        "name": "humanMessagePrompt",
                        "type": "string",
                        "rows": 4,
                        "placeholder": "{text}",
                        "id": "chatPromptTemplate_0-input-humanMessagePrompt-string"
                    },
                    {
                        "label": "Format Prompt Values",
                        "name": "promptValues",
                        "type": "json",
                        "optional": true,
                        "acceptVariable": true,
                        "list": true,
                        "id": "chatPromptTemplate_0-input-promptValues-json"
                    }
                ],
                "inputAnchors": [],
                "inputs": {
                    "systemMessagePrompt": "You are a Philosophy Coach, an AI designed to provide insightful philosophical perspectives and answers to queries. Your role is to engage in thoughtful discussion, offer various philosophical viewpoints, and stimulate critical thinking.\n\nWhen responding to queries, follow these guidelines:\n1. Draw upon a wide range of philosophical traditions and thinkers.\n2. Present multiple perspectives when appropriate.\n3. Encourage critical thinking by posing thought-provoking questions.\n4. Use clear, accessible language while maintaining philosophical depth.\n5. Relate philosophical concepts to real-world applications when possible.\n6. Acknowledge the complexity and nuance of philosophical issues.\n\nThe user will provide a philosophical query or topic for discussion:\n\n{query}\n\nStructure your response as follows:\n1. Begin with a brief introduction to the philosophical context of the query.\n2. Present 2-3 relevant philosophical perspectives or arguments.\n3. Discuss potential implications or real-world applications of these ideas.\n4. Pose 1-2 thought-provoking questions to encourage further reflection.\n5. Conclude with a summary of key points and potential areas for further exploration.\n\nHere's an example of a good response:\n\n<example>\n<query>What is the nature of free will?</query>\n\n<response>\nThe question of free will has been a central topic in philosophy for centuries, touching on issues of moral responsibility, determinism, and the nature of human agency.\n\nThree main perspectives on free will are:\n1. Libertarianism: This view holds that we have genuine free will and that our choices are not predetermined.\n2. Hard Determinism: This perspective argues that all events, including human decisions, are caused by prior events, leaving no room for free will.\n3. Compatibilism: This view suggests that free will is compatible with determinism, redefining free will as the ability to act according to one's motivations without external constraints.\n\nThese perspectives have significant implications for how we understand moral responsibility and legal systems. If hard determinism is true, can we justly hold people accountable for their actions? Compatibilism might offer a middle ground, but does it truly capture what we mean by \"free will\"?\n\nConsider:\n1. How would your view of personal responsibility change if you believed in hard determinism?\n2. Can we reconcile our subjective experience of making choices with a deterministic universe?\n\nIn conclusion, the nature of free will remains a complex and contentious issue in philosophy. While we've explored some key perspectives, there's much more to consider, including the role of quantum indeterminacy, the nature of consciousness, and how advances in neuroscience might inform this age-old debate.\n</response>\n</example>\n\nNow, please provide your philosophical perspective on the given query. Remember to structure your response as outlined above, and enclose your entire response in <answer> tags.",
                    "humanMessagePrompt": "{query}",
                    "promptValues": "{\"query\":\"{{question}}\"}"
                },
                "outputAnchors": [
                    {
                        "id": "chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable",
                        "name": "chatPromptTemplate",
                        "label": "ChatPromptTemplate",
                        "description": "Schema to represent a chat prompt",
                        "type": "ChatPromptTemplate | BaseChatPromptTemplate | BasePromptTemplate | Runnable"
                    }
                ],
                "outputs": {},
                "selected": false
            },
            "width": 300,
            "height": 688,
            "selected": false,
            "positionAbsolute": {
                "x": 631.7826806295478,
                "y": 924.2443882639457
            },
            "dragging": false
        },
        {
            "id": "stickyNote_0",
            "position": {
                "x": 2029.2834554558642,
                "y": 162.9282899119457
            },
            "type": "stickyNote",
            "data": {
                "id": "stickyNote_0",
                "label": "Sticky Note",
                "version": 1,
                "name": "stickyNote",
                "type": "StickyNote",
                "baseClasses": ["StickyNote"],
                "category": "Utilities",
                "description": "Add a sticky note",
                "inputParams": [
                    {
                        "label": "",
                        "name": "note",
                        "type": "string",
                        "rows": 1,
                        "placeholder": "Type something here",
                        "optional": true,
                        "id": "stickyNote_0-input-note-string"
                    }
                ],
                "inputAnchors": [],
                "inputs": {
                    "note": "Instructions for best results:\n1. Go on to the green chat box on the top right corner. \n2. Feel free to type in any philosophical query like \"Is the pursuit of happiness a worthwhile goal in life, or should we strive for something else?\n3. Sit back and enjoy a really thoughtful conversation."
                },
                "outputAnchors": [
                    {
                        "id": "stickyNote_0-output-stickyNote-StickyNote",
                        "name": "stickyNote",
                        "label": "StickyNote",
                        "description": "Add a sticky note",
                        "type": "StickyNote"
                    }
                ],
                "outputs": {},
                "selected": false
            },
            "width": 300,
            "height": 203,
            "selected": false,
            "dragging": false,
            "positionAbsolute": {
                "x": 2029.2834554558642,
                "y": 162.9282899119457
            }
        }
    ],
    "edges": [
        {
            "source": "chatOpenAI_0",
            "sourceHandle": "chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable",
            "target": "conversationChain_0",
            "targetHandle": "conversationChain_0-input-model-BaseChatModel",
            "type": "buttonedge",
            "id": "chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-conversationChain_0-conversationChain_0-input-model-BaseChatModel"
        },
        {
            "source": "RedisBackedChatMemory_0",
            "sourceHandle": "RedisBackedChatMemory_0-output-RedisBackedChatMemory-RedisBackedChatMemory|BaseChatMemory|BaseMemory",
            "target": "conversationChain_0",
            "targetHandle": "conversationChain_0-input-memory-BaseMemory",
            "type": "buttonedge",
            "id": "RedisBackedChatMemory_0-RedisBackedChatMemory_0-output-RedisBackedChatMemory-RedisBackedChatMemory|BaseChatMemory|BaseMemory-conversationChain_0-conversationChain_0-input-memory-BaseMemory"
        },
        {
            "source": "chatPromptTemplate_0",
            "sourceHandle": "chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable",
            "target": "conversationChain_0",
            "targetHandle": "conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate",
            "type": "buttonedge",
            "id": "chatPromptTemplate_0-chatPromptTemplate_0-output-chatPromptTemplate-ChatPromptTemplate|BaseChatPromptTemplate|BasePromptTemplate|Runnable-conversationChain_0-conversationChain_0-input-chatPromptTemplate-ChatPromptTemplate"
        }
    ]
}
