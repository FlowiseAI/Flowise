This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<directory_structure>
api/
  assistants/
    assistants-api.info.mdx
    create-assistant.api.mdx
    delete-assistant.api.mdx
    get-assistant-by-id.api.mdx
    list-assistants.api.mdx
    sidebar.ts
    update-assistant.api.mdx
  attachments/
    attachments-api.info.mdx
    create-attachment.api.mdx
    delete-attachment.api.mdx
    download-attachment-content.api.mdx
    get-attachment.api.mdx
    sidebar.ts
  chat-message/
    chat-message-api.info.mdx
    get-all-chat-messages.api.mdx
    remove-all-chat-messages.api.mdx
    sidebar.ts
  chatflows/
    chatflows-api.info.mdx
    create-chatflow.api.mdx
    delete-chatflow.api.mdx
    get-chatflow-by-api-key.api.mdx
    get-chatflow-by-id.api.mdx
    list-chatflows.api.mdx
    sidebar.ts
    update-chatflow.api.mdx
  document-store/
    create-document-store.api.mdx
    delete-document-store.api.mdx
    delete-file-chunk.api.mdx
    delete-loader-from-document-store.api.mdx
    delete-vector-store-data.api.mdx
    document-store-api.info.mdx
    get-all-document-stores.api.mdx
    get-document-store-by-id.api.mdx
    get-file-chunks.api.mdx
    preview-chunking.api.mdx
    process-chunking.api.mdx
    query-vector-store.api.mdx
    refresh-document.api.mdx
    sidebar.ts
    update-document-store.api.mdx
    update-file-chunk.api.mdx
    upsert-document.api.mdx
  feedback/
    create-chat-message-feedback-for-chatflow.api.mdx
    feedback-api.info.mdx
    get-all-chat-message-feedback.api.mdx
    sidebar.ts
    update-chat-message-feedback-for-chatflow.api.mdx
  leads/
    create-lead.api.mdx
    get-all-leads-for-chatflow.api.mdx
    leads-api.info.mdx
    sidebar.ts
  ping/
    ping-api.info.mdx
    ping-server.api.mdx
    sidebar.ts
  prediction/
    create-prediction.api.mdx
    prediction-api.info.mdx
    sidebar.ts
  tools/
    create-tool.api.mdx
    delete-tool.api.mdx
    get-all-tools.api.mdx
    get-tool-by-id.api.mdx
    sidebar.ts
    tools-api.info.mdx
    update-tool.api.mdx
  upsert-history/
    get-all-upsert-history.api.mdx
    patch-delete-upsert-history.api.mdx
    sidebar.ts
    upsert-history-api.info.mdx
  variables/
    create-variable.api.mdx
    delete-variable.api.mdx
    get-all-variables.api.mdx
    sidebar.ts
    update-variable.api.mdx
    variables-api.info.mdx
  vector-upsert/
    sidebar.ts
    vector-upsert-api.info.mdx
    vector-upsert.api.mdx
  full-api-spec.mdx
  index.mdx
browser/
  README.md
chat/
  README.md
community/
  getting-help.md
  README.md
developers/
  authorization/
    app-level.md
    chatflow-level.md
    google-oauth.md
    README.md
  deployment/
    aws.md
    azure.md
    gcp.md
    README.md
    render.md
  use-cases/
    calling-children-flows.md
    interacting-with-api.md
    multiple-documents-qna.md
    README.md
    sql-qna.md
    upserting-data.md
    web-scrape-qna.md
    webhook-tool.md
  building-node.md
  contributing.md
  databases.md
  earn-credits.md
  embed.md
  environment-variables.md
  README.md
  running-locally.md
  video-guide.md
integrations/
  README.md
  zapier-zaps.md
postlaunch/
  advanced-topics/
    best-practices-for-sidekick-design.md
    customization-and-extensibility.md
  analytic.md
  troubleshooting-faqs.md
sidekick-studio/
  agentflows/
    sequential-agents/
      agent-memory-node.md
      agent-node.md
      condition-agent-node.md
      condition-node.md
      end-node.md
      llm-node.md
      loop-node.md
      README.md
      start-node.md
      state-node.md
      tool-node.md
    multi-agents.md
    README.md
  agentflowsv2/
    README.md
  chatflows/
    agents/
      openai-assistant/
        README.md
        threads.md
      README.md
      tool-agent.md
    cache/
      in-memory-cache.md
      inmemory-embedding-cache.md
      momento-cache.md
      README.md
      redis-cache.md
      redis-embeddings-cache.md
      upstash-redis-cache.md
    chains/
      conversation-chain.md
      conversational-retrieval-qa-chain.md
      get-api-chain.md
      llm-chain.md
      multi-prompt-chain.md
      multi-retrieval-qa-chain.md
      openapi-chain.md
      post-api-chain.md
      README.md
      retrieval-qa-chain.md
      sql-database-chain.md
      vectara-chain.md
      vectordb-qa-chain.md
    chat-models/
      aws-chatbedrock.md
      azure-chatopenai.md
      chatanthropic.md
      chatcohere.md
      chathuggingface.md
      chatlocalai.md
      chatopenai-custom.md
      chatopenai.md
      google-ai.md
      google-vertexai.md
      groqchat.md
      mistral-ai.md
      README.md
    document-loaders/
      airtable.md
      api-loader.md
      apify-website-content-crawler.md
      cheerio-web-scraper.md
      confluence.md
      contentful.md
      csv-file.md
      custom-document-loader.md
      document-store.md
      docx-file.md
      figma.md
      folder-with-files.md
      gitbook.md
      github.md
      gmail.md
      google-drive.md
      json-file.md
      json-lines-file.md
      notion-database.md
      notion-folder.md
      notion-page.md
      pdf-file.md
      plain-text.md
      playwright-web-scraper.md
      puppeteer-web-scraper.md
      README.md
      s3-file-loader.md
      searchapi-for-web-search.md
      serpapi-for-web-search.md
      text-file.md
      unstructured-file-loader.md
      unstructured-folder-loader.md
      vectorstore-to-document.md
    embeddings/
      aws-bedrock-embeddings.md
      azure-openai-embeddings.md
      cohere-embeddings.md
      googlegenerativeai-embeddings.md
      googlevertexai-embeddings.md
      huggingface-inference-embeddings.md
      localai-embeddings.md
      mistralai-embeddings.md
      openai-embeddings-custom.md
      openai-embeddings.md
      README.md
      togetherai-embedding.md
      voyageai-embeddings.md
    memory/
      buffer-memory.md
      buffer-window-memory.md
      conversation-summary-buffer-memory.md
      conversation-summary-memory.md
      dynamodb-chat-memory.md
      mongodb-atlas-chat-memory.md
      README.md
      redis-backed-chat-memory.md
      upstash-redis-backed-chat-memory.md
      zep-memory.md
    moderation/
      openai-moderation.md
      README.md
      simple-prompt-moderation.md
    output-parsers/
      advanced-structured-output-parser.md
      csv-output-parser.md
      custom-list-output-parser.md
      README.md
      structured-output-parser.md
    prompts/
      chat-prompt-template.md
      few-shot-prompt-template.md
      prompt-template.md
      README.md
    retrievers/
      cohere-rerank-retriever.md
      embeddings-filter-retriever.md
      hyde-retriever.md
      llm-filter-retriever.md
      prompt-retriever.md
      README.md
      reciprocal-rank-fusion-retriever.md
      similarity-score-threshold-retriever.md
      vector-store-retriever.md
      voyage-ai-retriever.md
    text-splitters/
      character-text-splitter.md
      code-text-splitter.md
      html-to-markdown-text-splitter.md
      markdown-text-splitter.md
      README.md
      recursive-character-text-splitter.md
      token-text-splitter.md
    tools/
      bravesearch-api.md
      calculator.md
      chain-tool.md
      chatflow-tool.md
      custom-tool.md
      dalle-image.md
      exa-search.md
      google-calendar.md
      google-custom-search.md
      openapi-toolkit.md
      python-interpreter.md
      read-file.md
      README.md
      request-get.md
      request-post.md
      retriever-tool.md
      serp-api.md
      serper.md
      web-browser.md
      write-file.md
    tools-mcp/
      bravesearch-mcp.md
      confluence-mcp.md
      contentful-mcp.md
      custom-mcp.md
      github-mcp.md
      jira-mcp.md
      postgresql-mcp.md
      README.md
      salesforce-mcp.md
      slack-mcp.md
    utilities/
      custom-js-function.md
      if-else.md
      README.md
      set-get-variable.md
      sticky-note.md
    vector-stores/
      astradb.md
      chroma.md
      elastic.md
      faiss.md
      in-memory-vector-store.md
      milvus.md
      mongodb-atlas.md
      opensearch.md
      pinecone.md
      postgres.md
      qdrant.md
      README.md
      redis.md
      singlestore.md
      supabase.md
      upstash-vector.md
      vectara.md
      weaviate.md
      zep-collection-cloud.md
      zep-collection-open-source.md
    README.md
  credentials/
    api-credentials.md
    README.md
  custom-tools/
    README.md
  documents/
    document-loaders.md
    README.md
    record-manager.md
  sidekick-settings/
    allowed-domains.md
    chat-feedback.md
    general.md
    rate-limit.md
    README.md
    speech-to-text.md
    starter-prompts.md
    visibility.md
  README.md
  variables.md
use-cases/
  agent-use-cases.mdx
  ai-explained.mdx
  module-1-welcome-to-ai.mdx
  module-2-art-of-the-prompt.mdx
  module-3-answeragent-in-action.mdx
  module-4-answeragent-vs-others.mdx
  module-5-supercharging-answeragent.mdx
  README.mdx
apps.mdx
coming-soon.md
intro.md
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="api/assistants/assistants-api.info.mdx">
---
id: assistants-api
title: "Assistants API"
description: "API for managing AI assistants"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Assistants API

This section contains the API endpoints for the assistants service.

## Overview

The Assistants API provides endpoints for managing assistants resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/assistants/create-assistant.api.mdx">
---
id: create-assistant
title: "Create a new assistant"
description: "Create a new assistant with the provided details"
sidebar_label: "Create a new assistant"
hide_title: true
hide_table_of_contents: true
api: eJztl9ty2zYQhl8Fs9ekRcryiVdVT1OlndYTO2lTj8YDESsJDgmgAChb0fDdOwtSMsXIjpvm0jc2BS52Aez+HxcbEOhyK42XWkEGP1jkHhlnCu8Zd046z5Vn99IvmV8iM1avpEDBBHouCwcRaIOW0/SJgAzy4GC8nQkReL5wkN3AzpuDaQQO88pKv4bsZgMz5BbtuPJLyG6m9TQCi/9U6Pz3Wqwh20CulUfl6ZEbU8g8BBzcOVr0Bly+xJLTk18bhAz07A5zCm4sLc9LdPRWio6N81aqBUSAD7w0BQ2J4UUyTxHj0/xkFI9mSRpfJHgai7MkPRudz5PzkxTqCLab/98BuXP+9tPsd/wwqf6avLv69f0vd28+zt/qU1msKJLiJX7RRXvUYWGdZPZn1RGUWmDxvL+F8fGIbKVy3lY5+XLPT/mgK8YtVc0SCzOvisfKiZjQbK0ry2boPPOaceXu0bKQXqkV40rQD7ummB7LUE2V7e5aVeUMbTdkSrba3JoXWOm9RHFr+RoikB7LA9uqO9NvYF6psH+IINcCb6XyaI1FH+LMZYG3DrnNlzBtQ91adLqyOR4sDi6EJH+8uNwrk327uq7riIQkUHnJv5CvMzG7OM6TYXwuTng8StOzmJ/P0/hYzGenZ8N0dnx8EdKZa3Vl8+edDWTJF+gGu/wdmeZQGlmLH7k/WI5zbUvuSUHcY+xliXtuh8lwFCfn8XB0nY6y9CQbDv8mr5UR39wrJUL6YPBIoXCixBRpUUDmbYVhwBmtXJODYZLQv30c7jyw9gCYq/IcnZtXRbEOZfEKplcwvYLpFUxfDaY6gtEh9kzUihdSMKlM5XeNF61uNBx+bv6ejAN9GD7k2AyT8xL9UlNrZrQLIODUZsGg049RO2ZXaF3oxipbQAZL743LBgNjtThyvhJSH/klNio54nLAjRysUuhpGy6tFo04WeMU6qjrMxsMCp3zYqmdz0ZJkjzl6DeyYgJXWGhTovI7f5328YoA29Rqt4ncpYwi0vaCGWStEUTtw8/bRL758zocFoH77WPv+dM2pw0xX8bhPQyHac/TdAvTDjH7Z7GrvJaTLQz7LPxWwOvxLt0xLd1x6z8x6ACC+iR5GS46tHgCCT0iHNZiT+BPYkCquQ7F1NetY+PLyWdZGl9O2FxbVnLFF1It2HjC9lRGEmtM06PkKGn7B56H/qEtgnHIynjCripjtKVa2BdkV4SEnZJLeu0a8+/2XtMpk+xLrjohDl/y+tvp3Lq+5lrYCtDjgx+YgksVuGrDR6NBUP9KSESg0c1mxh2+s0Vd03BTktnNNIIVt5LPwieQvmtL5AJtYNZHXNMymwXH1xSbzIsq6KrfmRGRmhnjnFD5rO20g9DLP66uCR7ttZTUCBlYfg9R+JtBuBDvWpMwtoGCq0XFF2Tb+KS88H1S9cgUdrVtDdS6s8LNprG41h9R1TVE7VY8/YZ6Wtf1v5FAVbM=
sidebar_class_name: "post api-method"
info_path: docs/api/assistants/assistants-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new assistant"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/assistants"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create a new assistant with the provided details

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"details":{"type":"object","properties":{"id":{"type":"string","example":"asst_zbNeYIuXIUSKVHjJkfRo6ilv"},"name":{"type":"string","example":"assistant"},"description":{"type":"string"},"model":{"type":"string","example":"gpt-4"},"instructions":{"type":"string","example":"You are a helpful assistant, do your best to answer question and query"},"temperature":{"type":"number","example":1},"top_p":{"type":"number","example":1},"tools":{"type":"array","items":{"type":"string"},"example":["function","code_interpreter","file_search"]},"tool_resources":{"type":"object","additionalProperties":{"type":"object"}}}},"credential":{"type":"string","example":"7db93c02-8d5a-4117-a8f1-3dfb6721b339"},"iconSrc":{"type":"string","example":"/images/assistant.png"},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Assistant"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Assistant created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"details":{"type":"object","properties":{"id":{"type":"string","example":"asst_zbNeYIuXIUSKVHjJkfRo6ilv"},"name":{"type":"string","example":"assistant"},"description":{"type":"string"},"model":{"type":"string","example":"gpt-4"},"instructions":{"type":"string","example":"You are a helpful assistant, do your best to answer question and query"},"temperature":{"type":"number","example":1},"top_p":{"type":"number","example":1},"tools":{"type":"array","items":{"type":"string"},"example":["function","code_interpreter","file_search"]},"tool_resources":{"type":"object","additionalProperties":{"type":"object"}}}},"credential":{"type":"string","example":"7db93c02-8d5a-4117-a8f1-3dfb6721b339"},"iconSrc":{"type":"string","example":"/images/assistant.png"},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Assistant"}}}},"400":{"description":"Invalid input provided"},"422":{"description":"Validation exception"}}}
>
  
</StatusCodes>
</file>

<file path="api/assistants/delete-assistant.api.mdx">
---
id: delete-assistant
title: "Delete an assistant"
description: "Delete an assistant by ID"
sidebar_label: "Delete an assistant"
hide_title: true
hide_table_of_contents: true
api: eJyFVMtu2zAQ/BViTy1AWE6RXniqAaeAihyCxkUPhg9rcW0RkUiFXLk1BP17sZLjym6SniSKs8+ZUQeWUhFdwy54MLCkipgUeoUpucToWW2PKl+ChtBQRMHlFgzYAbl4QYEGxn0Cs4ZzZIKNhkRFGx0fwaw72BJGiouWSzDrTb/R0GDEmphiGgBOmmiQS9DgsSYw4CxoiPTcukgWDMeWNKSipBrBdMDHRlCJo/N76PXVQOcGZQYpGCk1wSdKEvxpPpfHWyHjjFaltigopV1bVUcpcfta2KoklRoq3M6RVflSuaScP2Dl7Bhz+14pH1jtQusH7OfX8ueeKXqsFMUYIvS9hpq4DH/JAD2uzkD2l4Osc7YH4SEeXrbcxgoMlMxNMlnWxGBniVvrwoxLQp9+UZyhy7Bx2eEGrlf6EINtCzmoMSn0eprTZFkVCqzKkNjczufztxLdC0pZOlAVmpo8n/NNdPMoTI9sTdVzJl4qwkkQch5BoE8vX0OskcHAt5+rYWfO78IQ7riiKQVJLR7yf3pcPORqF6Kq0ePe+b1a5GoicA2y1BF6M5vP5qChCJ6xYClykvBiWOkiV49t04QoZrmkYLp20EA1OrlOI/zLxbUM0YTENfpJiVeMez1LN3RGnv/j89NimX5z1lTovEhy6Lc76evC4xqMs+J04Vvuum6LiX7Equ/l83NLUdy/0XDA6HAra193YF2Sdwtmh1Wid5r98P3k/o/qws5vtHr6iP4o9GDVygk0PNFx/J30m6l3lnf3d6s70ICXwroS0tDzq6m7bkSswhP5vj9XYjlLsb7/A4432RQ=
sidebar_class_name: "delete api-method"
info_path: docs/api/assistants/assistants-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete an assistant"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/assistants/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete an assistant by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Assistant ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Assistant deleted successfully"},"400":{"description":"The specified ID is invalid"},"404":{"description":"Assistant not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/assistants/get-assistant-by-id.api.mdx">
---
id: get-assistant-by-id
title: "Get assistant by ID"
description: "Retrieve a specific assistant by ID"
sidebar_label: "Get assistant by ID"
hide_title: true
hide_table_of_contents: true
api: eJytVm1v2zYQ/ivEfdoAOZYc502f5iFb524ogiTd1gVBcCJPNlOJZEnKq2fovw8nOY4du2mB7ZNN8e654z3H57gCRUF67aK2BnK4pug1LUigCI6kLrUUGIIOEU0UxVJMLyEB68gje0wV5DCjOHky+XE5VZBAxFmA/A42rgHuEwgkG6/jEvK7FRSEnvykiXPI7+7b+wQceqwpkg+dgeZ8HMY5JGCwJshBM7anT432pCCPvqEEgpxTjZCvIC4dW4XotZlBm7w42yZJPgQH9BScNYECO4/SlH92XW4aKSmEsqnE5syQgLQmkolsj85VWnYbw8fATqv9jGzxSDJCAs4zTNR9SK32s06APmPtKv6kRhdpmRENTuXJeDAu0mxwkdLpQJ2l2dn4vEzPT7L+mBF1Ff57QAwhPvxTvKMP0+bP6fubX3//5fHtx/LanupqwZF6Hr4C0Rd5r/4H+Kmtoup1vJmLgzHbahOibyRjhdddPthGoOcWnlPlmLtNUolQVixt40VBIYpoBZrwN3nxqaHA0AKN4oVfcsxIdcd647dPbZq6IL8dMmNb6x7cN1jZHaLQe1xCAjpSfeBY7Zb7HZSNkZsOVPSgTSTvPF8ZSKDUFT0EQi/ncL8O9eAp2MZLOtgcqJRmPKyudtpk165t2zYB6UmRiRq/wteZKi6OZToanKsTHIyz7GyA52U2OFZlcXo2yorj44uOTmnNjZevgw11jTMKww1/R64vivSEkdQlxoPtWFpfY+QbhJEGUde0AztKR+NBej4YjW+zcZ6d5KPRX4zaOPW/ozIROnYGk+er0ZV0fEhxbuf0pLykxPRS6CC0WWClFXQ+432fZ2EzNorSNqazPTmEP+WeMVgJ8t76Lr+a4tyuhZwlgyU3h+eih+FKqxZYv/3iSZ0bX0EO8xhdyIdD5606CrFR2h7FOfWX6gj1EJ0eLjJ4KcVX3qr+LoseFNpkGzMfDisrsZrbEPNxmqZfAvqNrYSiBVXW1WTiBm9r3tywHvetvT11NgxzRFgPEl73RpCs//z8xPvbP267gmlT2s79JbFBTK6mezlOrqaitF7UaHCmzUxMpmJrMCbARe1Ns6P0KF0PGJTdgFmPvklX0slU3DTOWc9E7VKwXXbuyxo1b4fe/IedbT6EsyHWaLZCvKF4YNa/0PDN6PvGp8K6xJE+x6GrUJvuovlORfpO23klJJBrxW8FZp73VqsCA733Vdvy516a87v7BBboNRadMq5A6cD/FeQlVoFeSfu76/X74Xux8yD4QqpPOm1YpRdYNbyCBD7Ssn+QtKy1c0JFvsuk35hISS5uuey9E7hBNzfvzU+3rMe7bfmiDTv0g+msVr3Frf1Ipm032UVec4Jt+y/+HVr4
sidebar_class_name: "get api-method"
info_path: docs/api/assistants/assistants-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get assistant by ID"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/assistants/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a specific assistant by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Assistant ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"details":{"type":"object","properties":{"id":{"type":"string","example":"asst_zbNeYIuXIUSKVHjJkfRo6ilv"},"name":{"type":"string","example":"assistant"},"description":{"type":"string"},"model":{"type":"string","example":"gpt-4"},"instructions":{"type":"string","example":"You are a helpful assistant, do your best to answer question and query"},"temperature":{"type":"number","example":1},"top_p":{"type":"number","example":1},"tools":{"type":"array","items":{"type":"string"},"example":["function","code_interpreter","file_search"]},"tool_resources":{"type":"object","additionalProperties":{"type":"object"}}}},"credential":{"type":"string","example":"7db93c02-8d5a-4117-a8f1-3dfb6721b339"},"iconSrc":{"type":"string","example":"/images/assistant.png"},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Assistant"}}}},"400":{"description":"The specified ID is invalid"},"404":{"description":"Assistant not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/assistants/list-assistants.api.mdx">
---
id: list-assistants
title: "List all assistants"
description: "Retrieve a list of all assistants"
sidebar_label: "List all assistants"
hide_title: true
hide_table_of_contents: true
api: eJytVdtu2zgQ/RVhniVLcpybntbA3twtFkGTbtsNjGBEjmwmEsklKW+9hv59MZLrym6a9qFPksjDM6OZwzM7kOSFUzYoo6GANxScog1FGNXKh8hUEdZ1hN4rH1AHDzEYSw4Zv5BQAMPm4+2AKw/FPYzOLGPwJFqnwhaK+x2UhI7cvA1rKO6X3TIGR94a7clDsYNplvHjOLPbVgjyvmrr6JAAxCCMDqQD49HaWol+I330fGgHXqypQX4LW0tQADqHW4hBBWr8aN2UjyQCxGAd0wc1pKLkCOODU3oFMdBHbGzNS3J6nVU5UXIhzmfJrMzy5Dqji0ReZvnl7KrKrs5z6GKQFFDVPyAgeh8e/iv/pA+L9v3i7e0ff/3++OqpemMuVL3hSBob+ibF0JghsVGRT091MTRGUv0y38qGZMZYpX1wrWAu//KRD6aN0LHK1lRb7ukhqTiSJtqa1kUl+RAFE6H2/5KL/mnJM3WEWvKH23LMQE2vhtaN/1q3TUluHDJnrLEP9jtQ5qhRX1PM5xodjt9D1WpxUKakB6UDOeso9HEqVdODJ3RiDct9qAdH3rRO0LPiQCkV82F9cySTY1zXdV0MwpEkHRR+o1+Xsrw+E9k0uZLnmMzy/DLBqypPzmRVXlxO8/Ls7LpvpzD61omXyVLV4Ip8eujfxA5FEY4wkPwZw7NyrIxrMPANwkBJUA0d0U6z6SzJrpLp7C6fFfl5MZ3+zaytlT+clRuhQg+Yf74aQ03Pn7OiBTdVYx2Rc8b1BA2FtWE7XFF/p5GtDdIj3/TkNuR874Ctq6GAdQjWF2lqnZETH1qpzCSsaRD8BFWKVqWbHE6uKdw4I4d7Fg2k0MVjziJNayOwXhsfilmWZV8jes2oSNKGamMb0uHAN7LsW/bQQXZj4z5UnyPy7/UwKPYgiPcvv37qyat3d32tlK5Mf/y06D6a3yy+yHF+s4gq46IGNa6UXkXzxfE84qIO0HySTbL9UEDRD4XBDmHel3S+iG5ba43jHh23YFx21kyDirf9AP/paJt/whofGtSjEK95Xn4xLE/89TCuvmvS7gsc6GNIbY1K91fA9fd7kNjpmOWO8+puV6Knt67uOl4e7LK4X8awQaew7N2KLWhNKMn1mnyiLVdKCLJcnw3Wbe9/p2OVtXHQ+2+/3LFNHSviRAE9+yc31dsR9243IO7ME+mug3ifROBv6JZd1/0PUpHvNA==
sidebar_class_name: "get api-method"
info_path: docs/api/assistants/assistants-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all assistants"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/assistants"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a list of all assistants

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"details":{"type":"object","properties":{"id":{"type":"string","example":"asst_zbNeYIuXIUSKVHjJkfRo6ilv"},"name":{"type":"string","example":"assistant"},"description":{"type":"string"},"model":{"type":"string","example":"gpt-4"},"instructions":{"type":"string","example":"You are a helpful assistant, do your best to answer question and query"},"temperature":{"type":"number","example":1},"top_p":{"type":"number","example":1},"tools":{"type":"array","items":{"type":"string"},"example":["function","code_interpreter","file_search"]},"tool_resources":{"type":"object","additionalProperties":{"type":"object"}}}},"credential":{"type":"string","example":"7db93c02-8d5a-4117-a8f1-3dfb6721b339"},"iconSrc":{"type":"string","example":"/images/assistant.png"},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Assistant"}}}}},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/assistants/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/assistants/assistants-api",
    },
    {
      type: "category",
      label: "assistants",
      items: [
        {
          type: "doc",
          id: "api/assistants/create-assistant",
          label: "Create a new assistant",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/assistants/list-assistants",
          label: "List all assistants",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/assistants/get-assistant-by-id",
          label: "Get assistant by ID",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/assistants/update-assistant",
          label: "Update assistant details",
          className: "api-method put",
        },
        {
          type: "doc",
          id: "api/assistants/delete-assistant",
          label: "Delete an assistant",
          className: "api-method delete",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/assistants/update-assistant.api.mdx">
---
id: update-assistant
title: "Update assistant details"
description: "Update the details of an existing assistant"
sidebar_label: "Update assistant details"
hide_title: true
hide_table_of_contents: true
api: eJztl91v2zYQwP8V4p42QI4lx/nS07x1w9wNQ9Ak27ogCM7iyWYrkRxJufUM/e/DSYojO47bon3Mky3x7kjex093a5DkM6dsUEZDCjdWYiARFiQkBVSFFyYXqAV9VD4oPRfovfIBdYAIjCWHrDmVkELV6E566wHnHtJb2Oh4uIvAU1Y5FVaQ3q5hRujITaqwgPT2rr6LwKLDkgI53wgoPpbFsIAINJYEKSgJETj6t1KOJKTBVRSBzxZUIqRrCCvLUj44pedQRztX3BxQTF8Bb8iWyIcfjVyxemZ0IB34L1pbqKy54fCdZ+31033M7B1lfFvr2B9BkedVJZ+eJQL6iKUt+JUcXcR5QjQ4zU7Gg/EsTgYXMZ0O5FmcnI3P8/j8JGkP34Th6zdE78P9f7M/6O20+nt6c/Xbn7++e/0+f2NOVbHknVrvfsJEF9tdr+7xemkkFYftzW0YjFlWaR9clbEtf1jlrakEOhIoFlTYvCoeEzIS0oiVqZyYkQ8iGIHafyAnmvAqowVqyQ9uxXsGKpv0rVz/1roqZ+T6WyYsa+y9/QwpsxUodA5XEIEKVO65Vt1Tv4W80s39IYLMSLpXOpCzjgsBIshVQfee0GULuOu2unfkTeUy2pscKKVie1hcbqXJtlxd13UEmSNJOij8RLzO5OziOItHg3N5goNxkpwN8DxPBscyn52ejZLZ8fFFE87M6CuXHTY2VCXOyQ838TuyrVMyRxhIvsKwNx1z40oMXEEYaBBUSVtmR/FoPIjPB6PxdTJOk5N0NPqHrbZ0+rZWORAqNAKP2Gtd6shbo33r81Ec889zHOqOJnyVZeR9XhXFqkmDFxC9gOgFRC8g+hoQjfex53pBwlvKVK5IiukrobxQeomFksI4MTNyxa9K5X2XION4fAhh2gSRm0pLlj3Zt+WU00hjIcg545ojlxQWhjtHWzUU4SYvhcc4+OFayRq4Y3TLh36wcgWksAjB+nQ4tM7IIx8qqcxRWFBbZ0eohmjVcJnAbvN36Yxsy1u0RqGO+jbT4bAwGRYL40M6juP4OUO/s5SQtKTC2JJ02NjrdbhXjOg22/t97ibovCN0rSs/t0IQdX9+eUiF139dNw5j9L957FZ/fsiKlrmfR/ItkDdqh3m86br7jf+2Lza525G2w+kuTb8VMneImWyomGzI90UU2wOxXRZ9HnB6vHkGKjtM2V/NO4h4FiRK56ZJpt3K92JyOX0SpcnlVOTGiRI1znmSm0xFbzCLgEusFU2O4qO460AwazqQLgkmTVQmU3FVWWsc58J2QfaLkMFVouJl34r/sLXMXrbGhxJ1b4tuBN0c7WEQ3b1Qb1L7wrG1q75AH8PQFqh0g2XXfHNaCG2NrBGkSnKaMBR4bb2eoacbV9Q1v26zMr29i2CJTuGs+Y6uQSrP/yWkORaeDhz/uzfdMPu92JpOnznqw1ddcy0ssaj4CSJ4T6t2Oq75y7wglOSak7QLP7X7Da5Z/VHxSW/JRGw1JllGNhyUveth/PLmmtnVzdEMA0jB4Qee1vFDe0ZjN71V824NBep5hXOWbU1yWuA2KHfA2FxqrxfW61bi2rwnXdcbpwR+Zr/U9f9Eh7SK
sidebar_class_name: "put api-method"
info_path: docs/api/assistants/assistants-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update assistant details"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/assistants/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Update the details of an existing assistant

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Assistant ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"details":{"type":"object","properties":{"id":{"type":"string","example":"asst_zbNeYIuXIUSKVHjJkfRo6ilv"},"name":{"type":"string","example":"assistant"},"description":{"type":"string"},"model":{"type":"string","example":"gpt-4"},"instructions":{"type":"string","example":"You are a helpful assistant, do your best to answer question and query"},"temperature":{"type":"number","example":1},"top_p":{"type":"number","example":1},"tools":{"type":"array","items":{"type":"string"},"example":["function","code_interpreter","file_search"]},"tool_resources":{"type":"object","additionalProperties":{"type":"object"}}}},"credential":{"type":"string","example":"7db93c02-8d5a-4117-a8f1-3dfb6721b339"},"iconSrc":{"type":"string","example":"/images/assistant.png"},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Assistant"}}}}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Assistant updated successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"details":{"type":"object","properties":{"id":{"type":"string","example":"asst_zbNeYIuXIUSKVHjJkfRo6ilv"},"name":{"type":"string","example":"assistant"},"description":{"type":"string"},"model":{"type":"string","example":"gpt-4"},"instructions":{"type":"string","example":"You are a helpful assistant, do your best to answer question and query"},"temperature":{"type":"number","example":1},"top_p":{"type":"number","example":1},"tools":{"type":"array","items":{"type":"string"},"example":["function","code_interpreter","file_search"]},"tool_resources":{"type":"object","additionalProperties":{"type":"object"}}}},"credential":{"type":"string","example":"7db93c02-8d5a-4117-a8f1-3dfb6721b339"},"iconSrc":{"type":"string","example":"/images/assistant.png"},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Assistant"}}}},"400":{"description":"The specified ID is invalid or body is missing"},"404":{"description":"Assistant not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/attachments/attachments-api.info.mdx">
---
id: attachments-api
title: "Attachments API"
description: "API for managing file attachments"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Attachments API

This section contains the API endpoints for the attachments service.

## Overview

The Attachments API provides endpoints for managing attachments resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/attachments/create-attachment.api.mdx">
---
id: create-attachment
title: "Create a new attachment"
description: "Upload a file to create a new attachment"
sidebar_label: "Create a new attachment"
hide_title: true
hide_table_of_contents: true
api: eJztV02P2zYQ/SvEnBJAu/amOelUt0gAFwm6wHrRg2MEY3FsMZFILknZcQ3992JI2ZbWTtYBeirqi2VqPt97JMd7kOQLp2xQRkMOj7YyKAWKlapIBCMKRxhIoNC0FRgCFmVNOkAGxpJDdptKyCHZTfoGAdce8jmcvDwsMvBUNE6FHeTzPSwJHblJE0rI54t2kYGjp4Z8+M3IHeR7KIwOHC7fQ91UQVl0YbQyrr6RGJCXfVFSHZ/CzhLkYJZfqOACrOMagyLPb7mjnpUPTuk1ZMDBMEAOS6XR7SB7BsmspCMaTYQH2gxs46zxFwOeuyvuQZIUnZcwKxG6sJAB6aaOQHmvfEDGKYOavMc1eVi0/EnAKEcS8uAaigveGu1Td2/GY/4a5j7R0fEohW+KgrxfNVXFrfbgRWsrVURGR188+1+PrZIvA/Go1VNDQknSQa0UObEyLsLQk1WbHVJciHfE6WS/uAR3iiDYPRPbUhWlUF5gtcWdF5967p+AE3bQfMZ+UqZsTe5SF99EUDX5gLUVr5QWngqjpX8d+9mWpJ81JbboD/hzPmZdY32ldthyoJcfi+8FLf20OtsMlruQWH4BGo7l1d8Df6G0SP5tBj5gaPyPqrbOsDrToiOUrFFyzrjLpaeIg4S9EO1R359Twmvw/jj9+C5KZwjDYAPOWfDZaTP0FNSj98TTAcMjBIsMggp8HvX2KKR9/vbSTp7qDVZKCqVtE7jHjZIk/70NnDB+0awT0stITkTZ1KhvmENcViRiAnHwbzO4npEDGalGlj86rK/zjaYUiA8aDKLAxpOMtKaCXqmV6HBbVvQ6SUZe1WCNRak0PW8xuj8/secnER94f5e6SZTf/XJO+ft05xhRoVvT/1T/F6hu+SwOpeGJyRofUUeefWDUn5J4SHIbcj7OSI2rIIcyBOvz0cg6I299aKQyt6Ek1H5L7hbVCK0abe7Oqr93RjYF/xApKLRZP2Y+GlWmwKo0PuRvx+Px9wJ9YCshaUOVsfFeO8TrDXUPLMIkoP5od4SYM3J70YxnrmjEB2R8eH+Yxf74axbPXKVXJrqfnZZeTO6n51zdT+M1XKPGtdLrdCcMsWVgk/Xd7fh23O0sTDNHuphhEmGdTMVDY61xTNSQhj70fEfVqPi1T+a/Dl5zI8x2jbqX4vfvTtaDlnoj8M9M5x3egb6Fka1QadZ7bGHfSe5sMGcF8PJ+v0RPj65qW15+asjxsL7IYINOsf7jrJ5BSSjJRY1+pR23lCq9mXFyNq8aLuLS4M4iTE6ToiAbeuZnhxsL7Lht7v98mLFeuv8HddzCcYiPcbPTYyoShwJ8JrhYfPcK9a5XxX6fLGbmK+m2hawrN/BvaHkq/we77ZrS
sidebar_class_name: "post api-method"
info_path: docs/api/attachments/attachments-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new attachment"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/attachments"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Upload a file to create a new attachment

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"multipart/form-data":{"schema":{"type":"object","properties":{"file":{"type":"string","format":"binary","description":"The file to upload"},"purpose":{"type":"string","description":"The intended purpose of the file","enum":["assistants","messages"]}}}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Attachment created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for the attachment"},"object":{"type":"string","enum":["attachment"],"description":"The object type, which is always \"attachment\""},"created_at":{"type":"integer","description":"Unix timestamp (in seconds) for when the attachment was created"},"filename":{"type":"string","description":"The name of the file"},"purpose":{"type":"string","enum":["assistants","messages"],"description":"The intended purpose of the file"},"bytes":{"type":"integer","description":"The size of the file in bytes"},"status":{"type":"string","enum":["processing","ready","error"],"description":"The status of the file processing"},"content_type":{"type":"string","description":"The MIME type of the file"}},"required":["id","object","created_at","filename","purpose","bytes","status"],"title":"Attachment"}}}},"400":{"description":"Invalid input provided","content":{"application/json":{"schema":{"type":"object","properties":{"error":{"type":"object","properties":{"message":{"type":"string","description":"A human-readable error message"},"type":{"type":"string","description":"The type of error"},"param":{"type":"string","description":"The parameter that caused the error (if applicable)"},"code":{"type":"string","description":"A machine-readable error code"}}}},"required":["error"],"title":"Error"}}}},"413":{"description":"File too large","content":{"application/json":{"schema":{"type":"object","properties":{"error":{"type":"object","properties":{"message":{"type":"string","description":"A human-readable error message"},"type":{"type":"string","description":"The type of error"},"param":{"type":"string","description":"The parameter that caused the error (if applicable)"},"code":{"type":"string","description":"A machine-readable error code"}}}},"required":["error"],"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/attachments/delete-attachment.api.mdx">
---
id: delete-attachment
title: "Delete an attachment"
description: "Delete a specific attachment"
sidebar_label: "Delete an attachment"
hide_title: true
hide_table_of_contents: true
api: eJytVcGO4zYM/RWBp11AjbPFnHxqgJkCKfYw6E6xh9mgYCQm1q4taSU508DQvxe0ncSeTGfm0FNi+5F8JB/JDjRFFYxPxlko4ZZqSiRQRE/K7IwSmBKqqiGbQILzFJChaw0l6B68mgIS7iOUj3CxirCREEm1waQjlI8dbAkDhVWbKigfN3kjwWPAhhKF2AMsNgTlxMffRoMEwwQ9pgrkM9YPFYn1rXA7kSqaMBbJiYEkSAj0szWBNJQptCQhqooahLKDdPQcL6Zg7B4yMwoUvbORIn//dbnkn3nMS9pjCC1iqxTFuGvr+ggSlLOJq1J2gN7XRvWVK75Htu+u47vtd1JcRB+4zskM0Y2+5vh6AU58Jq3L8uT+BV9k22betMXognt3HWnwJNiNFE+VUZUwUWD9hMcovr3g5hswgZPPC4OtczWhvVDgzlyF/FpRqig87+0TxlOmkPO0v4/Q6+Vcz0kyyaSazjrXE+3mzE5uljevttq6JHautfr/6y+F4MLbsIZixD29rYWVqNoG7S+BUOO2JtEHECf7LEf792iKEayqgWMeJ/V9tuehFqnCJBS2kXTfxIHQB7MTY922NX1k78rpdyXYoKqMpecp9uZDH6diGMhPmn83ZDMgG0qVu2wzrjuvmBKKyQ4rutkyysArLRxOC6sNNZRQpeRjWRQ+OL2IqdXGLVJFaOMThQWaAr0pDp+u8rkPTreKH8TgFLKc+iyLonYK68rFVN4sl8v/cvSZUULTgWrne7me/E1W8BeW5SCp6SI+F50jwrgd+xntQSDHP7+70GCCEv74+tCPnbE715uPxb0MSxSr+/V19+7XYsd6RIt7Y/diZ+rpXEeQwIUd0J8Wy8VynDUcttd4HVZ9WVdr8aX13gUemHkbpqXnDdOg4c9xgP82+8yJeBdTg3YS4nQM7fwIzvLpLmvgreM5FjjRP6nwNRrLku85d6Pm5ndTQjk/gRsJrAHGdd0WI/0V6pz59c+WAh/XjYQDBsMz0QtTm8j/NZQ7rCO9wv3Dn+PEfBTvuqYvJjO+RMvn74B1y08g4Qcdr+553mQJFaGm0FMdMCulyKeJ9dVmZS2fZ/b27vPdwx1IwLmIn4m2D/Aiua4bEA/uB9mcz1wTPzPHnP8FVo0r9g==
sidebar_class_name: "delete api-method"
info_path: docs/api/attachments/attachments-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete an attachment"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/attachments/{attachment_id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete a specific attachment

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"name":"attachment_id","in":"path","description":"The ID of the attachment to delete","required":true,"schema":{"type":"string"}}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Attachment deleted successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"The ID of the deleted attachment"},"object":{"type":"string","enum":["attachment.deleted"],"description":"The object type, which is always \"attachment.deleted\""},"deleted":{"type":"boolean","enum":[true],"description":"Whether the attachment was deleted"}},"required":["id","object","deleted"],"title":"DeletedAttachment"}}}},"404":{"description":"Attachment not found","content":{"application/json":{"schema":{"type":"object","properties":{"error":{"type":"object","properties":{"message":{"type":"string","description":"A human-readable error message"},"type":{"type":"string","description":"The type of error"},"param":{"type":"string","description":"The parameter that caused the error (if applicable)"},"code":{"type":"string","description":"A machine-readable error code"}}}},"required":["error"],"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/attachments/download-attachment-content.api.mdx">
---
id: download-attachment-content
title: "Download attachment content"
description: "Download the content of a specific attachment"
sidebar_label: "Download attachment content"
hide_title: true
hide_table_of_contents: true
api: eJyVVU2P2zYQ/SvEnBKAsZxiTzrVaNLCRQ+LZoseFkYxpkYWE4lkyJFTQ+B/L0aSP3c32Z5sivP1hu/NDFBRMtEGtt5BCR/8N9d6rBQ3pIx3TI6VrxWqFMjY2hqFzGiajhyDBh8ooviuKyihmr1XJ5NfphCggXGXoHyEs3uCjYZEpo+WD1A+DrAljBRXPTdQPm7yRkPAiB0xxTQaOOwIyosY/9gKNFgpPSA3oG/wPDSk1h8EgQA6+yn26lgtaIj0tbeRKig59qQhmYY6hHIAPgTJmDhat4MsNUVKwbtESe5/Wi7l5zrrGf+ph8dkVKnUG0Mp1X3bHkDDbCFRMITWmrGfhTdM/C5xJOzk7qWSNNQ+dshQwtY6jAfIOWcNd8u77xbmPKva9656uYTPSZyeSe23n8nIo4YoBGA79YJi9PHHZh2lhDt6DspNtarpO3TvImGF25bUmEAd/bOe/X8UR0ggFkKDqcY8M+t1vicSKm6QlcE+0aSQqaA3tlZz37YtvZXoxlevAtihaayjW4ij+/SOZ2o+zg3eaGDLrcT9OKGZLDvixosMdzQ2XfRQQnEhuGK4Uk4uzEmeieL+qLI+tlBCwxxSWRQh+mqRuK+sX3BD6NI3igu0BQZb7N8/AXUffdUbOagpKGR9GbMsitYbbBufuLxbLpcvBfpDrFRFe2p9GDl7jHcxNz4JNydeXU6PU+clI8yClvNkBHr+8+tRO7///QDSROtqP7rPHT4rJqnV/frpE96vVS2kRIc763aqtu3loEmgQRo7Wb9fLBfLWXBoRsHNI201tnW1Vp/6EHyUJ7l+hsvWgwbq0Mp1msx/vroWIMEn7tBdpDjNdnwynm5hDeeR8L9Xwtx4pn+5CC1aJ3oYsQwzJ6+XgIbydp4fk280CEvEYxi2mOiv2OYsn7/2FGVnbDTsMVqRzkjdyib5X0FZY5voO7De/DkL66165ZJ4Ftj8EZ2M8j22vZxAwxc6PFlUeZM1NIQVxbHYyWZlDAW+8H5xCwjvTyL/7eMDaMBrtt+we8zybIXDMFk8+C/kcj4VzHKWQnP+D4FH0/0=
sidebar_class_name: "get api-method"
info_path: docs/api/attachments/attachments-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Download attachment content"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/attachments/{attachment_id}/content"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Download the content of a specific attachment

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"name":"attachment_id","in":"path","description":"The ID of the attachment to download","required":true,"schema":{"type":"string"}}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Attachment content downloaded successfully","content":{"application/octet-stream":{"schema":{"type":"string","format":"binary"}}}},"404":{"description":"Attachment not found","content":{"application/json":{"schema":{"type":"object","properties":{"error":{"type":"object","properties":{"message":{"type":"string","description":"A human-readable error message"},"type":{"type":"string","description":"The type of error"},"param":{"type":"string","description":"The parameter that caused the error (if applicable)"},"code":{"type":"string","description":"A machine-readable error code"}}}},"required":["error"],"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/attachments/get-attachment.api.mdx">
---
id: get-attachment
title: "Retrieve an attachment"
description: "Get information about a specific attachment"
sidebar_label: "Retrieve an attachment"
hide_title: true
hide_table_of_contents: true
api: eJytVt9v2zYQ/leIe2oALnaHPOlpBpoVHlYgaFPsITWCs3S22EokQ56cZoL+9+FE2ZFsL8mAPdki7+d33/GuhYJiHoxn4yxk8JFYGbtxoUY5Ubh2DStU0VNuNiZXyIx5WZNl0OA8hV5uWUAGW+LF+JZxGyG7g2eVCCsNkfImGH6C7K6FNWGgsGi4hOxu1a00eAxYE1OIvYDFmiAb2bg3BWgwEqxHLkEfZXBbklp+UG6juKRRuIqdCsTB0I5AQ6CHxgQqIOPQkIaYl1QjZC3wkxePkYOxW+gkpkDROxspyv2v87n8TL0+J35wUqjY5DnFuGmq6gk05M6yIJO1gN5XJu+hm32PYqE9jcCtv1MuQPogQLNJ/k1xGuUxCF+teWhImYIsm42hoDYuHAECnd67OGOPbFNPiye1O4U6WVCirtVjafJSmaiwesSnqL6N1L+BOMwDIVNxj2OnxjJtKZzL4qdiU1NkrL16Z6yKlDtbxIs+n8eS7HGVHzGqwYn425iKEoVeg0ySEck9c0RTLPgmeBfPGjhgFKOJjMJvDTXFiFuK5+GSVG1BhRrMHntbP3Gq8ivQiK1o/p7oK2NV0u80REZu4ktR++CEnekwEBbCUQrBhfOhJ4sThyMT3YHf98nhW/D+tPx03VNnCkM3bs876Bv+0AwjBo3K+1ynPYYHCFYa2HBFky6FrhM3V/OrF3vZOlYb19ji/2vfhPCrYgONXsdxocqmRvuLVBDXFanegdrrdxreXo99KVKM3fAYv0338G4rLpFVjk2koi9qCuid2agBt3VFF4kwxZsSrDEvjaXjFHv1VMcxXQ4U3lf9OmWTJGvi0g3TSkCXEZLBbDSjZu1k2HTCJAq7/UBqQgUZlMw+ZrOZD664jNwUxl1ySWjjI4VLNDP0ZrZ7f5LMTXBFk/ejNRmFTo9tZrNZ5XKsShc5u5rP5/9m6E+RUgXtqHK+5+re3mjEfhFOJj6NB+0BcfEIw+yT7yQkLdT/+b1fAyCDP/667btSNoNe/aSfolrcLE9Ld7PsH+oaLW6N3aZXY7wPaBBgk/T7y/nlfGg0TFNpmP6LHtbFUn1pvHdBCjctwxh6ecVqNHIdk/hvk2tJxLvINdqRi8/D1FZopzvOJKP2+RX4j6vSgDjTT575Co2VBuiTaAcSThclDdl051lpEFKIXNuuMdLXUHWdHD80FGSbWmnYYTDSIT1TCxPlfwHZBqtIL6Ty7vPQPxfqjevT2XSGQ7QySXZYNfIFGn7Q08kK1606DSVhQaEPNsks8pw8j7RPXlqh96GHP17fggackvqIxL31s5G1bZK4dT/Idt0hUJZvCbDr/gGbhN6o
sidebar_class_name: "get api-method"
info_path: docs/api/attachments/attachments-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Retrieve an attachment"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/attachments/{attachment_id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Get information about a specific attachment

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"name":"attachment_id","in":"path","description":"The ID of the attachment to retrieve","required":true,"schema":{"type":"string"}}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Attachment retrieved successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","description":"Unique identifier for the attachment"},"object":{"type":"string","enum":["attachment"],"description":"The object type, which is always \"attachment\""},"created_at":{"type":"integer","description":"Unix timestamp (in seconds) for when the attachment was created"},"filename":{"type":"string","description":"The name of the file"},"purpose":{"type":"string","enum":["assistants","messages"],"description":"The intended purpose of the file"},"bytes":{"type":"integer","description":"The size of the file in bytes"},"status":{"type":"string","enum":["processing","ready","error"],"description":"The status of the file processing"},"content_type":{"type":"string","description":"The MIME type of the file"}},"required":["id","object","created_at","filename","purpose","bytes","status"],"title":"Attachment"}}}},"404":{"description":"Attachment not found","content":{"application/json":{"schema":{"type":"object","properties":{"error":{"type":"object","properties":{"message":{"type":"string","description":"A human-readable error message"},"type":{"type":"string","description":"The type of error"},"param":{"type":"string","description":"The parameter that caused the error (if applicable)"},"code":{"type":"string","description":"A machine-readable error code"}}}},"required":["error"],"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/attachments/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/attachments/attachments-api",
    },
    {
      type: "category",
      label: "attachments",
      items: [
        {
          type: "doc",
          id: "api/attachments/create-attachment",
          label: "Create a new attachment",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/attachments/get-attachment",
          label: "Retrieve an attachment",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/attachments/delete-attachment",
          label: "Delete an attachment",
          className: "api-method delete",
        },
        {
          type: "doc",
          id: "api/attachments/download-attachment-content",
          label: "Download attachment content",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/attachments/create-attachment",
          label: "Create attachments array",
          className: "api-method post",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/chat-message/chat-message-api.info.mdx">
---
id: chat-message-api
title: "Chat Message API"
description: "API for managing chat messages"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Chat-message API

This section contains the API endpoints for the chat-message service.

## Overview

The Chat-message API provides endpoints for managing chat-message resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/chat-message/get-all-chat-messages.api.mdx">
---
id: get-all-chat-messages
title: "List all chat messages"
description: "Retrieve all chat messages for a specific chatflow."
sidebar_label: "List all chat messages"
hide_title: true
hide_table_of_contents: true
api: eJztWVF32joS/is6eto9a8BQkqbeh71OSHq5t03TJLTbzcnpEdaA1cqWK8kkLIf/vmdkAyaAoU32afelxfZovtFo5pP0ZUYtGxsa3NEoZjYBY9gY6L1HDUS5FnZKg7sZHQLToMPcxjS4u5/fe1RloJkVKu1zGtAx2FDKs5jZ94UHQz3KwURaZGhEA3oNVguYAGFSEsQiJZghI6UJIyaDSIxE5D6OpHpoUo9mTLMELGjj4hDoKWM2ph5NWQI0oIJTj2r4kQsNnAZW5+BRE8WQMBrMqJ1maGWsFumYzp9GdVZikX6Pzr0S4EcOerpCwHhu0c1uvx6FNE8wi/3L2/Pry/Ad9ej5P8uf9xuwF0Ja0GQ4LTLhnO2CV5qDPgg7vDmjHu2d35xtgbxR2pLCV91E+7wGat88apKYQKL0dH8aH1mSSXx1mo9GoMl7N64OufBcn0MDxhS1+kuTK4fXzc9Ypm2P2frpjZROmKUB5cxCw4oEamHRJ0HTnbCQ8hcGhZTXQ44A+JBF37dhDpWSwNI6/8vh+/wf3HK3vw/en958HVxRb/G79+HzZW3bLUDKsrlHCjGZSg0YhOr4Pv63PjokUhhL1GidvqhHI5VaSC0OYVkmReSYsfXN4LjZ5hSY1gxnLCwkpvJeDb9BZJH2NPKrFUU0gtcua547Dly1Du+88UdtgMZxdNRtdId+u/HGh+MGf+23X3dPRv7JURuXSCs0351YlomSzqlHcwP6/Wp7WKFVrOYeXZD3fyHmjvO/SvVu7vgdpFQeidUDiVhK+iQGmZGpyolVnE3/gY6MynUEPRXlCaTWbFmdNJeSDdFhsakcuFoZG8PZIVHexsIQYYiNgZTTwuLCR/TRxCgTsIwzy7bBMs4F1hmTV2sBbNDaEnJGWW5jpWlA/1BxSnoKV9Z1e0A7fqfb8E8anS6dz+cetcK6MBcpovgyN8BvlZIvmC+rlKxP1CVLYJEZZ43RKSX7aZbbl8qMKJxRA0xHMSlIaV4ifcjtOtTCRSVPgzI17uVISAjTVFnHBC+YLXR8hQeg2ozhEallVQutaRnOpaPXulFo1bSPdm1aF2szcZ/YGFJ7DcyoFF282Nyc3/1hOrOiOUoG3s+sW5b8jsZIE26bSBU/ID8GfoQLbBzR38pxm/Zffbqndf6XW2U/Ef+feD08X1p4RjBVx+F6/y74apBJxfgLctWT/G1ZCbQIRMLG0MrS8d+HzMBx1xOfTj9cP/h/vh2rMAzDy5tBfD4Yh2F4iv+EX87CL2EY9sb836+6+PbbuTz/+Om6f/O3zqeJr3ofr3tvXeE73Dp8B+3aeW/zO9NmVixsIg4yb2VPCv9imeaCSKPiaPlSCf/ZQ9dxewgwPDppHMMQGl1+1G4w5g8b3TbvRkfd49HIP3HTZVm2zvS7GDzLtJrsSc0XMO70Cc5FPZ2pBYGdMbmVOVd0fu9qHCRsp5G1BoiWe9nycv+zd/pqmEuD0uG+jQFt2p1X3aOCaZY34y2D1isBiWB5lT3EOtLALBR3xMOuhtU4V8x02+4G7aOg0/kXhiyB8fOEia2b0dPaXbnDK8Rv5WMzUsnamlTEI2Qr/HS07RbWTy3olEkCWiv9C5evHaUbKb6vpdMJk4J/RbEJjP1aBLA6h+zbdICUI8kDMyRPWRRBZjFVHlEjCynhOe7aJBHGiHRMGFnoWmSpg7nNyT3tCZYfyIDbJzWvamp3RXJWE71fLdt5ae8WLAEbq1IPdOIdnlNpq6IstmaCz/FWD3qyEPVyLWlAY2szE7RamVa8aWzOhWraGFhqHkA3mWixTLQm7Q1d8Uornrt+JoVTJyysfAatllQRk7EyNuj6vr/L0Tu0IhwmIFWGHLL0V1FDb7CaioKpaqLLDCPiQrRwgogzol7542LRb398vt0IILzqk+8wJXhUgNSWhUwehI3JSDwCJxMmcyDdN+3r+KSTjAcDcfYqnXzKX4uHdx8eXr95PJWfj1njYtR+/DMcfDRf3EKKdKRciJVOI2WrkfCqvzUQ1GQTlrIxFuJTvQOXrjBtN/2mXzYhK9i8lHFCt3Bhn9zkWaY01sP6QlcXF0uy4BNqCvPf1j7jNDJlbMLSCsQ7lGM25OSn05mtGOIXRehybS082lYmmXD6lpvMrKzxdfXco4Hg2CRYc/hxNsOTzUDL+RxfFwdmLH0uDPY/p8GISQM1kW9TjHdE9h2m66q1KxsaUNcZh0NWFONapIXJL8Jsash75+V03GfDVYXjWsg17frZsBUxuRa1qlk/H3QlJdeDVmTsZ4MupeRayJWE/WzAijRdA1ixejHEA6roia69gr7HBy2KM9NPkcJfrss9+q+k+oesHWEsjsPptAq/CA8PDCiFxMCwnTGQ4kPojimVIRunLJzBcut/e447G1vfFp9sg8771nBms8LiVn2HdD5fRmfxGQOcz/8Dh0K4Yw==
sidebar_class_name: "get api-method"
info_path: docs/api/chat-message/chat-message-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all chat messages"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/chatmessage/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve all chat messages for a specific chatflow.

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"},{"in":"query","name":"chatType","schema":{"type":"string","enum":["INTERNAL","EXTERNAL"]},"description":"Filter by chat type"},{"in":"query","name":"order","schema":{"type":"string","enum":["ASC","DESC"]},"description":"Sort order"},{"in":"query","name":"chatId","schema":{"type":"string"},"description":"Filter by chat ID"},{"in":"query","name":"memoryType","schema":{"type":"string","example":"Buffer Memory"},"description":"Filter by memory type"},{"in":"query","name":"sessionId","schema":{"type":"string"},"description":"Filter by session ID"},{"in":"query","name":"startDate","schema":{"type":"string","format":"date-time"},"description":"Filter by start date"},{"in":"query","name":"endDate","schema":{"type":"string","format":"date-time"},"description":"Filter by end date"},{"in":"query","name":"feedback","schema":{"type":"boolean"},"description":"Filter by feedback"},{"in":"query","name":"feedbackType","schema":{"type":"string","enum":["THUMBS_UP","THUMBS_DOWN"]},"description":"Filter by feedback type"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"A list of chat messages","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"role":{"type":"string","enum":["apiMessage","userMessage"],"example":"apiMessage"},"chatflowid":{"type":"string","format":"uuid","example":"d290f1ee-6c54-4b01-90e6-d701748f0852"},"content":{"type":"string","example":"Hello, how can I help you today?"},"sourceDocuments":{"type":"array","nullable":true,"items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}},"usedTools":{"type":"array","nullable":true,"items":{"type":"object","properties":{"tool":{"type":"string","example":"Name of the tool"},"toolInput":{"type":"object","additionalProperties":{"type":"string"},"example":{"input":"search query"}},"toolOutput":{"type":"string"}},"title":"UsedTool"}},"fileAnnotations":{"type":"array","nullable":true,"items":{"type":"object","properties":{"filePath":{"type":"string","example":"path/to/file"},"fileName":{"type":"string","example":"file.txt"}},"title":"FileAnnotation"}},"agentReasoning":{"type":"array","nullable":true,"items":{"type":"object","properties":{"agentName":{"type":"string","example":"agent"},"messages":{"type":"array","items":{"type":"string"},"example":["hello"]},"nodeName":{"type":"string","example":"seqAgent"},"nodeId":{"type":"string","example":"seqAgent_0"},"usedTools":{"type":"array","items":{"type":"object","properties":{"tool":{"type":"string","example":"Name of the tool"},"toolInput":{"type":"object","additionalProperties":{"type":"string"},"example":{"input":"search query"}},"toolOutput":{"type":"string"}},"title":"UsedTool"}},"sourceDocuments":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}},"state":{"type":"object","additionalProperties":{"type":"string"}}},"title":"AgentReasoning"}},"fileUploads":{"type":"array","nullable":true,"items":{"type":"object","properties":{"data":{"type":"string","example":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAABjElEQVRIS+2Vv0oDQRDG"},"type":{"type":"string","example":"image"},"name":{"type":"string","example":"image.png"},"mime":{"type":"string","example":"image/png"}},"title":"FileUpload"}},"action":{"type":"array","nullable":true,"items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","example":"61beeb58-6ebe-4d51-aa0b-41d4c546ff08"},"mapping":{"type":"object","properties":{"approve":{"type":"string","example":"Yes"},"reject":{"type":"string","example":"No"},"toolCalls":{"type":"array","example":[]}}},"elements":{"type":"array"}},"title":"Action"}},"chatType":{"type":"string","enum":["INTERNAL","EXTERNAL"],"example":"INTERNAL"},"chatId":{"type":"string","example":"chat12345"},"memoryType":{"type":"string","nullable":true},"sessionId":{"type":"string","nullable":true},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"leadEmail":{"type":"string","nullable":true,"example":"user@example.com"}},"title":"ChatMessage"}}}}},"500":{"description":"Internal error","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","example":"invalid_request_error"},"message":{"type":"string","example":"The request was unacceptable, often due to missing a required parameter."},"param":{"type":"string","example":"id"},"type":{"type":"string","example":"invalid_request_error"}},"required":["code","message"],"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/chat-message/remove-all-chat-messages.api.mdx">
---
id: remove-all-chat-messages
title: "Delete all chat messages"
description: "Delete all chat messages for a specific chatflow."
sidebar_label: "Delete all chat messages"
hide_title: true
hide_table_of_contents: true
api: eJztV99v2zYQ/leIe9oAzXa6bF31NDdxAG9pm6XOuiEwClo8WWwoUiUp/4Cg/304SonlOLa3dA99qF9sicf77njfHT9X4PncQXwLScZ9js7xOcI0AodJaaVfQ3xbwQy5RTssfQbx7bSeRmAKtNxLo8cCYrCYmwUOlTrLuH/TOHEQgUCXWFmQHcRwjgo9Mq4UIzDWojmWGss4cwUmMpVJWEyVWfYggoJbnqNH60IgkvwU3GcQgeY5QgxSQAQWP5fSooDY2xIjcEmGOYe4Ar8uyMp5K/Uc6scxnbVYbHwOddQCfC7RrjcIFM+YUP611wupPFo2WzeJHvCdY27sekLu9vuPAFc8LxS9el2mKVr2Juw7hNx4ZsHTPnSHzjU1fFZy7fZjZ3c8O13mRMHx28no+u3wEiIY/dX+nB473CbBKXHAFUY7dATxYjCgr91ib1gnAh0Fc2WSoHNpqRQFnhjtUXvazYtCySTQvP/JkYtqNwsz+4SJJ6paagovmwBarx3DmTEKue5Wk8hah08Ep09FPNYLrqRgnTb43yJMjMDDXJMN+kfqLnT+I1prLBHifk4c3D3JkLU72ZI7VmqeJFh4PlMYMZN61EyUyLxhuXRO6jnj7L6RNxn3CDA8HQlWkGGz/oyk6u4QuW0OZ5PoNAIvfdg/au2bmp0eY5k2nqWm1OJb4b6iwv30dLN5tJor1nj+Vq+vpV7hIHxmSGo0UzsoA1Ij0O/oln4lRU03DdrFvWIorYIYMu8LF/f7hTWi53wppOn5DLl2S7Q9Lvu8kP3FyY5kubJGlAk9sMZpuOg2PuN+X5mEq8w4H58OBoN9ji7JiglcoDJFjto/+OtorfdEqIYzXcX1cMiEeH+RhhslGEHU/rgwNuceYvjtw2QngOHVmN3hmvHSZ6h9y2W2lD5jqVyhYAuuSmSnr06us19e5PObG3n2o178Wb6Uy8t3y5evVq/Vh5/5Dxfpyer34c0f7u9QS6lTE0JsqxamXysB2fBq/GQgJPhyrvmcuLilBSECKl1jetIb9AZtH/Ik9GErK4ahcMMxe18WhbHUc9uF7haXWJlzScuuMf91a5nSKIzzOdcdiH1q9XFC1WZMPEvhtrX1uPL9QnGpqbtCMlXL8W1tHkEsBfUJcY4Wq2rGHd5YVdf0uhFgRH0hHY0AAXHKlcMDce/q1T1x3eG6K4gDZSCG0BXPgeuK1IOQWzr5i2E7wvUgalcffzHoRq0ePdydPKf0YCVh/sfSfnfdDtvvWfe/zp4Q2pdcr7vw96HR5Cc9niEXaEMgzcIw3DedLTvXJWXwMMPPR5ejyQgi4Nvz7dE8CwBPRlRVjcXE3KGu64cAPT1TjHX9D2oIHYg=
sidebar_class_name: "delete api-method"
info_path: docs/api/chat-message/chat-message-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete all chat messages"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/chatmessage/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete all chat messages for a specific chatflow.

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"},{"in":"query","name":"chatId","schema":{"type":"string"},"description":"Filter by chat ID"},{"in":"query","name":"memoryType","schema":{"type":"string","example":"Buffer Memory"},"description":"Filter by memory type"},{"in":"query","name":"sessionId","schema":{"type":"string"},"description":"Filter by session ID"},{"in":"query","name":"chatType","schema":{"type":"string","enum":["INTERNAL","EXTERNAL"]},"description":"Filter by chat type"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Chat messages deleted successfully","content":{"application/json":{"schema":{"type":"object","properties":{"success":{"type":"boolean","example":true}}}}}},"400":{"description":"Invalid parameters","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","example":"invalid_request_error"},"message":{"type":"string","example":"The request was unacceptable, often due to missing a required parameter."},"param":{"type":"string","example":"id"},"type":{"type":"string","example":"invalid_request_error"}},"required":["code","message"],"title":"Error"}}}},"404":{"description":"Chat messages not found","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","example":"invalid_request_error"},"message":{"type":"string","example":"The request was unacceptable, often due to missing a required parameter."},"param":{"type":"string","example":"id"},"type":{"type":"string","example":"invalid_request_error"}},"required":["code","message"],"title":"Error"}}}},"500":{"description":"Internal error","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","example":"invalid_request_error"},"message":{"type":"string","example":"The request was unacceptable, often due to missing a required parameter."},"param":{"type":"string","example":"id"},"type":{"type":"string","example":"invalid_request_error"}},"required":["code","message"],"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/chat-message/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/chat-message/chat-message-api",
    },
    {
      type: "category",
      label: "chatmessage",
      items: [
        {
          type: "doc",
          id: "api/chat-message/get-all-chat-messages",
          label: "List all chat messages",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/chat-message/remove-all-chat-messages",
          label: "Delete all chat messages",
          className: "api-method delete",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/chatflows/chatflows-api.info.mdx">
---
id: chatflows-api
title: "Chatflows API"
description: "API for managing chatflows"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Chatflows API

This section contains the API endpoints for the chatflows service.

## Overview

The Chatflows API provides endpoints for managing chatflows resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/chatflows/create-chatflow.api.mdx">
---
id: create-chatflow
title: "Create a new chatflow"
description: "Create a new chatflow with the provided details"
sidebar_label: "Create a new chatflow"
hide_title: true
hide_table_of_contents: true
api: eJztV99v2zgM/lcMPjuJE6Rb53tZLlvvcuhuwZptwBV5YGwm1mpLmiSnDQz/7wfK+eGkadEd7rEvrUx9pEiRH6lUkJJNjNBOKAkxjA2howADSfdBkqFb5uo+uBcuC1xGgTZqLVJKg5QcitxCCEqTQdaepBBD4vXHW0UIweHKQnwLO1sW5iFYSkoj3Abi2woWhIbMqHQZxLfzeh6CoZ8lWfe7SjcQV5Ao6Ug6XqLWuUj8cb0flj2uwCYZFcgrt9EEMajFD0ochKANO+cEWd4VaQtjnRFyBSHQAxY6Z1E6eBct+0SdN8nFsDNcRP3Ou4jedNK3Uf/t8HIZXV70oQ5BYkHPW/q04Qu44guoQ+CoP6DD53WqmrEp6VxtqO3pQqmcUPKusNNykYvk/C5qcUebc1HWob/+hXJjJZdi9RJPUItfAEvMN+7IsSexVhMl2UzN6MG9BJ+go5Uym+exO1T/t91qwMqNwhlFWRZcleM/R7Or68/fIYRPX69nk9EfH/+ewZyP9YWcfkB31sBSmQIdVw066jhR0JE7g2gw7ESXncFw1h/G/Yt4MPiH/Sl1+r9b5TCF84A972qWMo2E4WJypiQvsFpJ29BhEEX874T+O8Zvww9smSRk7bLM8w2Er1R8peIrFX+ZinUIw3Nsm8g15iINhNSl289Wdm44GDyGf2OwJ1xADwk1YjZekMsUT1+trCcb8iyF3mHm8sg1azLWT9zS5BBD5py2ca+njUq71pWpUF2XEUp7T6aLooda9NZ9CE/cmBqVlol3ozEKddi2Gfd6uUowz5R18TCKoqcMXTMqSGlNudIFSbe313oi3HBLabpG+6GwTxifyOF5GLPQgyDcLq52afzr+8zfFbeqL4f3xcddRpum9LLOs2s87e7Sbi5Ml3YD4fbbbhnN96FJHKrupDc0hlr83wr2HG++j3ncyA5cPUvIHR/btDvi2fkKP6HNk+QScql8kk7YYIPRdPKoEEbTSbBUJihQ4krIVdAuXK7aBtbvRt1oO4Qw8R1rm4eRr9nRJLgptVaGOXBc4+26Zh4XKHjbNvD3R9tcJUykAmXriLMP49NIWm/V//CS3mbE0YPr6RyFHyU+jmpL6ZNnNDOMhVW1QEtfTV7XLP5ZEuf9dh7CGo3ABafgljtpRpiS8T3gjrg0xo27nRkfzfC8ZBcezXZmeKMxSrjzPIudtzrS9PPNjMm4fcoXKmUdg3x1/DcG/xOCtT3HvayCHOWqxBVjG5ucFDxm/gnTfVTbLZSblodV1SBm6o5kzeRoQnH8DfW8rut/AW9uVcE=
sidebar_class_name: "post api-method"
info_path: docs/api/chatflows/chatflows-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new chatflow"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/chatflows"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create a new chatflow with the provided details

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Chatflow created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}}},"400":{"description":"Invalid input provided"},"422":{"description":"Validation exception"}}}
>
  
</StatusCodes>
</file>

<file path="api/chatflows/delete-chatflow.api.mdx">
---
id: delete-chatflow
title: "Delete a chatflow"
description: "Delete a chatflow by ID"
sidebar_label: "Delete a chatflow"
hide_title: true
hide_table_of_contents: true
api: eJx9VE1v2zAM/SsCTxsgxOnQXXRasHRAhh6KNcMOQQ6MxcRCZUmV6GyB4f8+yHZTN/04WR+P5NPjo1vQlMpoAhvvQMGSLDEJFGWFvLf+r9idxGoJEnygiBm10qBA97jvIwgkMB4SqA08xSXYSkhUNtHwCdSmhR1hpLhouAK12XZbCQEj1sQUUw8wmUBArkCCw5pAgdEgIdJjYyJpUBwbkpDKimoE1QKfQkYljsYdoJMXj3milx+Q60VKwbtEKcd+mc/z552I4X1apKYsKaV9Y+0pF7h+K2pdkUiBSrM3pMVqKUwSxh3RGj3EXH9QyXkWe9+4Hvr1rfQrxxQdWkEx+ghdJ6EmrvxzH0AOuikozvoXrdEd5B7E45PCTbSgoGIOSRVFiF7PEjfa+BlXhC79pThDU2AwxfEKLuW8i143Zd6IISl0cppTFYX1JdrKJ1bX8/n8vUS3GSU0Hcn6UJPjc76JZ+5zl4dWTZ1zbnquCKMZ8n4AgRwXP3yskUHBzz/rXjLj9r4PN2xp0oAkFnerVxQXdyux91HU6PBg3EE8+1pC1nOAXc3mszlIKL1jLDnnH5276NVcrMR9E4KPDPJC/aniIIFqNPk6DfBvL64z/+AT1+gmJV7N6uUr2p4XOf5wsEc9mf5xESwal43Yc21HV02nWoIyOs927nK+atsdJvodbdfl48eGYp73rYQjRoO7LPamBW1SXmtQe7SJPmD66dc475/FdIDfIToeojvlxqBt8g4kPNBp+H902+m8LG9ub9Y3IAFfuunCPT3lN1O37YBY+wdyXXeuxHmfi3Xdf9lE0IY=
sidebar_class_name: "delete api-method"
info_path: docs/api/chatflows/chatflows-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete a chatflow"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/chatflows/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete a chatflow by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Chatflow deleted successfully"},"400":{"description":"The specified ID is invalid"},"404":{"description":"Chatflow not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/chatflows/get-chatflow-by-api-key.api.mdx">
---
id: get-chatflow-by-api-key
title: "Get chatflow by API key"
description: "Retrieve a chatflow using an API key"
sidebar_label: "Get chatflow by API key"
hide_title: true
hide_table_of_contents: true
api: eJytVd9v2zYQ/leEe1oBOZYNp021l3ppk2VLt6BxUWCGH87SyWIrkSx5cmII+t+Hk2THTpwgA/Ykivzu+N0PfldDSj5xyrIyGmL4QuwUrSnAIMmRs8LcBZVXehWgDqY3V8EP2kAIxpJDMblKIYYV8XkP/m0zterPFsO48hDPYevHwyIET0nlFG8gntewJHTkphXnEM8XzSIEiw5LYnK+BSihZJFzCEFjSRADWtVRcPSzUo5SiNlVFIJPcioR4hp4YwXp2Sm9giZ8FGIfRYDem0QhUxrcKc4DzmkXMwgZR94a7cmL03EUyefQ1W2VJOR9VhXBLiMQQmI0k2bBo7WFStqD4XcvRvVTpmb5nRKGEKwTN6y6K1X6NJoQ6B5LW8hWOn4fZSOiwdvkdDKYLKPR4H1Ebwfpu2j0bnKWRWenIwm/y9xLnj5vpH4XbeAhSAI+Ih/J5b5N3XSptYXZ0D7TpTEFoZZT5W+qZaGS46ddKY9F2YRt1ywNnxudqdVrmKBV/wGssdjwAbFnsd4SJfnMzOieX4NPkGll3OZl7BY1+nW7GotxZ3DEUFelPKbz36ezi+u/v0EIn79ez66ml5/+msFCrnUkvfwR+aiDzLgSWboGmQasSjqgM47Gk0F0NhhPZqNJPDqNx+N/hE9l0//dq4SpuAWc795bI9uTY49sllPgLSUqU5RuNShQPlB6jYVKoTWcPDXcOg+04SAzlW6hp8fuuNJMTmMRkHPGtRRL4tz06iZPU1QohuFOzYZd+w7r7tuAiJtbb6WrcgXEkDNbHw+H1pn0xHOVKnPCOaH2d+ROUImT4XoEjzXqxpm0SuQn6JxCE+77jIfDwiRY5MZzPImi6DlH14IKUlpTYWxJmnf+9sT4VgSp05x9Sd6VW26EXmHbN9yCIOwXF9sm+OPbrE2d0plpzR9V2Uv1nlCUimbGBSVqXMmkeRgYIUg+O9joJDqJenHFpH2J/UiYttmcXgW3lbXGSbUOs7+fcenPEpUc+w7+4eBY+FvjuUS9d8Ul8cM8XG72JuFBLPWD9L92kvYpZrrnoS1QtcrY0q/7ntufoOHD/Iv71SIE6QLB1fUSPX11RdPI9s+KRIXmixDW6BQupRjzGlLlZZ1CnGHh6YUYfvnSD9k3waum5jPh9Juohfcai0r+IAShv5vojYhYTpiSa1l2h9MkIct7Zk8GqjTy7q1efppJjg7b91G7tt6PUqrrDjEzP0g3zY4hy78QbJp/AXJPGa8=
sidebar_class_name: "get api-method"
info_path: docs/api/chatflows/chatflows-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get chatflow by API key"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/chatflows/apikey/{apikey}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a chatflow using an API key

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"apikey","required":true,"schema":{"type":"string"},"description":"API key associated with the chatflow"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}}},"400":{"description":"The specified API key is invalid"},"404":{"description":"Chatflow not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/chatflows/get-chatflow-by-id.api.mdx">
---
id: get-chatflow-by-id
title: "Get chatflow by ID"
description: "Retrieve a specific chatflow by ID"
sidebar_label: "Get chatflow by ID"
hide_title: true
hide_table_of_contents: true
api: eJytVVFv20YM/isCnzZAjmXDaVPtZV7SZB7SLmhcFFiQB1qirGuku+sd5UQQ9N8HSrZrx06QAXvS6fiR95F3/NhASj5xyrIyGmL4QuwUrSjAwFtKVKaSIMmRs8I8Bos6mF1ACMaSQ3GYpRDDkvh8jfijnqUQAuPSQ3wHG0cP9yF4SiqnuIb4roEFoSM3rTiH+O6+vQ/BosOSmJzvAErIWOQcQtBYEsSgJLSjH5VylELMrqIQfJJTiRA3wLUVlGen9BLa8FliG4qSgZznyFujPXnxHUeRfPY9bqskIe+zqgi2CUMIidFMmgWP1hYq6QzD716cmkNCZvGdEoYQrJMwrPojVXpIOgR6wtIWspWOP0TZiGjwLjmdDCaLaDT4ENG7Qfo+Gr2fnGXR2elIsuyL81qkT7XkflmYR8FLDS6Qj5Rs16dp+wrawtS0y3RhTEGoxar8TbUoVHLcilY9UH0syzbsHsbC8LnRmVq+hQla9R/AGoua94i9iPWWKMnnZk5P/BZ8gkxL4+rXsRvU6LfNaizOvcMRR12V0i/nf07nl9d/f4MQPn29ns+mVx8/z+FejnWETOkF8tEAmXElsrwaZBqwKmmPzjgaTwbR2WA8mY8m8eg0Ho//ET6VTf/3qJKm4g6waTloW9meHGuyeU4boaE0mF0EygdKr7BQKXQ+k0OfbStrw0FmKt1BT4+Fn2kmp7EIyDnjOnYlcW7WuiVdKRoTw3CrVcNGpS2IXrnVRo0qV0AMObP18XBonUlPPFepMiecE2r/SO4E1RCtGq5G8Fx6bpxJq0R+gj4otOFuzHg4LEyCRW48x5Moil4KdC2oIKUVFcaWpHkbb0dfb0WAeo3ZVdnt9cqJsBbOrmc7EITrxeXm0v/6Nu/qpXRmOvdnt+qD6c3sgOL0ZhZkxgUlalwqvQx+zoAQpJ49bHQSnURrMcWk67y1yk+7ak5nwW1lrXFyRfvV3624vMcSlZh9D/99zyz8rfFcot454or4cKbtpdH8VPm3TcR1bZmeeGgLVJ0Edryb9QvbnYYhxCqVmSg3LqamWaCnr65oW9n+UZEozN19CCt0ChdS+LsGUuVlnUKcYeHpFdK/fFnPyV+D3cH3AtH1JupaLgmLSv4ghAeq+7nbigblhCm5jkhvmCYJWd5xOZiH8i63/Xb1cQ4h4P5rfPb6uuhH6TRNj5ibB9Jtu2XH8i8E2/ZfN2r0Zw==
sidebar_class_name: "get api-method"
info_path: docs/api/chatflows/chatflows-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get chatflow by ID"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/chatflows/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a specific chatflow by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}}},"400":{"description":"The specified ID is invalid"},"404":{"description":"Chatflow not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/chatflows/list-chatflows.api.mdx">
---
id: list-chatflows
title: "List all chatflows"
description: "Retrieve a list of all chatflows"
sidebar_label: "List all chatflows"
hide_title: true
hide_table_of_contents: true
api: eJytVV1v2koQ/SvWPBswiLSp+3JR2vRylbZRQ1XpIh4Ge4Bt1rt7d8ckluX/Xo2B1FAapdLlhfXOmdmzZ+ejhpxC5pVjZQ2k8IXYK9pShJFWgSO7ilDrKNsgr7R9CBCDdeRR4NMcUhDUVcfKuA6QzuGnxyKGQFnpFVeQzmtYEnryk5I3kM4XzSIGT8FZEyhAWsMoSeTvmNZdmWUUwqrU0dPxEENmDZNhwaNzWmWtYfA9iFMNIdtQgbLiyhGkgN5jBTEopiJ09u3yO2UMMTgv4VntqKi8gwnslVlDDPSIhdOylY/eJKshUe9VdjHujZfJsPcmoVe9/HUyfD2+XCWXF0NoYjBY0PORPlai4bW2D4IX3d4h4/M+dSPYnJy2FXWZLq3VhEasKtyWS62y81Z06p6qc7ds4vYBl5avrFmp9UuYoFN/ADaoKz4i9ltscETZZmZn9MgvwWfItLa+eh57QA3fHlYjcd45nHE0ZSF5ffX3ZHZ98/kbxPDx681sOvnw/tMMFnKsJ2TK3yGfDbCyvkCWrEGmHquCjuiMktG4l1z2RuPZcJwOL9LR6F/hU7r8f48q11TcAg6lC438Yrg4V31Tw+QN6oi8t771L4g3Vup/TW3doFQzDLp9IpDfkg9tzZdeQwobZhfSwcB5m/cDl7myfd4QmvBAvo9qgE4NtkOIT86/9TYvM/mIdkGhibsx08FA2wz1xgZOx0mS/C7QjaCinLakrSvI8FO8TpO6k66xawDdVvWkvZwo12thUlAtCOL94vrwIv98m7VSKbOyrfuJ5CGa3E5/oTi5nUYr66MCDa6VWR+1XtFzBxv2k36y74CYtWWx6zIwadWcTKO70jnr5XWO1e8qLslSoBJz2MH/OjILf2cDF2g6R9zIZDgdC0fXqH+25peMlL2yTI88cBpV251a1vU+tU4Giry0bNb1EgN99bppZPu/kqTs54sYtugVLkXwuRTnhjAn3+biPVUiU5aRE3G2qMt2NpwOEMmJpzT/8H4GMeBxJpy8fBv9MGlM1Yld1zvEzN6TaRqI9yRYvqFZNE3zA6tNijg=
sidebar_class_name: "get api-method"
info_path: docs/api/chatflows/chatflows-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all chatflows"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/chatflows"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a list of all chatflows

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}}}},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/chatflows/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/chatflows/chatflows-api",
    },
    {
      type: "category",
      label: "chatflows",
      items: [
        {
          type: "doc",
          id: "api/chatflows/create-chatflow",
          label: "Create a new chatflow",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/chatflows/list-chatflows",
          label: "List all chatflows",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/chatflows/get-chatflow-by-id",
          label: "Get chatflow by ID",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/chatflows/update-chatflow",
          label: "Update chatflow details",
          className: "api-method put",
        },
        {
          type: "doc",
          id: "api/chatflows/delete-chatflow",
          label: "Delete a chatflow",
          className: "api-method delete",
        },
        {
          type: "doc",
          id: "api/chatflows/get-chatflow-by-api-key",
          label: "Get chatflow by API key",
          className: "api-method get",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/chatflows/update-chatflow.api.mdx">
---
id: update-chatflow
title: "Update chatflow details"
description: "Update the details of an existing chatflow"
sidebar_label: "Update chatflow details"
hide_title: true
hide_table_of_contents: true
api: eJztV1Fv2zYQ/ivCPW2AHMuG06bay7yk2Ty0W9A6KLDAD2fxZLGRSJak3BiC/vtwkmzLjhOkwx7zkojk3fGOvO/j5woEucRK46VWEMOtEegp8BkFgjzK3AU6DVAF9CCdl2oVJBn6NNffIQRtyCI7zgTEUDaul/tljysH8R1sPRwsQnCUlFb6DcR3FSwJLdlp6TOI7xb1IgSDFgvyZF1jIDkngz6DEBQWBDFIASFY+lZKSwJib0sKwSUZFQhxBX5j2Mp5K9UK6vCovm16wewKeD8ORM7/psWGvROtPCnPn2hMLpOmvOFXx87V42308islHkIwlg/DS3K8KsXjVEKgByxMzlNi/C5KR0SDN8n5ZDBZRqPBu4jeDMTbaPR2cpFGF+cjzr0t+blIHzdc0TUfeB0CV3aF/sRB9H2quj0Xk+sN9TNdap0TKl6V7qZc5jI5vYpG3tPmVJV12Fz3UvtLrVK5ekkmaOQPGCvMN/4gsSdtnSFKsrme04N/iX2Cnlbabp633VqNftl+jdm5dTjhqMqCUXD5x3R+/eHvLxDCx9sP89n09/d/zWHB21pCT+IK/ckAqbYFeu4a9DTwsqCDdMbReDKILgbjyXw0iUfn8Xj8D+fTwvH/jcplSt8Y7HBe1zxtyRmtXNv+4yjif08gr0sscGWSkHNpmecbCF+h9wq9V+j9F+hNTqFtnlHgDCUylSSC2VUgXSDVGnMpAm2DpRYbniqkc13vTKLJM6BV2gepLpVg0/NTO86UJ6swD8habZuEC/KZZmlgygao/I7HMNzpgWElRQ2sCex6++KXNocYMu+Ni4dDY7U4c74UUp/5jFC572TPUA7RyOF6BMfP+43Vokx4ELRBoQ77MePhMNcJ5pl2Pp5EUfRUoA9sFQhaU65NQcrv4vU0zGfmpJZ2+kpmd+O8I3TipIFxYwRh93G97YM/v8yb82Ku+7QXJO+3LdGy2suoa6eTevTUZyfGW5+BWv2055x2vGeZfdsekUsbqEcg3cSOJNrxIRG0c3uwn0T0FtB93B4A9TREjnD3JDqlSnVzSUdwcsH0ZvaoEaY3syDVNihQ4aqvfx2EwF3bmo3OorOoe8UwaSivu4dp07PTWfC5NEZbBsJhj/f7momgQMnLrjX/9WCZu8Ro5wtUvS060b7NbKvcj2vpydsf0/nddXh68EOTo2weoqaIqgN1X+SHEEvBUp9BxktVtURHtzava57+VhJf/d0ihDVaiUu+hbsKhHT8LSBOMXf0TPI/ferk/89BX88/kWg3iYpVxhrzkkcQwj1t2p8TNb8EGaEg2yTSLly22w3m7L53fKRNmGBaj2mSkPHP2i56pHhzO2cq6H55FFqwi0U+cP7b5Kib6huGaeYqyFGtSlyxbRuSWwIPeeeIZ5qiTp5CVbUWc31Pqq53h+J5zOdS1/8C4UazEg==
sidebar_class_name: "put api-method"
info_path: docs/api/chatflows/chatflows-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update chatflow details"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/chatflows/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Update the details of an existing chatflow

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}}}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Chatflow updated successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","example":"d290f1ee-6c54-4b01-90e6-d701748f0851"},"name":{"type":"string","example":"MyChatFlow"},"flowData":{"type":"string","example":"{}"},"deployed":{"type":"boolean"},"isPublic":{"type":"boolean"},"apikeyid":{"type":"string"},"chatbotConfig":{"type":"string","example":"{}"},"apiConfig":{"type":"string","example":"{}"},"analytic":{"type":"string","example":"{}"},"speechToText":{"type":"string","example":"{}"},"category":{"type":"string","example":"category1;category2"},"type":{"type":"string","enum":["CHATFLOW","MULTIAGENT"]},"createdDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","example":"2024-08-24T14:15:22Z"}},"title":"Chatflow"}}}},"400":{"description":"The specified ID is invalid or body is missing"},"404":{"description":"Chatflow not found"},"500":{"description":"Internal error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/create-document-store.api.mdx">
---
id: create-document-store
title: "Create a new document store"
description: "Creates a new document store with the provided details"
sidebar_label: "Create a new document store"
hide_title: true
hide_table_of_contents: true
api: eJztV0tv2zgQ/ivEnOn4kdhpdVpv4l24SFOjdlB0Ax/G4thmK5EqSdk1DP33gpT8thNgC+xlcxJFDr/hDGc+zqzB4cxC9AxCx3lKytWs04ZgzMFSnBvpVhA9r2FCaMh0czeH6HlcjDkIsrGRmZNaQQR3htCRZcgULdkGjAUwtpRuztycWGb0QgoSTJBDmVjgoDMy6EH6AiKIA8x9tX0YjsLB0I+crPtTixVEa4i1cqScH2KWJTIO++vfrD/JGmw8pxT9yK0yggj05BvFDjhkxmtzkqxflWJPxjoj1Qw4TLVJ0UEEeS4FHJv5pOSPnJgUpJycSjJsqk0w7dBkKDgoTOmchkPER0yJ6ekFjAPZ16Dud3+XERONgox9He2hFGRorY4lOhK7ezyE5eVHMLTsw/DTI6sQCw7LORl6snTW1Yf6BgnGZFnYcUYJk5bllsQLyqxDl5+1jFSe+iDvfRyMvgKH4dfHu+rTf/zbj0bdhx5weOx9AQ5Pg2Hv86hcKce9ezgJ+WFQd9nTC4qdNiGG77SaytnrPijl8jIhtqFVAr3qa0onJIRUs9/TtoV5QZWhWBvxERXOyPyeuhKKpSXWCzpLahD36M5m1TZvBTqqOZnSaX6gI4ZKML/qA02di7MlWlbp8mrzTPxnahO0jlUKoSg4OOkSr+6QEIuiKDlRGp9YzuQUJmymlS25rdVo+M9RwOZxTNZO8yRZbUw8jlv+xq5v7PrGrm/s+n9m14LDzTkC7asFJlKwqh5lE1+QFhza54UdGYUJs2QWZBgZo03Qm5Kba1/tZtoG4kRfVUP9sASvbwi53G9DGZ6bBCKYO5fZqF7PjBZX1uVC6is3J1R2SeYKZR0zWV80T0nAaJHHIUBKUCj4PmZUryc6xmSurYtuGo3GJaAHL8UELSjRWenmCm+vbxj6l6J8DPa7h+0te43evCAGUSUEvBr8tbn7D19GwW3+Bfq86wR6PzHNEtq8NXA9xXftaeem1r5t3tZu2p1WbXI9jWut+H3netrp4BQ7sHk3LmXRdnpL5LupPbbdTW5YcUt+Z3hpjymPSWS3dDbpd8sH+QmtRuum1rittd6Pmu2o3Yxa764at81/4CijLgsWHKSa6nAdR2nAwuFZd9A/cU930A/UEvhEqtlRmlnOYp0kFELM8pCWGwnLpGLdEKHdfvCTsSVo86px1agKD4xD4VFd0kacDfMs08bnymEC7Ad9cC9Kv2xL8T8Oln0I+YRLUe2pKPvWs23rsfV7jee/b3er2Hf009WzBKUKLOiNWlc8cNKK+yCrWnKfmF5ivZ6gpSeTFIWf/pGT8W36mMMCjcSJv8/nccFhTj6MA3V8p1X5THgTaiN/Di+e5P48J5WeJ4ZyRzeOKXMvyo73OG3waTjyOVz16qkWfo/BpY9xXEIEoekPEeIFwtwaElSzHGdetsT014WHhHFEEMGqagnVau+E63UpMdLfSRUF8MoU5/+hGBdF8QuaVd9A
sidebar_class_name: "post api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new document store"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/document-store/store"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Creates a new document store with the provided details

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store"},"name":{"type":"string","description":"Name of the document store"},"description":{"type":"string","description":"Description of the document store"},"loaders":{"type":"string","description":"Loaders associated with the document store, stored as JSON string"},"whereUsed":{"type":"string","description":"Places where the document store is used, stored as JSON string"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store"},"vectorStoreConfig":{"type":"string","description":"Configuration for the vector store, stored as JSON string"},"embeddingConfig":{"type":"string","description":"Configuration for the embedding, stored as JSON string"},"recordManagerConfig":{"type":"string","description":"Configuration for the record manager, stored as JSON string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the document store was created"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the document store was last updated"}},"title":"DocumentStore"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully created document store","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store"},"name":{"type":"string","description":"Name of the document store"},"description":{"type":"string","description":"Description of the document store"},"loaders":{"type":"string","description":"Loaders associated with the document store, stored as JSON string"},"whereUsed":{"type":"string","description":"Places where the document store is used, stored as JSON string"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store"},"vectorStoreConfig":{"type":"string","description":"Configuration for the vector store, stored as JSON string"},"embeddingConfig":{"type":"string","description":"Configuration for the embedding, stored as JSON string"},"recordManagerConfig":{"type":"string","description":"Configuration for the record manager, stored as JSON string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the document store was created"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the document store was last updated"}},"title":"DocumentStore"}}}},"400":{"description":"Invalid request body"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/delete-document-store.api.mdx">
---
id: delete-document-store
title: "Delete a specific document store"
description: "Deletes a document store by its ID"
sidebar_label: "Delete a specific document store"
hide_title: true
hide_table_of_contents: true
api: eJyNVE1v2zAM/SsCTxugxe7QXXxagHRAhh6KpcMOQQ6KxSRCZUmVqGyB4f8+0HbafLTDLrYlU4+PfE9sgdQ2QbUE7evcoKNPiXxEWElIWOdo6ADVsoU1qohxmmkH1XLVrSRoTHU0gYx3UMEMLRImocQRSPRAYn0QhpKYz0CCDxgVH5hrqED3R2Zj+KJPKyGoqBokjKnPaxg8KNqBBKcahAqMBgkRn7OJqKGimFFCqnfYKKhaoEPgqETRuC1I2PjYKIIKcjYauiviR7o9AebJxUVMwbuEiRE/lyW/zs8tcl1jSpts7UEMpeiL2jnZbXl7fXZ23iLnSWx8dj27L28lmzvC6JQVCeMeo8AYfYSuk9Ag7fxrN/sGskZQnAtaDM/W6A5YWoYZOpyjhQp2RCFVRRGi15NEWRs/oR0ql35jnChTqGCK/Q1ctu8hep1rXozcoJOnmFVRWF8ru/OJqtuyLN8DuucooXGP1oehOSPeiRUXrPIgyqkhX0TnjDCagddDEMjx49vRCt9/PfbdM27j++OGLF6bYfowv+I5fZiLjY+iUU5tjdteaJ6kqL212LckSaHcqyuSME5M+45OGZglGEBvJuWkBAm1d6RqYkqj2Y/hYpFD8JFAXgh2KhJIwEYZ/p2G8K9nv7nk4BM1yp2kGK6uUCIFrM3G1Jc2vmhB29NER/977UdxCP9QEawyjn3eV9GObr0aPxKO78poHkbsH45r27VK+DParuPt54yRB9RKwl5Fo9Ys47IFbRJ/a6g2yib8RwUffoyT5KO4HgXvcB83lTuwispmXoGEJzwM86lbnV7N2d393eMdSFDnbr1wZ0/8Tei2HSIe/RO6rnvJRLzmZF33F64d/cM=
sidebar_class_name: "delete api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete a specific document store"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/document-store/store/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Deletes a document store by its ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document Store ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully deleted document store"},"404":{"description":"Document store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/delete-file-chunk.api.mdx">
---
id: delete-file-chunk
title: "Delete a file chunk"
description: "Delete a specific file chunk in a document store"
sidebar_label: "Delete a file chunk"
hide_title: true
hide_table_of_contents: true
api: eJytVE1v2zAM/SsCTxugxenQXXRasLRAhh6KpcMOQQ6qxSRCZUmV5GyB4f8+UHa+0x7anmzZ5HsUH/kaSHIZQcxAubKu0KYvMbmAMOcQsayDThsQswYeUQYMozqtQMzm7ZyDwlgG7ZN2FgSM0WBCJln0WOqFLtlCG2TlqrZPTFsm2ZaAdQQcnMcgKX2iQIDKALfa4A/KAQ5eBllhwhBzBZpovEwr4GBlhSAgI00UcAj4XOuACkQKNXKI5QorCaKBtPFdaNB2Ce1Z3duqpoTFJmNo+UUu46TC8EFkdxnsFbbctw8iy/3MXHOCi97ZiJEQvg6H9DjOm9ZliTEuamM2rFNFdToSx/WllIldS6MVm4yZD26tFaou9vo89qTh1iW2cLXNCd8ugycMVhoWMawxMAzBBWhbDhWmldvPTh4Zmk8ojoe5yNXHounnpS2arZpt0fStboEmnhi6cauDAQGrlHwUReGDU4OYaqXdIK1Q2vgXw0DqQnpdrK/gtPn3wam6pENfdhZ6jymKwrhSmpWLSVwPh8OXgO4oiilco3G+W58e72BDpzQTnaSHe7obEWKEfnTo3AUB719uXahkAgE//zzkxmq7cDldJ4Pnmo3uJ2d1ju4nbOECq6SVS22XJ+seOSudMZhbEjmTVu0iIvnDKHd0RMAkQQd6NRgOhsChdDbJMlFJ/X5sw9m09t6FBPxEsEORgANWUtPv2IV/P/pNV/YupkraA4qdo+2N7PTWTa4MbXqbAfbqJPyXCm+ktrQD+RpNP8lntsw7Z4jAQezdTxyYk9hax5wDTReBNM2jjPg7mLalz881BnL1OYe1DFo+ksizBpSO9K5ALKSJ+MplP/3qXekzOzfQFy7Wf5R2QxpLU9MJODzh5sDKaU3eV8feW99UyK6V769kZ7xvKmSrYzs/tLrxzd3Nww1wkMcrfrLSWc+L+E3TRTy4J7Rtu6NLdCaytv0PjvbENg==
sidebar_class_name: "delete api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete a file chunk"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/document-store/chunks/{storeId}/{loaderId}/{chunkId}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete a specific file chunk in a document store

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"storeId","required":true,"schema":{"type":"string"},"description":"Document Store ID"},{"in":"path","name":"loaderId","required":true,"schema":{"type":"string"},"description":"Document Loader ID"},{"in":"path","name":"chunkId","required":true,"schema":{"type":"string"},"description":"Document Chunk ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully deleted chunk"},"400":{"description":"Invalid ID provided"},"404":{"description":"Document Store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/delete-loader-from-document-store.api.mdx">
---
id: delete-loader-from-document-store
title: "Delete specific document loader and associated chunks from document store"
description: "Delete specific document loader and associated chunks from document store. This does not delete data from vector store."
sidebar_label: "Delete specific document loader and associated chunks from document store"
hide_title: true
hide_table_of_contents: true
api: eJzFVU1v2zAM/SsCTxugxenQXXxagLRAhh6KJcMOQQ6sxcRCZcmj5GyB4f8+SHaaz+6wFdgpkf30SL5H0i0E3HjIl6Bc0VRkwwcfHBOsJHgqGtZhB/myhSdCJp40oYR8uepWEhT5gnUdtLOQw5QMBRK+pkKvdSH2dMI4VMQCrRLovSs0BlKiKBv77MWaXXWApsgjsSi1F8qRF9YFoXpihQF7+JaK4HgAgwRXE2PMYqYghx7+kILes6umA/k8VSWhRsaKArFPZemYe42hBAkWK4IcEvFMgQSmH41mUpAHbkiCL0qqEPIWwq7uoaztBroLMfYVpahiNoVOXo3Vi/NGwfqiU7RV5PO1s558pPg4Hsef04vzpijI+3VjzG6QWe3tumJMjHx7jWdmt2i0ErOpqNlttSLVY28vsWfKRIPXrrHpwqfr5IHYohGeeEssiNkxdJ2EikLpDpYnb2N3QnbayllfUtYOxnZZu5e9g9jlkbfvhoYN5FCGUPs8y2p2auRDo7QbhZLQ+p/EI9QZ1jrb3sC5EY/sVFPEw5Bscv3AmWeZcQWa0vmQ347H49eIHiJKKNqScXVvwMB3NJXz2B+9u8ez+dIuMSIMbRTPPQjk8OfecYUBcvjyfZHk1Hbt0nUdDF06NXmcXeQ5eZyJtWNRocWNtpuzhvFSFM4YSpJ4mVbAHuGFtmKSFJ1E4mhBT3ozGo/GIKFwNmARYkrDsOzhYt7UteMA8sywY5NAAlWo42vfwz+fvI4l186HCu1RiDfbYudatakesuG/rsqhNQL9ClltUNs4dknDdhiei++AHHYUSMgPmzF/WVwrCbGf4822fUJP39h0XXz8oyGO346VhC2yxqfYVssWlPbxv4J8jcbTH4R693XYie/F5UZ9pZrhIdpd7Co0TTyBhGfaHe32OJj/lsdh2f5VIi/6davjVTa9e7hb3IEEPB3ms+FNOl4N0LY9YuGeyXbdS7wQzzFY1/0GdB7C9g==
sidebar_class_name: "delete api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete specific document loader and associated chunks from document store"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/document-store/loader/{storeId}/{loaderId}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete specific document loader and associated chunks from document store. This does not delete data from vector store.

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"storeId","required":true,"schema":{"type":"string"},"description":"Document Store ID"},{"in":"path","name":"loaderId","required":true,"schema":{"type":"string"},"description":"Document Loader ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully deleted loader from document store"},"400":{"description":"Invalid ID provided"},"404":{"description":"Document Store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/delete-vector-store-data.api.mdx">
---
id: delete-vector-store-data
title: "Delete vector store data"
description: "Delete vector store data for a specific document store"
sidebar_label: "Delete vector store data"
hide_title: true
hide_table_of_contents: true
api: eJylVE1v2zAM/SsCTxugxenQXXxagHRAhh6KpdsORQ6sxSRCZUmV6GyB4f8+0HazfLS77GRJfnp8JJ/YAuMmQ/kAJlRNTZ4/ZA6JYKUhU9Uky3soH1p4JEyUZg1voXxYdSsNhnKVbGQbPJQwJ0dMakcVh6R6DmWQUa1DUqhypMqubaVewgwQ0BAiJRSShYESTE/zo2dZCmKOjKAhYsKamFLu1VgJGZG3oMFjTVCCNaAh0XNjExkoOTWkIVdbqhHKFngfBZU5Wb+B7kL+i6w+qFrMQVJMlGPwmbIwfJxO5XN6b9lUFeW8bpzbq0G8GfNOoT4phwS9fo1j4XforFGLuYop7KwhM2CvL7FnOn1gtQ6N7y98ep2cKXl0KlPaUVKUUkjQdRpq4m34W/O+yNJdKE6tUAxJDOvWmg7EGkI29KJJDkrYMsdcFkVMwUwyN8aGCW8Jff5FaYK2wGiL3RWcF/4uBdNUshkVQqePOcuicKFCtw2Zy+vpdPoW0a2glKEduRAHh418R1Zeih+Gdh4b+mAPiQijbWQ/gECPiy8h1chQwtef930NrV+H/rplR5ftmd0tLnTO7hb9m6jR48b6zdmLyFpVwTnqS5K1Qm8OiKysV7O+ojMhlhYMpFeT6WQKGqrgGSsWSeOzeIGrZRNjSAz6rGHHTQINVKOV33mAfz75LSnHkLlGfxTirad/nnrbyyPP/zMuxkYx/eYiOrRenN9n1I7+vRhlUqiDg0FDaY2MN3GUoNv2ETN9T67r5Pi5oSQjb6Vhh8niozT2oQVjs6wNlGt0mf6R27tv4xR6ry7HyhsZjIfo9yIXXSM70PBE+2G2davjJzu/ub25vwENeOrfM7/2wl+lbtsBcR+eyHfdIRLLXoJ13R+1ORym
sidebar_class_name: "delete api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete vector store data"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/document-store/vectorstore/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete vector store data for a specific document store

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Document Store ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully deleted data from vector store"},"400":{"description":"Invalid ID provided"},"404":{"description":"Document Store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/document-store-api.info.mdx">
---
id: document-store-api
title: "Document Store API"
description: "API for managing document stores, collections, and documents in AnswerAgentAI"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Document-store API

This section contains the API endpoints for the document-store service.

## Overview

The Document-store API provides endpoints for managing document-store resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/document-store/get-all-document-stores.api.mdx">
---
id: get-all-document-stores
title: "List all document stores"
description: "Retrieves a list of all document stores"
sidebar_label: "List all document stores"
hide_title: true
hide_table_of_contents: true
api: eJy9Vk2P2zYQ/SvEnBnbCdqLTjWybuBg4xqxF0Gw8GEsjS0mEqmQI28NQ/+9GEp2/KHdRREguogSh++R88E3B2DcBkgeIXNpXZLlN4GdJ1hpCJTW3vAekscDrAk9+XHNOSSPq2alIaOQelOxcRYS+EzsDe0oKFSFCazcRmFRqCOsirABNLiKPMqqaQYJbInHRXHXWS2ORp5C5WygAMkB3o1G8rokHJ9obilSZ5ksyyKsqsKkkW/4LcjKA4Q0pxJlxPuKIAH0HvegwTCV4ey/W3+jlEFD5WXXbNr9mOzMJrA3dgsaNs6XyJBAXZsMrv3zYM2PmpTJyLLZGPJq47zinK62D40GiyX1MVwizrAkOX4/xoXta1B3P7+eRywcZuTD62j3raHCEFxqkClTT4bzHljdvjKFQX1c/DNTHWKj4SknTw+Bel19yTcvMKWg4ooeEmWCqgNlL5AFRq57T0a2LqU6Jp/my6+gYfF19r57TWcfZLQc309Aw2zyBTQ8zBeTz8t2ph1P7uCmVhaR7nlP7yhl52MtvHd2Y7av+6C1q9vCOqVWC/Sqr6lcU5YZu/01thPMC1SeUuezT2hxS/7X6FooVbZYL3CmniQJ75B7q+pUtxkyvWFT0m19IJNCmymZlUSzfXn2hEF1XEJbV9lvoy0wsOoIoWk0sOFC6C4uVmjk0fBn3306tUzeYqEC+R15Rd47H7FK4tx1d7XchSgaAMNLwRi22SuqIctDFI3aF5BAzlyFZDisvMsGgevMuAHnhDY8kR+gGWJlhru3t3XtXVanMeYtKDT6HDMZDguXYpG7wMkfo9HoOaB7sVIZ7ahwVeu5Du9M5RYiCu39fq51p8AJoxwvmkHSGYHuBn8fw/nxyzJ6zdiNi8uvIqFiKNR4Pr3Z53g+jdkdU9rY7bWyaZW6oqDokqBjZhwtgjJWjaNHxwIsIWhB3w5Gg1GniZhGTWz1BY7malFXlfMS28uAnQdJ7sISjUyH1vyvi2k5cuUCl2jPKO5FovvbgCuBOkn2/+gkusgw/cvDqkBjY9nJEQ5dlt60NXLXd+2NpI1YHA5rDPTgi6aR3z9q8tLyrDTs0BtcS/QeV42GnETXYmJ/p704ME2pErftsKhjH3HdbEiCncrnw2QJGvAyra7SKKIfuxK7P8M+HFqLpftOtmlAd5tg+YZm1TTNf9nlaT0=
sidebar_class_name: "get api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all document stores"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/document-store/store"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieves a list of all document stores

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"A list of document stores","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store"},"name":{"type":"string","description":"Name of the document store"},"description":{"type":"string","description":"Description of the document store"},"loaders":{"type":"string","description":"Loaders associated with the document store, stored as JSON string"},"whereUsed":{"type":"string","description":"Places where the document store is used, stored as JSON string"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store"},"vectorStoreConfig":{"type":"string","description":"Configuration for the vector store, stored as JSON string"},"embeddingConfig":{"type":"string","description":"Configuration for the embedding, stored as JSON string"},"recordManagerConfig":{"type":"string","description":"Configuration for the record manager, stored as JSON string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the document store was created"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the document store was last updated"}},"title":"DocumentStore"}}}}},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/get-document-store-by-id.api.mdx">
---
id: get-document-store-by-id
title: "Get a specific document store"
description: "Retrieves details of a specific document store by its ID"
sidebar_label: "Get a specific document store"
hide_title: true
hide_table_of_contents: true
api: eJy9Vk1v2zgQ/SvEnHYBNXYX2YtO6629gYs0G8QOiiLIgRbHFluJVMihs4ag/14MKbv+SnIIsBdLMofvzScfWyC58pA/gLJFqNHQB0/WITxm4LEITtMG8ocWFigdulGgEvKHx+4xA4W+cLohbQ3kcIfkNK7RC4UkdeWFXQopfIOFXupCbOFFhBeLjdDkxXQMGdgGnWSYqYIcVkjj3nbGpn9vpgoyaKSTNRI6H93RzNlIKiEDI2uEHDSbOXwK2qGCnFzADHxRYi0hb4E2DVt5ctqsIIOldbUkyCEEraA7jmfrg4hOsKMcs0PfWOPRM+IfwyE/DvfNQlGg98tQVRvh+qSoo/Ahg8IaQkMMIJum0kXMwOC7Z5T21G+7+I4FcSIc54t08kGrt2M7Du3e6KeAQis0pJcanVhaJ6jEYy+7bW5PGQ4Rb2SNXO/zGAe2b0GNf329jFhZqWInvIV2nQyF9N4WWhIq8aypPAObpYcS0ovPs39vRI/YZfBcosN7j2dTfch3W8kCvYg7zpAI7UXwqF4h8yQpnI0MTah5UCdfbuffIIPZt5tP/WN6c8Vv89H1BDK4mXyFDO5vZ5O7eVpJ75MxnIztLNK9nOk1FmRdnIFP1iz16u0cJLuQRnrXWgnozVxjvUCltFm9j20H8wqVw8I69UUauUL3ProEJeqE9Qpn4ZCbcCzp7FTt5lZJwg+kazydD0kopFGCV7nRzLk+e5Ze9FxMGxr1v9FW0pPoCaHrMiBNFe6dqLPUWx2vXQ4vT4/Q8SGosSSWNpgYyZ/nztypIXRGVsKjW6MT6Jx1kbtGKm2vKlFEWL5gcKh1g/TbatUBqx5jJJUJroIcSqLG54NB46y68BSUthdUojT+Gd2F1APZ6MH64+lh4KwKRWyUBApdto+ZDwaVLWRVWk/55XA4fAnomq2EwjVWtkmZ6fH2VHrGipFEYV+rd9VmRugFkb+TEWT9yz/bHvj8dR5Tp83Sxu1H5esFcXQ7PfFzdDuNIxHnQJvVUXv4TBS2qjCmxGexnbYWXmgjRjGjIwbmEiTQjxfDi2EvmLKIgtkL/tZczELTWMcFPizYfpH4AK2l5mWfzP86WOaQG+uplmaP4grp5UvMcfztL1F/z3WorxfhfzRoKqlNnGAOrO0b+OSyxrKRnrlWfMhzS7Fd2y6kx3tXdR3//RTQ8XXuMYO1dFouuLIPLSjt+V1BvpSVx1fi+u2uv2D9Lk5vSC/43v8pzYYLK6vAX5DBD9yka1v32GVQIkt1dCctjIoCG9rbcnJP4vbfTfjVZA4ZyMOmP2ryiH7WnbZNFnP7A03X7bwj/mYHu+4ndzzf/w==
sidebar_class_name: "get api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get a specific document store"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/document-store/store/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieves details of a specific document store by its ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document Store ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully retrieved document store","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store"},"name":{"type":"string","description":"Name of the document store"},"description":{"type":"string","description":"Description of the document store"},"loaders":{"type":"string","description":"Loaders associated with the document store, stored as JSON string"},"whereUsed":{"type":"string","description":"Places where the document store is used, stored as JSON string"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store"},"vectorStoreConfig":{"type":"string","description":"Configuration for the vector store, stored as JSON string"},"embeddingConfig":{"type":"string","description":"Configuration for the embedding, stored as JSON string"},"recordManagerConfig":{"type":"string","description":"Configuration for the record manager, stored as JSON string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the document store was created"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the document store was last updated"}},"title":"DocumentStore"}}}},"404":{"description":"Document store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/get-file-chunks.api.mdx">
---
id: get-file-chunks
title: "Get file chunks"
description: "Get file chunks for a specific loader in a document store"
sidebar_label: "Get file chunks"
hide_title: true
hide_table_of_contents: true
api: eJzNWEtv2zgQ/ivEnHYBNnIX3YtOayRu4UUbGLWLoAhyoKWRxVYiVZJymhr678VQD8uSXTcu9nGyEg6/Gc43nAd34MTGQngPsY7KHJV7YZ02CA8cLEalke4JwvsdrFEYNNPSpRDeP1QPHGK0kZGFk1pBCG/QsURmyKK0VJ8tS7RhgtkCI5nIiGVaxGiYVEywVhOrNXHQBRpBOPMYQtigey0zvPY4wKEQRuTo0FhviCRthXApcFAiRwjB48xj4GDwSykNxhA6UyIHG6WYCwh34J6KWtRItQEOiTa5cBBCWcoYquFxblobl4TN5jdQ8aO664P9Q8obr53WXogN3upn6B7pWoiNVN75TJX5Gg0QtwZtoZVFSwh/TCb0c7hvWUYRWpuUWfbEDDojcYtxx77R+Z7n+hjAIdLKoXKEJooik5FXHHyyBLkbG63XnzByFAOGYsTJ2qBaSU9OGCOegIN0mNvz+2V8npShnz4o+aVEJmNUTiYSjY9wl2Iv6r13dTS/AL6jfH7DHqVLpfLY9Q2peBfilwP3Arl24K3uoUnlcOM5Otzub2ETGX3DWm4JjYLwes/s0L4BYC3IdOJxOr/l6EQs3NHrcojwrpFkwlodSeEw9pb18SoOTroMew7w5+8SixeJdHlgcnMBOOBXkRe0+2XFgfj9t2PqMEW2F6ji+3Rz1k3zm9bHw923Pnec209SpxCutUrk5phTRmQnclPWyb073B7NFpl07rnncfjVsXZrH+b5BzsJ9WsnHME67UR2PUxbXbwdYq5IuL1zOmlyag9GmItQhBGRL6M+nwhX2mPOQlXm1A/M3i1WH4HD8uPtdfMzv31DX6vp2xlwuJ3dAYcPi+Xs/apeqb9nNzDqDpZeXev1k9H9gyx3KhgGzURzX/9XxYGMUs8OzXZnLnNcGEzk159IjfN3M1Z44SGMld/wfNAs5beRAf9pqLRGlIWPkh9zEwuHL5zMcVwFhUMmVMxolT2mqPal+1FY1sGfrBxvfZC+9uaQU3Rpop9gdOnlzgZ+ZNDHjsh+ooJ2skcr4EkdBtOn2JBwT8da6wyFGim5S9Gl2M/XLBWWrREV6wFR9Te4lfh4Gaa0NBI0ECzXMfYgfb68PlWjBzdnkC3HsGeI1WbRHIToKI1B5RZic+TOtHnqeLUZNtij9bOtCWmN3zcNOFQV7Xg1eTVuwW8OmVbasUSXyrPy57Gefa4cGiUyZtFs0TA0RhtvUY4u1c3w5UcuGvMgOJwJg9q1wa5J01Wwa9uRKtjVo0gFNDcSej2tlSaDEFLnChsGQWF0fGVdGUt95VIUyj6iuRIyEIUMti9HrC6MjsvIl9Ya1M9Be8wwCDIdiSzV1oWvJpPJKaC3JMVi3GKmi9pnDV5vzl3S+FEXgf602zFIGqEZrXyceyHgzcfrNg39fbfyTpUqqVvsAd1NKz5dzEd2ThdzXzpyoWgu2wxus+Us0lmG3iWW+4zWSviQn3qPTgmYKKhBX15NribN9CUif5ma8bEVZ8uyKLQh6g8J65NECT8XkpZtLf7XwTIdudDW5UL1VAzeBYYn3u1nwl96QmgoosYrKDIhlS8axqfTOppHLxwcOpPC/QtC2Bvow2a8fuBAEUYYu91aWPxgsqqif38p0dD7yAOHrTBSrIno+x3E0tJ3DGEiMos/OPRv75vJ/Xc2HtVOnKttbBS1NVuRlfQXcPiMT73XELoqv2bH/vnhIkM6T15syfhx4iJDGhqrh4pDir4eEkn14jSKsHC9baOXCcoRXYJ8M1sBB3GYGQaZwKMfNWm3qyVW+jOqquosdPQ3GVhV3wE4G8G/
sidebar_class_name: "get api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get file chunks"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/document-store/chunks/{storeId}/{loaderId}/{pageNo}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Get file chunks for a specific loader in a document store

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"storeId","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document Store ID"},{"in":"path","name":"loaderId","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document loader ID"},{"in":"path","name":"pageNo","required":true,"schema":{"type":"string"},"description":"Pagination number"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully retrieved chunks from document loader","content":{"application/json":{"schema":{"type":"object","properties":{"chunks":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the file chunk"},"docId":{"type":"string","format":"uuid","description":"Document ID within the store"},"storeId":{"type":"string","format":"uuid","description":"Document Store ID"},"chunkNo":{"type":"integer","description":"Chunk number within the document"},"pageContent":{"type":"string","description":"Content of the chunk"},"metadata":{"type":"string","description":"Metadata associated with the chunk"}},"title":"DocumentStoreFileChunk"}},"count":{"type":"number","example":1},"file":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store loader"},"loaderId":{"type":"string","description":"ID of the loader"},"loaderName":{"type":"string","description":"Name of the loader"},"loaderConfig":{"type":"object","description":"Configuration for the loader"},"splitterId":{"type":"string","description":"ID of the text splitter"},"splitterName":{"type":"string","description":"Name of the text splitter"},"splitterConfig":{"type":"object","description":"Configuration for the text splitter"},"totalChunks":{"type":"number","description":"Total number of chunks"},"totalChars":{"type":"number","description":"Total number of characters"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store loader"},"storeId":{"type":"string","description":"ID of the document store"},"files":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the file"},"name":{"type":"string","description":"Name of the file"},"mimePrefix":{"type":"string","description":"MIME prefix of the file"},"size":{"type":"number","description":"Size of the file"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the file"},"uploaded":{"type":"string","format":"date-time","description":"Date and time when the file was uploaded"}},"title":"DocumentStoreLoaderFile"}},"source":{"type":"string","description":"Source of the document store loader"},"credential":{"type":"string","description":"Credential associated with the document store loader"},"rehydrated":{"type":"boolean","description":"Whether the loader has been rehydrated"},"preview":{"type":"boolean","description":"Whether the loader is in preview mode"},"previewChunkCount":{"type":"number","description":"Number of chunks in preview mode"}},"title":"DocumentStoreLoaderForPreview"},"currentPage":{"type":"number"},"storeName":{"type":"string"},"description":{"type":"string"}},"title":"DocumentStoreFileChunkPagedResponse"}}}},"404":{"description":"Document store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/preview-chunking.api.mdx">
---
id: preview-chunking
title: "Preview document chunks"
description: "Preview document chunks from loader"
sidebar_label: "Preview document chunks"
hide_title: true
hide_table_of_contents: true
api: eJztWFFv2zYQ/ivEPcux4yZpq6dliTu4aDOjdlF0gTHQ0sliK5EqSSV1Df334UjJlmwnbrpi28P60MjS8bs73nfHO67B8qWB8BZiFZU5StszVmmEeQAGo1ILu4Lwdg0L5Br1ZWlTCG/n1TyAGE2kRWGFkhDCROOdwHvWwLAoLeVnwxKtcpYpHqOGAFSBmtOKcQwhFH7NFUkKuYQANH4p0dhfVbyCcA2RkhalpUdeFJmI3Nr+J0Mq12CiFHNOT3ZVIISgFp8wshBAoUmTFWjoq4hbMsZqrytROucWQihLEcOuP++l+FIiEzFKKxKBmiVKM5vi1kO3UY1vVQD+aXxQWRd8fM1U4sB2V9/wHI+vJ6mHEK6UTMTy0KZ0Mbxc6cOxcW6LZopMWPtUfyx+taxZ2oZ5umMPQv09D/dgrbI8cxw0LUxZ5gtH2S7mjISZ/0iGepa3YLj+IRSueWRROyRjuS3Noc1CWeaUq6O3k9lHCGD68eaq/jO++Y2eZpdvRhDAzegDBPB+Mh29m/kv/nl0DXuZO3Xqml1/kN3u99PI0MUikERk2PaMa81XEICwmJt/OpHJGDJKPpmazcpc5DjRmIivx9e/Hb8dscIJ78IY8Q2Pk2Yqvu0Z8K9SpTGiLBxLHo9NzC32rMhxz69rbpFxGTP6yu5TlBt0ds8N28BXlGXCZqTguqbWlJj1xpH0lTOHNkWVOvqOiE6d3FHiRxodd3h2HPJqI8u4MSoS3GLM7oVNH9ehMV3FmoRbOhZKZcjlnpIPKdoU2/WapdywBaJkLaAqaA7YH8MUhgnJagiWqxhbkK5eXqnSH86PE/dmp1ruwx4JrNJ1dwFVVfkmQWjaKqtLdC9MoaTxNWI4GNCfnVCXUYTGJGWWrTa669od/Kw2I9o9Q55Y3Qq+xKutKXsZ/ZXnhdujWUqxMS5Yte0NiwnjxJUmtDzm9qDZPI4FecizSceArsaqpXINvLSp0hDCa5VKdq1cHnNL8sPB8Kw3eNEbnvn47IbSx/fgKSukxaWjzEbV6eAIyw6sOa/cvwDODgV/LO94JmJWN5dsQd3lz4s6MfhoYRhprTRzsu1ICm/an7VpPnDG8OV3QzbiXX7gxlmqoLUWQo/RcpEdJGMX/XLDEYZOUbOyw4pEYEadvDtCAxDGlNiclcKwTaZ2eOEMBx+w88MBs6hJs0F9h9ob8H/E/gsR85UlVW58U8ZtLqeZEPrdAbLvT5F+cwTROEnBNG6aLHUGIaTWFibs9wut4hNjy1ioE5sil+Ye9QkXfV6I/t3pnqcTreIycl29B4UqaGOG/X6mIp6lytjwbDAYPAT0hqRYjHeYqcIfzDVea/ydEq08c9pD8CYcpJHcc2J0xDohCOqHV00H9PrDzFVCouu77Zw72saG2lt4lvAX58nFWe/8+enz3tn5xbC3eJZEvWH08uJZcnHBE34B7WFzy7b2CLn7djM07Yx2W7nupLb/vo3QqeaD7vAz2Lalm+5zMzy0OkM/Ctw+zW25Y1y7A2+Z7BrqQ4Zse9X62HreG76cnZ6H56fh8MXJ4PnpHz74dQ+5xWy3gdu37caNupFWz9X52T7J6IATMlGOQztnJXN9D7ucjPfzezJ2o0vOJV8KudzpJk3AIpVl6PLCBK6jbiRcy3Xp0uqSgCkPPejpyeBkUJdWHrnSWm9xI86mZVEoTZnezdp2plKNyblwW+PFf+l8Jt5Tuci5bKl44M5o1/PWPdB3XjPVmUmjfr/IuJBuTNGZ77LcFdbufVeTJ7CN4DwAqiAkvF4vuMH3Oqsqev2lRE3XYvMA7rgWfEExvJ1XAaToQIjWn3HlLyHI9N6MTCLxzNXbvfOLKphfcRlFWNhHZeetKjz5fTqjYlNfmbmGOgTNqeTS/yG4ezfHChJw79aQcbks3akFHpNCxLuVbaeSOa+a1lauWhau115ipj6jrCoIalcs/YZqXlXVXw3CBUc=
sidebar_class_name: "post api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Preview document chunks"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/document-store/loader/preview"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Preview document chunks from loader

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store loader"},"loaderId":{"type":"string","description":"ID of the loader"},"loaderName":{"type":"string","description":"Name of the loader"},"loaderConfig":{"type":"object","description":"Configuration for the loader"},"splitterId":{"type":"string","description":"ID of the text splitter"},"splitterName":{"type":"string","description":"Name of the text splitter"},"splitterConfig":{"type":"object","description":"Configuration for the text splitter"},"totalChunks":{"type":"number","description":"Total number of chunks"},"totalChars":{"type":"number","description":"Total number of characters"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store loader"},"storeId":{"type":"string","description":"ID of the document store"},"files":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the file"},"name":{"type":"string","description":"Name of the file"},"mimePrefix":{"type":"string","description":"MIME prefix of the file"},"size":{"type":"number","description":"Size of the file"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the file"},"uploaded":{"type":"string","format":"date-time","description":"Date and time when the file was uploaded"}},"title":"DocumentStoreLoaderFile"}},"source":{"type":"string","description":"Source of the document store loader"},"credential":{"type":"string","description":"Credential associated with the document store loader"},"rehydrated":{"type":"boolean","description":"Whether the loader has been rehydrated"},"preview":{"type":"boolean","description":"Whether the loader is in preview mode"},"previewChunkCount":{"type":"number","description":"Number of chunks in preview mode"}},"title":"DocumentStoreLoaderForPreview"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully preview chunks","content":{"application/json":{"schema":{"type":"object","properties":{"chunks":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}},"totalChunks":{"type":"integer","example":10},"previewChunkCount":{"type":"integer","example":5}}}}}},"400":{"description":"Invalid request body","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","description":"Error code","example":"invalid_request"},"message":{"type":"string","description":"Error message","example":"The request was invalid"},"details":{"type":"object","description":"Additional error details","example":{"field":"name","issue":"Name is required"}}},"title":"Error"}}}},"500":{"description":"Internal server error","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","description":"Error code","example":"invalid_request"},"message":{"type":"string","description":"Error message","example":"The request was invalid"},"details":{"type":"object","description":"Additional error details","example":{"field":"name","issue":"Name is required"}}},"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/process-chunking.api.mdx">
---
id: process-chunking
title: "Process loading & chunking operation"
description: "Process loading & chunking operation of document from loader"
sidebar_label: "Process loading & chunking operation"
hide_title: true
hide_table_of_contents: true
api: eJztV99v2zYQ/leIexg2QLGdNkkLPS1rO8D71aD2sIfYGM7S2WJLkSpJOfUE/e/DkbIjJc6aNQO2h+UlNHX87o53992xAY8bB+k15CarS9L+xHljCZYJOMpqK/0O0usGVoSW7GXtC0ivl+0ygZxcZmXlpdGQwpU1GTknlMFc6o34SmRFrT/w0lRkkcWEWYu9GrG2pgzSZCGBg8w0hxSqCPaqQ4AELH2syfnvTL6DtIHMaE/a8xKrSsksnB2/d2xLAy4rqERe+V1FkIJZvafMdzjSUs4eB0enOSQQzQhLVynpffixTNiQiqyX5AJsd+AW13kb7Rtexuu9k+GEmL6GBOgTlpXiQxeT5/hidX56gvQiOzlbTfDk5cuL8xM6Oz+j/Nn64sXFBbQ9qz6rb/qar9YX1F2o+JpGm1EiKoVSz+mTT0SVrxNBPht9M7DlIHGr8Bcs6REuSlcp3AmNJQ2VD/CvGF8MFbwyei03x6IzVBHl6i531sY+oKQBz/gpsBrhjViRCGFc6IWeF9IJ6QRq4wuyMSshCUdmXawhBUigJI85eoQUmgU4U9uMFpDqWqmWM7SU/udO5EfaOT7UtoOE+RtxYvVif3RwY5arzsktvSrQYubJzvum9jR+eaAe1v5ur10c1IfoiWMGPC2QDxrRQAjSTP7BmM8nkMSNt1uyCitIgbccVWjRG8uRuF7AYqEXsIQ2/PUL3duawoarjHaxkp9NJvxvaOaszph11rVSO9FR0BEWY3O+lH7u8ElU2BNcGaMIdf82gvmdU2fHzJ7qLSqZi44ixYo58h+zMTP5I5LsjbXGiiDbzyYZTfu9M41zpyTncPNoyL14H3Ve0MHZG3Si08LoOXmUyn0+JS/zXPISlaCgaH9ykIdrSYr7EdcOJCCdqxmTC4855ZBjIThe+mBeMBxiwM6PB8yTZc2O7JZsNOD/iP0XIhbc9YUJQ4hx4XKRRx4YD+ejcWxD444mAh9xMF0YlmqrIIXC+8qlY5bJR87XuTQjXxBqd0N2hHKMlRxvT+95emVNXmeBKyMotEkfMx2PlclQFcb59GwymTwE9BNLiZy2pEwV55EOrzfdzTitYub0Z7xDOFgjuxfEmKKCECTd4ntjS+Tm+8Nv89APOV3f3U5rb25jcxifHjcB9Qeg3pwyHFP6A8aR+eLfmguGY8Hnevrdlv6ILnysCT+payYg9dqES+uq4jDEzsIQe3k1vV+TV9PQzkvUuOEmmQ8GX5eIzChFIZddIlDnBwkmAXEZSuGSgbl2IujpaDKadHSIWaBDHa9lLy5mdVUZyxEfVlq/upgXSpT82UXxbwef2WUu8RJ1T8VjnjF3r6H3HHnqM6irOU7CcUh5TqXgY9MR0b2H2j7rYx8IZLRMgLmBhZtmhY5+tapteftjTZbfc8sEtmglrjjS18s2gYICCLPXB9rFoY19OpmzSSyuApPe60zMTfHEZZZR5f9Sdtnj16u3sznTSPekK0PbAos3/EzDm1h8JtxxYKew14BCvalDP4KIyYHEIWfd4ajgVfcJ9a5nYdNEibn5QLrlco6ueP4N7bJt2z8BCKNAZg==
sidebar_class_name: "post api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Process loading & chunking operation"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/document-store/loader/process"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Process loading & chunking operation of document from loader

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","required":["storeId","loaderId","splitterId"],"properties":{"storeId":{"type":"string","description":"Document store ID","example":"603a7b51-ae7c-4b0a-8865-e454ed2f6766"},"loaderId":{"type":"string","description":"ID of the loader (e.g., plainText, pdf, etc.)","example":"plainText"},"loaderName":{"type":"string","description":"Display name of the loader","example":"Plain Text"},"loaderConfig":{"type":"object","description":"Configuration for the loader","example":{"text":"Text to be split\n\nThis is another chunk","textSplitter":"","metadata":"{\"source\":null}","omitMetadataKeys":""}},"splitterId":{"type":"string","description":"ID of the text splitter","example":"recursiveCharacterTextSplitter"},"splitterName":{"type":"string","description":"Display name of the text splitter","example":"Recursive Character Text Splitter"},"splitterConfig":{"type":"object","description":"Configuration for the text splitter","example":{"chunkSize":"30","chunkOverlap":"0","separators":"[\"\\n\"]"}}}}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully process chunking operation","content":{"application/json":{"schema":{"type":"object","properties":{"success":{"type":"boolean","example":true}}}}}},"400":{"description":"Invalid request body","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","description":"Error code","example":"invalid_request"},"message":{"type":"string","description":"Error message","example":"The request was invalid"},"details":{"type":"object","description":"Additional error details","example":{"field":"name","issue":"Name is required"}}},"title":"Error"}}}},"500":{"description":"Internal server error","content":{"application/json":{"schema":{"type":"object","properties":{"code":{"type":"string","description":"Error code","example":"invalid_request"},"message":{"type":"string","description":"Error message","example":"The request was invalid"},"details":{"type":"object","description":"Additional error details","example":{"field":"name","issue":"Name is required"}}},"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/query-vector-store.api.mdx">
---
id: query-vector-store
title: "Query vector store"
description: "Query the vector store for similar documents"
sidebar_label: "Query vector store"
hide_title: true
hide_table_of_contents: true
api: eJydVk1v4zYQ/SvEnLqAbCup8wFdtm7TBVwUaLZxu4fAhzE5sbiRSC1JOTEE/fdiKNmWkrRpm0soab7fm0c3EHDrIbsHZWVdkgkTH6wjWCfgSdZOhz1k9w1sCB25RR1yyO7X7ToBRV46XQVtDWTwuSa3FyEnsSMZrBMxjHjgky51gU4cMnhIwFbkkF2XCjL4xs5/Rr+7mD0BR99q8uFHq/aQNSCtCWQCH7GqCi2j8+yr5+QNeJlTiXwK+4ogA7v5SjL0cbQjxS3GkpYKki4h91g5riRo8jFMb3CK44PTZgsvu73pWxGxXLG8gQToGcuqYKfL9Hu82lycTZCu5GS+SXFyfX15MaH5xZzU+cPl1eUltIcq3k3Wj9YKT+hkzjMdpfuSYxDax+FLrHTAQtgH8cmhkfQRWv4bDiK4muILX1nju87P05T/jRPf1VKS9w91UewFPZOsAykRqxbWjICG5P9DNMYg6JJW+EhmYGrqckPu1WBWuiQR2Jan0xcYx9DV+J02otRFoT1Ja5T/wENXVvpBaHQO95CADlT696urcEs/nRp9CdwJlFWu/RGUzoFB4UeOMeVSSgqoMLw5FFRKc5tY3I7HM8rYDlI2gHXIrYMMfrG5ETeWUVEY2P48PZ9P0uvJ+RwiG4IOscwDkTuW8Jf5W0xYmh0WWol+K8WG17JN4OJt40DOYCE8uR05Qc5ZB23XcG554Svr42yR5QRmY+2ZdcTqzt2OsBhxLB+1qHYFZJCHUPlsNqucVVMfaqXtNOSExj+Rm6KeYaVnu7NXrLl1VtWSH/oCoU2GMbPZrLASi9z6kM3TNP27QL+ylVC0o8JWUQ0O8Qbiece076AbSugRSM7I7UUzyHojSPrDJ+tKDAzpl1UcIa/T7ydt/PmE/lG8/p3+HOXnXf1IQJsH2+3mmDW9/C1ul6+Gs7hdRvEv0eBWm+1R/Tu98ImQtigo4uATgUad7gehjVhEGBccmHHvgp5N02naSw3KuIEG49gO5uKurirrmFxjlgyZwYtaoubPvjP/YfSZ58wMLdEMUnQy/EL0Rk0Prqn/eh/2bAj0HGZVgdrw2GMHTb8lr25oHsxxT4Z3GhOX7Ztmg57+cEXb8use7vt1Ajt0GjcM5f26TSAnVOTiaj0SM6JXuMmKq2Lzoo5a+VLWeXE6j4WUVIV/tF0P9v/2t7sVc7y/3Uur2MfhE9/Y+AQZxN8IkRxsEN81UKDZ1rhl2y4mI4XjhXqxQLGrg9ab/aDCpuksVvaRTNtC0rcS+Bnaddu2fwHAuyQn
sidebar_class_name: "post api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Query vector store"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/document-store/vectorstore/query"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Query the vector store for similar documents

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","required":["storeId","query"],"properties":{"storeId":{"type":"string","description":"Document Store ID","example":"603a7b51-ae7c-4b0a-8865-e454ed2f6766"},"query":{"type":"string","description":"Query to search for","example":"What is the capital of France?"}}}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully executed query on vector store","content":{"application/json":{"schema":{"type":"object","properties":{"timeTaken":{"type":"number","description":"Time taken to execute the query (in milliseconds)"},"docs":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}}}}}}},"400":{"description":"Invalid request body"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/refresh-document.api.mdx">
---
id: refresh-document
title: "Re-process and upsert all documents in document store"
description: "Re-process and upsert all existing documents in document store"
sidebar_label: "Re-process and upsert all documents in document store"
hide_title: true
hide_table_of_contents: true
api: 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
sidebar_class_name: "post api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Re-process and upsert all documents in document store"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/document-store/refresh/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Re-process and upsert all existing documents in document store

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document Store ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"items":{"type":"array","items":{"type":"object","properties":{"docId":{"type":"string","format":"uuid","nullable":true,"description":"Document ID within the store. If provided, existing configuration from the document will be used for the new document"},"metadata":{"type":"object","nullable":true,"description":"Metadata associated with the document","example":{"foo":"bar"}},"replaceExisting":{"type":"boolean","nullable":true,"description":"Whether to replace existing document loader with the new upserted chunks. However this does not delete the existing embeddings in the vector store"},"createNewDocStore":{"type":"boolean","nullable":true,"description":"Whether to create a new document store"},"docStore":{"type":"object","nullable":true,"description":"Only when createNewDocStore is true, pass in the new document store configuration","properties":{"name":{"type":"string","example":"plainText","description":"Name of the new document store to be created"},"description":{"type":"string","example":"plainText","description":"Description of the new document store to be created"}}},"loader":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"plainText","description":"Name of the loader (camelCase)"},"config":{"type":"object","description":"Configuration for the loader"}}},"splitter":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"recursiveCharacterTextSplitter","description":"Name of the text splitter (camelCase)"},"config":{"type":"object","description":"Configuration for the text splitter"}}},"embedding":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"openAIEmbeddings","description":"Name of the embedding generator (camelCase)"},"config":{"type":"object","description":"Configuration for the embedding generator"}}},"vectorStore":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"faiss","description":"Name of the vector store (camelCase)"},"config":{"type":"object","description":"Configuration for the vector store"}}},"recordManager":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"postgresRecordManager","description":"Name of the record manager (camelCase)"},"config":{"type":"object","description":"Configuration for the record manager"}}}},"title":"DocumentStoreLoaderForUpsert"}}},"title":"DocumentStoreLoaderForRefresh"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully execute refresh operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"numAdded":{"type":"number","description":"Number of vectors added","example":1},"numDeleted":{"type":"number","description":"Number of vectors deleted","example":1},"numUpdated":{"type":"number","description":"Number of vectors updated","example":1},"numSkipped":{"type":"number","description":"Number of vectors skipped (not added, deleted, or updated)","example":1},"addedDocs":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}}},"title":"VectorUpsertResponse"}}}}},"400":{"description":"Invalid request body"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/document-store/document-store-api",
    },
    {
      type: "category",
      label: "document-store",
      items: [
        {
          type: "doc",
          id: "api/document-store/create-document-store",
          label: "Create a new document store",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/document-store/get-all-document-stores",
          label: "List all document stores",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/document-store/get-all-document-stores",
          label: "List all document stores",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/document-store/get-document-store-by-id",
          label: "Get a specific document store",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/document-store/update-document-store",
          label: "Update a specific document store",
          className: "api-method put",
        },
        {
          type: "doc",
          id: "api/document-store/delete-document-store",
          label: "Delete a specific document store",
          className: "api-method delete",
        },
        {
          type: "doc",
          id: "api/document-store/preview-chunking",
          label: "Preview document chunks",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/document-store/process-chunking",
          label: "Process loading & chunking operation",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/document-store/get-file-chunks",
          label: "Get file chunks",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/document-store/update-file-chunk",
          label: "Update a file chunk",
          className: "api-method put",
        },
        {
          type: "doc",
          id: "api/document-store/delete-file-chunk",
          label: "Delete a file chunk",
          className: "api-method delete",
        },
        {
          type: "doc",
          id: "api/document-store/query-vector-store",
          label: "Query vector store",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/document-store/delete-vector-store-data",
          label: "Delete vector store data",
          className: "api-method delete",
        },
        {
          type: "doc",
          id: "api/document-store/upsert-document",
          label: "Upsert document to document store",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/document-store/refresh-document",
          label: "Re-process and upsert all documents in document store",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/document-store/delete-loader-from-document-store",
          label: "Delete specific document loader and associated chunks from document store",
          className: "api-method delete",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/document-store/update-document-store.api.mdx">
---
id: update-document-store
title: "Update a specific document store"
description: "Updates the details of a specific document store by its ID"
sidebar_label: "Update a specific document store"
hide_title: true
hide_table_of_contents: true
api: eJztWE1v2zgQ/SvEnHYBOXbcfLQ6rbfJLlx0s0HtoOgGOYzFkc1WIlWSSmoY+u+LIeVvOzkU2MvmYkvk6D3OcOaJowV4nDpI70GarC5J+47zxhI8JOAoq63yc0jvFzAhtGQHtZ9Bev/QPCQgyWVWVV4ZDSncVRI9OeFnJCR5VIUTJhcoXEWZylUmlgQiEIjJXCjvxPAKEjAVWWSgoYQU6gB11ZqPwnISqNBiSZ6sC+tRTFqhn0ECGkuCFJSEBCx9r5UlCam3NSXgshmVCOkC/LxiK+et0lNIIDe2RM98tZLQ7Dq05BdhAbxOdprhyfnfjZwzZma0J+35EquqUFnwovvVMcRin9xMvlLm2RvLPntFjmeVfHmBewHX6ntNQknSXuWKrMiNjeHfCjR7FgO0z7CNeIMl8Z4dxtiyfQnqan13HLEwKMN2voT2MRoKdM5kCj1J8aT87ABsEv+kQCc+jP6+ES1ik8DTjCzdOToY6m2+2wIzciI8cYBEKCdqR/IZMufR1wc9I12XXG7Xf92Ov0ACoy8379u/4c2ffDUefLyGBG6uP0MCd7ej60/jOBOvr69gr/hGge54pB8p88aGRH5vdK6mL8cg2tWxLFepFYFejDWVE5JS6enPsa1gnqGylBkr/0KNU7I/RxehRBmxnuHMLHESXqE/WFWrumUZ63hV0n59oCeBWgqe5UTTh/LsCZ1ouZg26uJ/Q1ug86IlhKZJwCtf0IYsRllummZXcsOAq4x2Udv6vR7/7SRsnWXkXF4XxXxJs5u3yau6vqrrq7q+quv/WV2bBM56Z/sCerUNqo0Xual18OT8kOIOtSersRCO7CNZQdYaG7hL8jPD5+6qDtrJZ+oUutv9QDf+LpRsgDsDxogH8doWkMLM+8ql3W5ljTxxvpbKnPgZoXZPZE9QdbFS3cfTfTGwRtZZSJQICk2yiZl2u4XJsJgZ59OzXq93DOgjWwlJj1SYKkamxdvoZEb8xogvhc1+ZrXbzAhtz8D30QiS9uKPZQ58+DwOoeM30ad1R3D9A8uqoOU7B97k+PY8vzjrnF+eXnbOzi/6ncmbPOv0s3cXb/KLC8zxYt2+HKmm1fBK0NdDG6q7Hlyq40oED+jThmLuisl66mDxr6e36hT6vf5Zp3fZ6b8bn56n56dp/+1J7/L0H9iprOOGTQJK5yZsx045tD3Y4Ha4F57B7TBITNAVpac75eYSkZmioJBiLgnlubRwQmkxCBk6GIY4WRdBT096J732AIJZOIC0m7Q0F6O6qozlgtkugM2kD+FFxdMumv+2Nc0pVBnnS9QbFLGTPt4574Zgowv9uS68LQJPP3y3KlDpIIvs3aJVhb2vBJxt8T9Vkt+cXKdst1hM0NGdLZqGh7/XZPk7wkMCj2gVTnh77xcgleNrCWmOhaNnPPvlU3vK/FXsd+ZH1t4Oop7z7mJR8x0k8I3m8XNB89AkMCOuq7CcOPE+knbG/Pj6wb0jKCtVfGKQZVT5Z20fNoT29m7MmtJ+QyiN5EcsPnHN4VNcowkxCFIVxhZQoJ7WOGXbCMnpg9sCtiNYwamDUVgsosXYfCPdNKugeL7nuDTNv105P24=
sidebar_class_name: "put api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update a specific document store"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/document-store/store/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Updates the details of a specific document store by its ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document Store ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store"},"name":{"type":"string","description":"Name of the document store"},"description":{"type":"string","description":"Description of the document store"},"loaders":{"type":"string","description":"Loaders associated with the document store, stored as JSON string"},"whereUsed":{"type":"string","description":"Places where the document store is used, stored as JSON string"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store"},"vectorStoreConfig":{"type":"string","description":"Configuration for the vector store, stored as JSON string"},"embeddingConfig":{"type":"string","description":"Configuration for the embedding, stored as JSON string"},"recordManagerConfig":{"type":"string","description":"Configuration for the record manager, stored as JSON string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the document store was created"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the document store was last updated"}},"title":"DocumentStore"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully updated document store","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store"},"name":{"type":"string","description":"Name of the document store"},"description":{"type":"string","description":"Description of the document store"},"loaders":{"type":"string","description":"Loaders associated with the document store, stored as JSON string"},"whereUsed":{"type":"string","description":"Places where the document store is used, stored as JSON string"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store"},"vectorStoreConfig":{"type":"string","description":"Configuration for the vector store, stored as JSON string"},"embeddingConfig":{"type":"string","description":"Configuration for the embedding, stored as JSON string"},"recordManagerConfig":{"type":"string","description":"Configuration for the record manager, stored as JSON string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the document store was created"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the document store was last updated"}},"title":"DocumentStore"}}}},"404":{"description":"Document store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/update-file-chunk.api.mdx">
---
id: update-file-chunk
title: "Update a file chunk"
description: "Update a specific file chunk in a document store"
sidebar_label: "Update a file chunk"
hide_title: true
hide_table_of_contents: true
api: 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
sidebar_class_name: "put api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update a file chunk"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/document-store/chunks/{storeId}/{loaderId}/{chunkId}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Update a specific file chunk in a document store

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"storeId","required":true,"schema":{"type":"string"},"description":"Document Store ID"},{"in":"path","name":"loaderId","required":true,"schema":{"type":"string"},"description":"Document Loader ID"},{"in":"path","name":"chunkId","required":true,"schema":{"type":"string"},"description":"Document Chunk ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully updated chunk","content":{"application/json":{"schema":{"type":"object","properties":{"chunks":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the file chunk"},"docId":{"type":"string","format":"uuid","description":"Document ID within the store"},"storeId":{"type":"string","format":"uuid","description":"Document Store ID"},"chunkNo":{"type":"integer","description":"Chunk number within the document"},"pageContent":{"type":"string","description":"Content of the chunk"},"metadata":{"type":"string","description":"Metadata associated with the chunk"}},"title":"DocumentStoreFileChunk"}},"count":{"type":"number","example":1},"file":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the document store loader"},"loaderId":{"type":"string","description":"ID of the loader"},"loaderName":{"type":"string","description":"Name of the loader"},"loaderConfig":{"type":"object","description":"Configuration for the loader"},"splitterId":{"type":"string","description":"ID of the text splitter"},"splitterName":{"type":"string","description":"Name of the text splitter"},"splitterConfig":{"type":"object","description":"Configuration for the text splitter"},"totalChunks":{"type":"number","description":"Total number of chunks"},"totalChars":{"type":"number","description":"Total number of characters"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the document store loader"},"storeId":{"type":"string","description":"ID of the document store"},"files":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the file"},"name":{"type":"string","description":"Name of the file"},"mimePrefix":{"type":"string","description":"MIME prefix of the file"},"size":{"type":"number","description":"Size of the file"},"status":{"type":"string","enum":["EMPTY","SYNC","SYNCING","STALE","NEW","UPSERTING","UPSERTED"],"description":"Status of the file"},"uploaded":{"type":"string","format":"date-time","description":"Date and time when the file was uploaded"}},"title":"DocumentStoreLoaderFile"}},"source":{"type":"string","description":"Source of the document store loader"},"credential":{"type":"string","description":"Credential associated with the document store loader"},"rehydrated":{"type":"boolean","description":"Whether the loader has been rehydrated"},"preview":{"type":"boolean","description":"Whether the loader is in preview mode"},"previewChunkCount":{"type":"number","description":"Number of chunks in preview mode"}},"title":"DocumentStoreLoaderForPreview"},"currentPage":{"type":"number"},"storeName":{"type":"string"},"description":{"type":"string"}},"title":"DocumentStoreFileChunkPagedResponse"}}}},"404":{"description":"Document store not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/document-store/upsert-document.api.mdx">
---
id: upsert-document
title: "Upsert document to document store"
description: "Upsert document to document store"
sidebar_label: "Upsert document to document store"
hide_title: true
hide_table_of_contents: true
api: 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
sidebar_class_name: "post api-method"
info_path: docs/api/document-store/document-store-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Upsert document to document store"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/document-store/upsert/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Upsert document to document store

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string","format":"uuid"},"description":"Document Store ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"docId":{"type":"string","format":"uuid","nullable":true,"description":"Document ID within the store. If provided, existing configuration from the document will be used for the new document"},"metadata":{"type":"object","nullable":true,"description":"Metadata associated with the document","example":{"foo":"bar"}},"replaceExisting":{"type":"boolean","nullable":true,"description":"Whether to replace existing document loader with the new upserted chunks. However this does not delete the existing embeddings in the vector store"},"createNewDocStore":{"type":"boolean","nullable":true,"description":"Whether to create a new document store"},"docStore":{"type":"object","nullable":true,"description":"Only when createNewDocStore is true, pass in the new document store configuration","properties":{"name":{"type":"string","example":"plainText","description":"Name of the new document store to be created"},"description":{"type":"string","example":"plainText","description":"Description of the new document store to be created"}}},"loader":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"plainText","description":"Name of the loader (camelCase)"},"config":{"type":"object","description":"Configuration for the loader"}}},"splitter":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"recursiveCharacterTextSplitter","description":"Name of the text splitter (camelCase)"},"config":{"type":"object","description":"Configuration for the text splitter"}}},"embedding":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"openAIEmbeddings","description":"Name of the embedding generator (camelCase)"},"config":{"type":"object","description":"Configuration for the embedding generator"}}},"vectorStore":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"faiss","description":"Name of the vector store (camelCase)"},"config":{"type":"object","description":"Configuration for the vector store"}}},"recordManager":{"type":"object","nullable":true,"properties":{"name":{"type":"string","example":"postgresRecordManager","description":"Name of the record manager (camelCase)"},"config":{"type":"object","description":"Configuration for the record manager"}}}},"title":"DocumentStoreLoaderForUpsert"}},"multipart/form-data":{"schema":{"type":"object","properties":{"files":{"type":"array","items":{"type":"string","format":"binary"},"description":"Files to be uploaded"},"docId":{"type":"string","nullable":true,"example":"603a7b51-ae7c-4b0a-8865-e454ed2f6766","description":"Document ID to use existing configuration"},"loader":{"type":"string","nullable":true,"example":"{\"name\":\"plainText\",\"config\":{\"text\":\"why the sky is blue\"}}","description":"Loader configurations"},"splitter":{"type":"string","nullable":true,"example":"{\"name\":\"recursiveCharacterTextSplitter\",\"config\":{\"chunkSize\":2000}}","description":"Splitter configurations"},"embedding":{"type":"string","nullable":true,"example":"{\"name\":\"openAIEmbeddings\",\"config\":{\"modelName\":\"text-embedding-ada-002\"}}","description":"Embedding configurations"},"vectorStore":{"type":"string","nullable":true,"example":"{\"name\":\"faiss\"}","description":"Vector Store configurations"},"recordManager":{"type":"string","nullable":true,"example":"{\"name\":\"postgresRecordManager\"}","description":"Record Manager configurations"},"metadata":{"type":"object","nullable":true,"description":"Metadata associated with the document","example":{"foo":"bar"}},"replaceExisting":{"type":"boolean","nullable":true,"description":"Whether to replace existing document loader with the new upserted chunks. However this does not delete the existing embeddings in the vector store"},"createNewDocStore":{"type":"boolean","nullable":true,"description":"Whether to create a new document store"},"docStore":{"type":"object","nullable":true,"description":"Only when createNewDocStore is true, pass in the new document store configuration","properties":{"name":{"type":"string","example":"plainText","description":"Name of the new document store to be created"},"description":{"type":"string","example":"plainText","description":"Description of the new document store to be created"}}}},"required":["files"]}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully execute upsert operation","content":{"application/json":{"schema":{"type":"object","properties":{"numAdded":{"type":"number","description":"Number of vectors added","example":1},"numDeleted":{"type":"number","description":"Number of vectors deleted","example":1},"numUpdated":{"type":"number","description":"Number of vectors updated","example":1},"numSkipped":{"type":"number","description":"Number of vectors skipped (not added, deleted, or updated)","example":1},"addedDocs":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}}},"title":"VectorUpsertResponse"}}}},"400":{"description":"Invalid request body"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/feedback/create-chat-message-feedback-for-chatflow.api.mdx">
---
id: create-chat-message-feedback-for-chatflow
title: "Create new chat message feedback"
description: "Create new feedback for a specific chat flow."
sidebar_label: "Create new chat message feedback"
hide_title: true
hide_table_of_contents: true
api: eJztVt9v2zYQ/leIe5Z/JnZaPc1tF8zDuhqNgwILjOFMnSw2EqmQlF3D0P8+HC278uwEzR6HPlkm7767430feTvwuHIQP0BKlCxRPsIiAkeysspvIX7YwZLQkp1UPoP4YVEvIjAlWfTK6GkCMUhL6Ol9hv4jOYcrum2Qbo3l1TQ3G4ggISetKtkNYngfnISmjTgEFqmxAoUrSapUSSEz9IKduxCBpaeKnH9nki3EO5BGe9KeP7EscyVDOr2vjsF34GRGBfKX35YEMZjlV5IeIigtJ+8VOd5VScvGeav0CiJIjS3QQwxVpZKzzO+1eqpIqIS0V6kiG/L2GR0LgToC2RT+XyJMz6GPZ3HAnl7E/SEchij2nbqM8ursGjQGZl7o1SVU0lXBPJv/dv/x3d3f9zOIDt8fPn35k2l3GuhzgLoUpNX9l0/gwERx8GDnwLzkA3p6sfgEPXW8KugMlV0F6kTwrthkpE/aLzboRBMF6joCr3zOMS5IBGo2YHIrSwnE3lYUFlxptNuTdNjv888zlblKSnIurfJ8e4wa/RTIT4H8bwRSR3B9SQNTvcZcJULpsvKitGatEkaMYHTZ3JPVmAtHdk1WkLXGhvgF+czwU1YaFzSA/NZB78hXfhHZx4UHsbI5xJB5X7q41yutSbrOV4kyXZ8Rarch20XVw1L11oOzw5lZk1SS/zSJQB21MeNeLzcS88w4H1/3+/3ngP5gK5HQmnJTFqT9Ea/1gt+x0Pdabr/jx7ZyRC4vmEHcGEHUfNwemv37l3k4Kr5APn9/i3/9hkWZ0+GqgKsU34zS8XVndDO46VyPxsPO8iqVnaF8O75Kx2NMcQyn4n+NT5g3jkxsafRHQQ7qOxHZUS3foU80AMP+8LrTv+kM384Ho3g0iIdvuv2bwV9MNaVTE86z4fBRUZPZ9Kxlk9k0yLVAjSvWbri2mjpEi27Mtb3LoNvv9pskUQZJawytmgSmTabiripLY5m4p8xss5HvlwIVb7u9+S8n29xbZn+BuhWiNaU9l+lJga3B7NUTXkNIT998r8xRaT7dUNCuEeTpjMr64LXdbomO7m1e17z8VJHluXURwRqtwiV35WFRR5ARJmSDgh9pyxnuc+3MOTKb5xVncPZesj73HhMpqfQv2i5a18ns092cpdQMrYVJ2MfiJjBxAzEAD9PsHRQa1naQo15VfJPHsMfk5uCpbv+l01BVs4V628pwt9tbzM0j6bqGqCnF83+oF3Vd/wOu3BWW
sidebar_class_name: "post api-method"
info_path: docs/api/feedback/feedback-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create new chat message feedback"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/feedback"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create new feedback for a specific chat flow.

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the feedback"},"chatflowid":{"type":"string","format":"uuid","description":"Identifier for the chat flow"},"chatId":{"type":"string","description":"Identifier for the chat"},"messageId":{"type":"string","format":"uuid","description":"Identifier for the message"},"rating":{"type":"string","enum":["THUMBS_UP","THUMBS_DOWN"],"description":"Rating for the message"},"content":{"type":"string","description":"Feedback content"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the feedback was created"}},"title":"ChatMessageFeedback"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Feedback successfully created","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the feedback"},"chatflowid":{"type":"string","format":"uuid","description":"Identifier for the chat flow"},"chatId":{"type":"string","description":"Identifier for the chat"},"messageId":{"type":"string","format":"uuid","description":"Identifier for the message"},"rating":{"type":"string","enum":["THUMBS_UP","THUMBS_DOWN"],"description":"Rating for the message"},"content":{"type":"string","description":"Feedback content"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the feedback was created"}},"title":"ChatMessageFeedback"}}}},"400":{"description":"Invalid input provided"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/feedback/feedback-api.info.mdx">
---
id: feedback-api
title: "Feedback API"
description: "API for managing chat message feedback"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Feedback API

This section contains the API endpoints for the feedback service.

## Overview

The Feedback API provides endpoints for managing feedback resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/feedback/get-all-chat-message-feedback.api.mdx">
---
id: get-all-chat-message-feedback
title: "List all chat message feedbacks for a chatflow"
description: "Retrieve all feedbacks for a chatflow"
sidebar_label: "List all chat message feedbacks for a chatflow"
hide_title: true
hide_table_of_contents: true
api: eJy1Vk1v4zYQ/SvEnHYBNfYW7UWnupumdbHbDTYJ9hAYxVgaWdxIpEKOnAaC/nsxlGTLHzGcoPXFksh58zhffA0wrjzE95ARpUtMHmARgaekdpqfIb5vYEnoyM1qziG+X7SLCFLyidMVa2sghq/ETtOaFBaFGlC8yqxTqJIcOSvsE0RgK3IoNvMUYlgRz4riY478mbzHFV0N/iOo0GFJTM4HAlq8VMg5RGCwJIhBpxCBo8daO0ohZldTBD7JqUSIG+DnSnZ5dtqsoN1n/LEnpeaX0Ea9g8ea3PPWgxCfi5dXoar5pWKrMl0wuVEw3tmwC4v3Lzr01vEXl5I74TMCMnUp2UKfQEcAQkIyrAuGOHw/YHZjHSsr2Mpmr2TF6PgSmU6yyqwrUdynyPQD65IOSVztxyRAa7NSmbOl4lx7JebnsCKT/j+c6krSd5zLQirOV9Z48uLwx+lU/vZCXScJeZ/VhdrUO0SQWMNkWPZjVRU6CQuT716MmsNToHMo59VMpR99t8vvlLC0iBN41h0VnZ6MQF2Hftllemf0Y01Kp2RYZ1rCYJ3inDbhkHgN/fsWD/NDaIFTYR702POjuGfhCETZzY7jKK9m16MJsGTOrE514O0fd59/vfn77hqi4fnyy7e/4HA+Ylflh05GVXE6AsNsVIOFGDtCpq4Pziv/fVQxVWhSJavqKSezk371hF71XqBtI2DNBfWDbn9ot/KL4OdjPTE3TM5goTy5NTlFzlkXEEvi3Pa3QZj7csfAZGAwaXTaSpMHu+4yqF0BMeTMlY8nk8rZ9MJznWp7wTmh8U/kLlBPsNKT9YeDI187m9aJvPRkwojZYsaTSWETLHLrOf5pOp2+BPRJdqmU1lTYqiTDG7zR9Xkjbd116PgS3SRLPA4zTN67TRD1D1dDCv/8dhvCpU1mg3mfiE1ZzK7nBwxn1/NQcyUaXEkBht7rq2/b4xFIaDuTDxfTi2k/rTAJddkP3FkI7Gyubuqqsk6StZuIcfClSUrUsuy77b/sLMtRKuu5RDNy8Ul7DhLiKM8jemLnuM22mc4WJH0WmP7hSVWgNtJW4VhNX4ojVRRBrFNpbikNWWmaJXq6c0XbyufuhpIKTbXHZSGyJMPC0wmiZyqGF5g+0PNYp6yxqGVTqOjzOZyhDU76H8uWN1J4izI4zWkkWv4rTieUwUkuW6myZbKQF6eFyisr5t3XXvO+V2MR+wKDQUyY57H7gZlOoV20EeSEkj4h0i3MkoQqHpkcSBY5wWZ2//7bLUSAu6Ntb5QF9KN0mqbbcWsfyLTthh3LuxBs238BtUxUfg==
sidebar_class_name: "get api-method"
info_path: docs/api/feedback/feedback-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all chat message feedbacks for a chatflow"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/feedback/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve all feedbacks for a chatflow

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"},{"in":"query","name":"chatId","schema":{"type":"string"},"description":"Chat ID to filter feedbacks (optional)"},{"in":"query","name":"sortOrder","schema":{"type":"string","enum":["asc","desc"],"default":"asc"},"description":"Sort order of feedbacks (optional)"},{"in":"query","name":"startDate","schema":{"type":"string","format":"date-time"},"description":"Filter feedbacks starting from this date (optional)"},{"in":"query","name":"endDate","schema":{"type":"string","format":"date-time"},"description":"Filter feedbacks up to this date (optional)"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the feedback"},"chatflowid":{"type":"string","format":"uuid","description":"Identifier for the chat flow"},"chatId":{"type":"string","description":"Identifier for the chat"},"messageId":{"type":"string","format":"uuid","description":"Identifier for the message"},"rating":{"type":"string","enum":["THUMBS_UP","THUMBS_DOWN"],"description":"Rating for the message"},"content":{"type":"string","description":"Feedback content"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the feedback was created"}},"title":"ChatMessageFeedback"}}}}},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/feedback/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/feedback/feedback-api",
    },
    {
      type: "category",
      label: "feedback",
      items: [
        {
          type: "doc",
          id: "api/feedback/create-chat-message-feedback-for-chatflow",
          label: "Create new chat message feedback",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/feedback/get-all-chat-message-feedback",
          label: "List all chat message feedbacks for a chatflow",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/feedback/update-chat-message-feedback-for-chatflow",
          label: "Update chat message feedback",
          className: "api-method put",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/feedback/update-chat-message-feedback-for-chatflow.api.mdx">
---
id: update-chat-message-feedback-for-chatflow
title: "Update chat message feedback"
description: "Update a specific feedback"
sidebar_label: "Update chat message feedback"
hide_title: true
hide_table_of_contents: true
api: eJztVttu4zYQ/RVinlpAji+xnV091btpUBfdNkgcLNDAKGhxZHEjkQxJ2WsI+vdiKNmxY8fY7WOxT7pw5syFc2amAs+XDuJHSBHFgidPMI/AYVJa6TcQP1awQG7RTkqfQfw4r+cRCHSJlcZLrSCGByO4R8aZM5jIVCZsBxWBNmg5CU4FxFAG0Y8Z95/QOb7Em1byRlv6m+Z6DREYbnmBHq0LDkiyYrjPIALFC4QYpIAILD6X0qKA2NsSI3BJhgWHuAK/MSTlvJVqCfVrj8kUaz1gWxfY9BooOEJF5z9osSGoRCuPytMrNyaXSYim+8URUnVsUy++YOIpCEuxe4mOTqU49iuCVNuCe0pMGSJ6lVgln0tkUqDyMpVoWaot8xm+JLiOIGnz9l8sTI+hCY6Fe2ixpydxvwmHIIomzadRvtu7Fo2AqazU8hQqqrKgip799vDpw/0/D7cQbd+v//r8JxxV8F2AOmVk7/bPZ2BXRVsNUrbIPYpr7vFs8MSJjpcFHqFeB2IpweiUrTNUB9fP1tyx1grUdQRe+hzbAn/FMKhrkrDojFauKcpBr0ePNyJxZZKgc2mZ5xvWMJfu6AchfhDif0SI4SkOTNWK51IwqUzpmbF6JQUhkvjwDGXW0mfBoXYUomDT6+CU0p6lulQBZHTapkereM4c2hVahtZqG4Io0GeaxqcpA49oEsbQ3QbdraSogYY26TUjs7Q5xJB5b1zc7RqrxYXzpZD6wmfIlVujveCyy43srvpHWb61WpQJfbTOQB3tY8bdbq4Tnmfa+XjY6/XeAvqDpJjAFebaFKj8Dm9vybinjtE0hf1VY1cfZBHa6U7fjRBE7cvNtmp+/zwL6aJOdPcyxH/9yguT47bnwGXK343S8bAzuupfdYaj8aCzuEyTziB5P75Mx2Oe8jEcdpHv0Ql7zq6k98j+rSBbGh+wdUe7F+gDMsGgNxh2eledwftZfxSP+vHg3UXvqv83lZtUqQ75bMmwK9fJ7fToyia308D7giu+pCYQ+l8bx/5mR7XWqPQvehe91kmehN7QrmmTUGmTKbsvjdGWqvewMverkRpVwSUdu0b8l4NjulujnS+42jPRbp9veXkQ3N42d35rbSvP41ffNTmXitIYPK9a+u3tyxHEUlAPJTrQSVUtuMMHm9c1/X4u0dImPY9gxa3kC7qExwqEdPQuIE557vCMrz/dtZvuz+zN1fUNp9ufXG3oynhe0hdE8ISbZo2u53UEGXKBNnjVHHxsbHdmpP6ieDTwqS80GpMkQePPys73Wtntw4wY3C7ZhRakYvk6EGDd+KhDKkJjCP8qyLlaljSJYmggqSb4Ybt41R5CUCezUFWNxEw/oarrXVI8fVNe6vpfGBZy+A==
sidebar_class_name: "put api-method"
info_path: docs/api/feedback/feedback-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update chat message feedback"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/feedback/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Update a specific feedback

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chat Message Feedback ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the feedback"},"chatflowid":{"type":"string","format":"uuid","description":"Identifier for the chat flow"},"chatId":{"type":"string","description":"Identifier for the chat"},"messageId":{"type":"string","format":"uuid","description":"Identifier for the message"},"rating":{"type":"string","enum":["THUMBS_UP","THUMBS_DOWN"],"description":"Rating for the message"},"content":{"type":"string","description":"Feedback content"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the feedback was created"}},"title":"ChatMessageFeedback"}}}}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Feedback successfully updated","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the feedback"},"chatflowid":{"type":"string","format":"uuid","description":"Identifier for the chat flow"},"chatId":{"type":"string","description":"Identifier for the chat"},"messageId":{"type":"string","format":"uuid","description":"Identifier for the message"},"rating":{"type":"string","enum":["THUMBS_UP","THUMBS_DOWN"],"description":"Rating for the message"},"content":{"type":"string","description":"Feedback content"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the feedback was created"}},"title":"ChatMessageFeedback"}}}},"400":{"description":"Invalid input provided"},"404":{"description":"Feedback with the specified ID was not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/leads/create-lead.api.mdx">
---
id: create-lead
title: "Create a new lead in a chatflow"
description: "Create a new lead associated with a specific chatflow"
sidebar_label: "Create a new lead in a chatflow"
hide_title: true
hide_table_of_contents: true
api: eJztV02P2zYQ/SvEXCvZsmKvbZ2y7aaAg6BdNJsW6MKHkThaMZFIhaTWWRj678XQH5Fjb7vtqYecLJOPM2/IecPhFjw+OMjuoSaUDtYROCo6q/wTZPdbyAkt2evOV5Ddr/t1BKYli14ZvZKQQWEJPb0jlBCBJFdY1fIkZPBTmBIoNG0EWxfonCkUepJio3wlULiWClWqQhQV+rI2G4jA0ueOnP/RyCfItlAY7Ul7/sS2rVURnI8/OnayBVdU1CB/+aeWIAOTf6TCQwStZapekeNZJQcY563SDxBBaWyDHjLoOnUewQetPncklCTtVanIitJY4SsK4UAE9AWbtmaLRSlnryaUxIu0LOLJhJZxXkzTeJZeYTmfX03Lqyn0EWhs6BKRU8e/YEPClJddvTWVFjeG2Bw1qOp/tveGYQKltOTcs4Y/mkqPpKHX+6FRYRp20lZGv4D0LcOE7pqc7LM+fpikr6azq/limbDpw7FfPp1T+6ubg9XDqqMLody3yXXidV5MaZHPMZ7nS4yn+VTGy/IVxalMF+WEcsIkPdBZ/WsqwpFzyugX05HzPMlnchEvZnQVT8sU42UxKeOlXGJCKeXlVR7oBAXJG/QXt/+YvBI9xV41dEb0JihQS8GzYlPRgOQGndh7OGGXJuk0ThZxOr2bTLPJLEvTP6HvI/DKB0BQe88jLFVlSULmbUdhwLVGu53k0iThn1NGvPjgVriuKMi5sqvrJ4i+S/271L9L/X8p9T6C6SU1r/Qj1kqK/Z0tcr60GZym5+DfGRokLchaYxk4u2zVk9VYC0f2kewB3UfQkK8M9x2tcUH4yI0JjHfNC/cuvMCF1qWzNWRQed+6bDxurZEj5zupzMhXhNptyI5QjbFV48fJeYpbI7sikN0ZhT4a2szG49oUWFfG+WyaJMlzht4xSkh6pNq0DWl/tDfotd5zadtVr2HHdUwE9sjhBRhkexBE+4+fD+nx9o+7sE9cMn/72ke9OZz4rji+rIQdKtjXKnQsQpfrx7F8nEj/VPkv0+dXeb5MQN/o55m8jkDp0oRdHaS3E9e3q7NTu75dhRugQY0PSj8EHTmh9LEicbpxru3wk1EySvZXGBbhCtvv3nXItOuVeN+1rbGctaeZOczGwRa7Hfz1yTSfLad+g3rg4rzTVlrgsKc+CW7QU//nJn2fl56++HFbo9K8vSGu7V6UgxcFa4QHttscHX2wdd/z8OeOLL8y1hE8olWY85ncr/sIKkJJNqj4Ez0xzR3h+I7dMrzu2P1Zl8Aa3a24Lgpq/d9i14N6cvvr+zuW0/7R0RjJayyGBwluIAPgpw+vDioNY1uoUT90+BCu1GCTDwhPtfuNVkNU+ynUTwOG2+0OcWc+ke57iPaheP4P/brv+78AQoGJaw==
sidebar_class_name: "post api-method"
info_path: docs/api/leads/leads-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new lead in a chatflow"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/leads"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create a new lead associated with a specific chatflow

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the lead","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the lead","example":"John Doe"},"email":{"type":"string","description":"Email address of the lead","example":"john.doe@example.com"},"phone":{"type":"string","description":"Phone number of the lead","example":"+1234567890"},"chatflowid":{"type":"string","description":"ID of the chatflow the lead is associated with","example":"7c4e8b7a-7b9a-4b4d-9f3e-2d28f1ebea02"},"chatId":{"type":"string","description":"ID of the chat session the lead is associated with","example":"d7b0b5d8-85e6-4f2a-9c1f-9d9a0e2ebf6b"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the lead was created","example":"2024-08-24T14:15:22Z"}},"title":"Lead"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Lead created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the lead","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the lead","example":"John Doe"},"email":{"type":"string","description":"Email address of the lead","example":"john.doe@example.com"},"phone":{"type":"string","description":"Phone number of the lead","example":"+1234567890"},"chatflowid":{"type":"string","description":"ID of the chatflow the lead is associated with","example":"7c4e8b7a-7b9a-4b4d-9f3e-2d28f1ebea02"},"chatId":{"type":"string","description":"ID of the chat session the lead is associated with","example":"d7b0b5d8-85e6-4f2a-9c1f-9d9a0e2ebf6b"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the lead was created","example":"2024-08-24T14:15:22Z"}},"title":"Lead"}}}},"400":{"description":"Invalid request body"},"422":{"description":"Validation error"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/leads/get-all-leads-for-chatflow.api.mdx">
---
id: get-all-leads-for-chatflow
title: "Get all leads for a specific chatflow"
description: "Retrieve all leads associated with a specific chatflow"
sidebar_label: "Get all leads for a specific chatflow"
hide_title: true
hide_table_of_contents: true
api: eJydVm1v2zYQ/ivEfdowyZYV+U2faixt4aIYgibFgAX5cBJPEVOJVEnKaWDovw8ny65dO1s3f7FI3j333AvvuAWPjw7Se6gIpYOHABzlrVX+BdL7LWSEluyq9SWk9w/dQwCSXG5V45XRkMIn8lbRhgRWleghBDpncoWepHhWvhQoXEO5KlQu8hJ9UZlnCMA0ZJFB1hJSeCS/qqqPrP/O2N+/izVosSZP1vV0FNts0JcQgMaaIAUlIQBLX1tlSULqbUsBuLykGiHdgn9pWMp5q/QjdD/y35sS62tg7yy5xmhHjnXjKOK/U43bNs/JuaKtxMEHCCA32pP2LI9NU6m8Pxg/OVbanhNCa/EFAlCeane0b7Inyj17bhneqx0VJc+dCaAwtkYPKbRtH4ZTpp+1+tqSUJK0V4UiKwpjhS+pTxQEQN+wbipGzAs5vZpQFC7iIg8nE1qGWZ7E4TSeYTGfz5JilnDwdjE/J3Jq+A+sSZjisqkPptTi2hDDUY2q+ne8tywmUEpLzr0K/GRKPZKG3gxbo9zUbKQpjf4J0jcsJnRbZ2RftfHbJL5KprP5Yhkx9L6eL2fnFH99vUfdax1MCHV2a06szvOEFtkcw3m2xDDJEhkuiysKYxkvigllhFG8p7P+z1SEI+eU0T9NR86zKJvKRbiY0ixMihjDZT4pwqVcYkQxZcUs6+lYYoBr9BfDfyheiZ5Cr2o6I8qqArUUfCqeSzoi+YxODBZO2MVRnITRIoyTu0mSTqZpHP8FXReAV74X4C4DHf8CSC7d8LXeYKWkWF+LxpqNkiShl03OZfuWJbTxojCt7uWmlzE9WY2VcGQ3ZAVZa2xPqyZfmqEH9g2PWy2M+1463irZAbdkVtq1wNZWkELpfePS8bixRo6cb6UyI18SavdMdoRqjI0abybnVW6NbHNeDEygC44x0/G4MjlWpXE+TaIoeg3oI0sJSRuqTFOT9ge8oxFyy11v18COB8mhFtgiDN2a1zshCIaPd/sK+fDnXR8rpQvTqx+l0onVzfqM3upm3Xe7GjU+Kv04zCalD7fPQQAc1J38ZBSNoqGNY9638WG+rPqQrtbitm0aYzlHpyk4Djscehq4nfibk2N2ojHO16iPTLwnfzQ/mfblmXni4vb7yPn/U3jIg6dvftxUqDQXcO/edqjE/cMggFRJfh5wZfD2dpuho8+26jre/tqS5SfDQwAbtAozTtD9FqRy/C0hLbBy9A9O/PJpGOK/iuOp/ArJ/RjVPEQ3WLW8ggC+0MvuUdA9dAGUhJJsT2R3sMpzavyRytmw5vo93Mn3b+8gADyt2h+qtEe/SGe73UncmS+ku+7AzvOaCXbd3yLmM9Q=
sidebar_class_name: "get api-method"
info_path: docs/api/leads/leads-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get all leads for a specific chatflow"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/leads/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve all leads associated with a specific chatflow

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the lead","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the lead","example":"John Doe"},"email":{"type":"string","description":"Email address of the lead","example":"john.doe@example.com"},"phone":{"type":"string","description":"Phone number of the lead","example":"+1234567890"},"chatflowid":{"type":"string","description":"ID of the chatflow the lead is associated with","example":"7c4e8b7a-7b9a-4b4d-9f3e-2d28f1ebea02"},"chatId":{"type":"string","description":"ID of the chat session the lead is associated with","example":"d7b0b5d8-85e6-4f2a-9c1f-9d9a0e2ebf6b"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the lead was created","example":"2024-08-24T14:15:22Z"}},"title":"Lead"}}}}},"400":{"description":"Invalid ID provided"},"404":{"description":"Leads not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/leads/leads-api.info.mdx">
---
id: leads-api
title: "Leads API"
description: "API for managing leads in chatflows"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Leads API

This section contains the API endpoints for the leads service.

## Overview

The Leads API provides endpoints for managing leads resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/leads/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/leads/leads-api",
    },
    {
      type: "category",
      label: "leads",
      items: [
        {
          type: "doc",
          id: "api/leads/create-lead",
          label: "Create a new lead in a chatflow",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/leads/get-all-leads-for-chatflow",
          label: "Get all leads for a specific chatflow",
          className: "api-method get",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/ping/ping-api.info.mdx">
---
id: ping-api
title: "Ping API"
description: "Simple API for checking server status"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Ping API

This section contains the API endpoints for the ping service.

## Overview

The Ping API provides endpoints for managing ping resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/ping/ping-server.api.mdx">
---
id: ping-server
title: "Ping the server"
description: "Ping the server to check if it is running"
sidebar_label: "Ping the server"
hide_title: true
hide_table_of_contents: true
api: eJyVU02P0zAQ/SvRnKMki+DiEz0AKuKwUkEcqh6myTSxNrHNeFK2ivLf0Thhty1w4JR4/ObrvecJGoo12yDWOzDwaF2bSUdZJD4TZ+KzuqP6KbOnzEpmY8ajc9a1kIMPxKh52wYMBOvaXUqCHATbCGafgnDIgSkG7yJFMBO8qSr93DZeUm8b1N4JOVGw0LOUoUfr9BTrjgZM8UsgMBCFlxR6xiH0GgretTDP85zDu7813Dohdtj/3pSYPYPCB5LO60otCeQQUDowUIalwwLX7SYYuQcDnUiIpiwD+6aIMjbWF9IRuviTuEBbYrDl+QHye67ZN2Oth3UGmPPrmqYse19j3/ko5m1VVf8q9EVRWUNn6n0YyMlLvYPOW49s5bJTzhYBjoRMvBl1rxcKtaOul2BgVhDk689HzwMKGPj8/WtiybqTT+lWEt/JOZvH7R/T7awqolfZyfNiJ8WuvEdBGSPkoKwuGQ9FVVSr/lgn/R2moTaJ0802240heFZ5bjW45l3tMKDV67jA399c6xbBRxnQXbW4ewH360yvrvyvx7KyfOXjeZ19Wh32+lpUcD1P0xEjfeN+njX8YyS+gNkfcjgjWzwq7/vDnENH2BAnSz7RRXmqawrKzhn78b7v4crinz6omr8AipBg1A==
sidebar_class_name: "get api-method"
info_path: docs/api/ping/ping-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Ping the server"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/ping"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Ping the server to check if it is running

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Server is running","content":{"text/plain":{"schema":{"type":"string","example":"pong"}}}},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/ping/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/ping/ping-api",
    },
    {
      type: "category",
      label: "ping",
      items: [
        {
          type: "doc",
          id: "api/ping/ping-server",
          label: "Ping the server",
          className: "api-method get",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/prediction/create-prediction.api.mdx">
---
id: create-prediction
title: "Create a new prediction"
description: "Create a new prediction for a specific chatflow"
sidebar_label: "Create a new prediction"
hide_title: true
hide_table_of_contents: true
api: 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
sidebar_class_name: "post api-method"
info_path: docs/api/prediction/prediction-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new prediction"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/prediction/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create a new prediction for a specific chatflow

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"question":{"type":"string","description":"The question being asked"},"overrideConfig":{"type":"object","description":"The configuration to override the default prediction settings (optional)"},"history":{"type":"array","description":"The history messages to be prepended (optional)","items":{"type":"object","properties":{"role":{"type":"string","enum":["apiMessage","userMessage"],"description":"The role of the message","example":"apiMessage"},"content":{"type":"string","description":"The content of the message","example":"Hello, how can I help you?"}}}},"uploads":{"type":"array","items":{"type":"object","properties":{"type":{"type":"string","enum":["audio","url","file","file:rag","file:full"],"description":"The type of file upload","example":"file"},"name":{"type":"string","description":"The name of the file or resource","example":"image.png"},"data":{"type":"string","description":"The base64-encoded data or URL for the resource","example":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAABjElEQVRIS+2Vv0oDQRDG"},"mime":{"type":"string","description":"The MIME type of the file or resource","example":"image/png"}}}}},"title":"Prediction"}},"multipart/form-data":{"schema":{"type":"object","properties":{"question":{"type":"string","description":"Question to ask during the prediction process"},"files":{"type":"array","items":{"type":"string","format":"binary"},"description":"Files to be uploaded"},"modelName":{"type":"string","nullable":true,"example":"","description":"Other override configurations"}},"required":["question"]}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Prediction created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"text":{"type":"string","description":"The result of the prediction"},"json":{"type":"object","description":"The result of the prediction in JSON format if available"},"question":{"type":"string","description":"The question asked during the prediction process"},"chatId":{"type":"string","description":"The chat ID associated with the prediction"},"chatMessageId":{"type":"string","description":"The chat message ID associated with the prediction"},"sessionId":{"type":"string","description":"The session ID associated with the prediction"},"memoryType":{"type":"string","description":"The memory type associated with the prediction"},"sourceDocuments":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}},"usedTools":{"type":"array","items":{"type":"object","properties":{"tool":{"type":"string","example":"Name of the tool"},"toolInput":{"type":"object","additionalProperties":{"type":"string"},"example":{"input":"search query"}},"toolOutput":{"type":"string"}},"title":"UsedTool"}},"fileAnnotations":{"type":"array","items":{"type":"object","properties":{"filePath":{"type":"string","example":"path/to/file"},"fileName":{"type":"string","example":"file.txt"}},"title":"FileAnnotation"}}}}}}},"400":{"description":"Invalid input provided"},"404":{"description":"Chatflow not found"},"422":{"description":"Validation error"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/prediction/prediction-api.info.mdx">
---
id: prediction-api
title: "Prediction API"
description: "API for creating and managing predictions in TheAnswer"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Prediction API

This section contains the API endpoints for the prediction service.

## Overview

The Prediction API provides endpoints for managing prediction resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/prediction/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/prediction/prediction-api",
    },
    {
      type: "category",
      label: "prediction",
      items: [
        {
          type: "doc",
          id: "api/prediction/create-prediction",
          label: "Create a new prediction",
          className: "api-method post",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/tools/create-tool.api.mdx">
---
id: create-tool
title: "Create a new tool"
description: "Create a new tool"
sidebar_label: "Create a new tool"
hide_title: true
hide_table_of_contents: true
api: eJztV9tu2zgQ/RVi9mFfZMt2nKSrp/W2GyBF0Qa1swUaGAVNjm22EqnwEtcw9O/FULIjX9IGCyywD3mxJfJwLuScw9EGJDphVemV0ZDBa4vcI+NM44p5Y3JIwJRoOc1fS8hARMSknvJ84SC7A0I6mCbgUASr/Bqyuw3MkFu0o+CXkN1Nq2kCFu8DOv+XkWvINiCM9qg9PfKyzJWIbtKvjmLZgBNLLDg9+XWJkIGZfUXhIYHSUlBeoaNZJVsY563SC0hgbmzBPWQQgpKQHCR6q9V9QKYkaq/mCi2bG8v8ErdZ43delDlZFHN5ftbHXufVYC46/T7+0ZmJ4aBzPrjg88vLi+H8YghVApoXeCqQfcfveYHMzE+7ktzjF68K/BKnqoPFv7L95vHtSRejOMSCQxlTlvG8tWTklu3O2pFzYXJjf+32NcEYd84IxT1KtlJ+edr7b1dX55dnZ2RcCaPHVpwyr0Oe8xkt8DbgobuxCVYgu/34bu/MfneMTO65W3pfuixNm5GuMEVKIJdS2t1SLyiSozp7ZiBvxx/es3rx09lXCcyD/hdpXgUt6JHnyq+ZbB+tZcJI/KnPmqbyDfcna3JHDtqHDh39cTHtFcZqiXrngK24Y42Hvf0e9AbDTu9VZzCc9IdZ/zwbDD5TNKGU/3E0OXeeNW6eEVKVgFc+AqKUVTRC6qQsyvo84oArjXa1ygx6PfrbD4sWb3eCuSAEOjcPeb6G5EXdXtTtRd1e1O3/oW5VAsNTAnatH3iuJGs6Mzaj1ozAg8Ex+B+CRgIxtNZYAp6fturRap4zh/YB7RZdJVCgXxrqI0vjotZx6g4hrTtIaiBpgYv9Y7B5q8pLa2TX+SCV6folcu1WaLtcpbxU6UP/aE9vrJEh1lgTBVRJ22aWprkRPF8a57Nhr9d7ytA7QjGJD5ibskDtd/ZaDe+YiFILdrvt3VUDeYSGjPRegyBpHq62NfL20yTuE90SHx+75b+3J17fB89T7a1oHwrvYYLPFs2dZu6UriV0z5GjRzVq0SPqx+P7HrdPF/gB5Z4kptJzE8+gRQbHRjfXx1twcx1TL7jmC6UXTATnTcG2ZUk1WSP73V6319zuXMTbvdnlUazI0TUbh7I0lqp7v4LbVUssLriiaVfD/9ybphogihRct1yc+jw7uEN3XcdJcFONHr/7tMy50lG/KMpNQ8XWxxwxgwY2mxl3eGvzqqLh+4CWPvCmCTxwq2qNv5tWCSyRS7SRu99wHe/OGExnQm4Jngdyf9QOETPrFSMhsPQ/xU5bKnLzYTwhEjUflIWRtMbyFSTxN4P4AVuXb7apxzaQc70IfEHY2iZtN99n7AFDY1bNFNfrVoSbTY2YmG+oqwqSJhVP71BNq6r6AW73NgU=
sidebar_class_name: "post api-method"
info_path: docs/api/tools/tools-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new tool"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/tools"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create a new tool

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the tool","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the tool","example":"date_time_tool"},"description":{"type":"string","description":"Description of the tool","example":"A tool used for date and time operations"},"color":{"type":"string","description":"Color associated with the tool","example":"#FF5733"},"iconSrc":{"type":"string","nullable":true,"description":"Source URL for the tool's icon","example":"https://example.com/icons/date.png"},"schema":{"type":"string","nullable":true,"description":"JSON schema associated with the tool"},"func":{"type":"string","nullable":true,"description":"Functionality description or code associated with the tool"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the tool was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the tool was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Tool"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Tool created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the tool","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the tool","example":"date_time_tool"},"description":{"type":"string","description":"Description of the tool","example":"A tool used for date and time operations"},"color":{"type":"string","description":"Color associated with the tool","example":"#FF5733"},"iconSrc":{"type":"string","nullable":true,"description":"Source URL for the tool's icon","example":"https://example.com/icons/date.png"},"schema":{"type":"string","nullable":true,"description":"JSON schema associated with the tool"},"func":{"type":"string","nullable":true,"description":"Functionality description or code associated with the tool"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the tool was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the tool was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Tool"}}}},"400":{"description":"Invalid request body"},"422":{"description":"Validation error"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/tools/delete-tool.api.mdx">
---
id: delete-tool
title: "Delete a tool by ID"
description: "Delete a specific tool by ID"
sidebar_label: "Delete a tool by ID"
hide_title: true
hide_table_of_contents: true
api: eJyFVE1v2zAM/SsCTxsgxOnQXXRagHSAhx6KNcMOQQ6KxcRCZUmV6GyB4f8+0PayJEu3k/XxSD498rkDg7lKNpINHhQs0SGh0CJHrOzOVoJCcGJ7FOUSJISISTO0NKDADOBVCA4kkN5nUGtgfIaNhIxVmywdQa072KJOmBYt1aDWm34jIeqkGyRMeQBYrh411SDB6wZBgTUgIeFraxMaUJRalJCrGhsNqgM6RkZlStbvoZdXL2FaTJprJcwx+IyZ4z7M5/y5gR7fY0Ruqwpz3rXOHTnx/a2I0h+0s0aUSxFTOFiDZsTev5HdBxK70PoB9vF2SsLktRMZ0wGTwJRCgr6X0CDV4Y/kIEepFBSD3EVnTQ8sOceNgrbJgYKaKGZVFDEFM8vUGhtmVKP2+QemmbaFjrY43MG1ek8pmLbizUQGenmeUxWFC5V2dcik7ufz+VuJHhklDB7Qhdigp1O+sxF55qaO3TkflFOPuSJMvef9CAI5LT6H1GgCBV++rwa5rN+FIdySw0n/LBZP5V/0Fk+l2IUkGu313vq9qNpMoRHjFEtgOUfk3Ww+m4OEKnjSFXH6aU4Xg5iLUjy3MYZEIK/EPxccJGCjLV/nEf7p4prpx5Cp0f6sxMmWF268eEo3MENP/3fxpCrhTyqi09bzRA6Uu2mufttYgrKGzcx95uOu2+qM35Lrez5+bTGxwTcSDjpZvWW51x0Ym3ltQO20y/gPqu++TgZ/L3479g2C06H2R+6Ldi3vQMILHsefRb85d8ry4fFh9QAS9OUsXc3OQPdm6q4bEavwgr7vT5WI91ys738BTknG7g==
sidebar_class_name: "delete api-method"
info_path: docs/api/tools/tools-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete a tool by ID"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/tools/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete a specific tool by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Tool ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Tool deleted successfully"},"400":{"description":"Invalid ID provided"},"404":{"description":"Tool not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/tools/get-all-tools.api.mdx">
---
id: get-all-tools
title: "List all tools"
description: "Retrieve a list of all tools"
sidebar_label: "List all tools"
hide_title: true
hide_table_of_contents: true
api: eJy1VVFv2zYQ/ivE7WEvsmU7TtLpqca6DCmCLqgdDFhgFAx1stlSpEoenRqG/vtwkuLKjrMUBeYXi7yP9328O97tIMegvK5IOwsZfETyGjcopDA6kHCFkMYIcs4ESMBV6CVDr3PIYIU0M2bR2UiuAmT30GKXCQRU0WvaQna/gweUHv0s0hqy+2W9TMBjqJwNGCDbwWQ04r9DMfOoFIZQRCP2xJCAcpbQEuNlVRmtGkP6OfChHQS1xlLyF20rhAyk93ILCWjCMvT23cNnVAQJVJ7dk26l6LyHCeS1XUEChfOlJMggRp1DcqT0zuqvEYXO0ZIuNHpROC9ojU3kIAH8JsvKsEdV5OdnYxwN3kwKNRiP8bfBg5pOBueTC1lcXl5Mi4sp1AlYWeIpIYfEH2SJnKSTVLkk/ES6xE+NqT46/Jrvd99XL1LMmi0RA+bNlZlSSJsLpv2etcDkyhnnX6f9nWFChuCUloS5eNS0Ps3+y9XV+eXZGTvXytm5V6fc22iMfOAD5CMe081d9ArF3cebg5z9GgS7PKBbE1UhS9NuZ6hcmTIopHztYWVXrORZ/f2gkPfzvz6I9vDLt68TKKL9iWteRav4UxpNW5H3U+uFcjn+J6fyyIZ3kk7W5P5xcBwGnPrnxXRQGI9rtHsC8SiD6BgO4j0ZTaaD0ZvBZLoYT7PxeTaZ/MNqYpX/z2qMDCQ6mh+QVCdAmhrAookY/xI4P9XUri2ht9KIgH6DXqD3zjcuSqS16/oqdyXJvRLSp97b4kPTTaM3vXqsvMuHgWKu3ZDWKG14RD+UOpWVTjfjZ7e/9S6PTTV0IqBO+j6zNDVOSbN2gbLpaDR6ydENo0SOGzSuKtHS3l+v/c+5pNvW2h8C+7wxI3TPhtctCJLu4+opm+//XjRh0rZwzfFexIOY3V4/kze7vW7edCmtXGm7EioGcuV+mnE4W+R4OBqOutEiVTNa2v4LsyaYs2sxj1XlPCfmMPj9gHOplFKzObTwtwdmll+5QKW0PYobHrT9KXvUpffz7rXp3AWU8BullZHaNo+F1e66aupNZ04ub+x2DzLgnTd1zdtfI3qe2MsENtLrtqHcL+sE1ihz9E35fcEth0YprDggG2liM2iPpzGXwb6q//xjAQnIw+QfJbvx/jS27bbne7drEQv3BW1dQ9KJIF5Dvazr+l+fWACf
sidebar_class_name: "get api-method"
info_path: docs/api/tools/tools-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all tools"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/tools"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a list of all tools

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the tool","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the tool","example":"date_time_tool"},"description":{"type":"string","description":"Description of the tool","example":"A tool used for date and time operations"},"color":{"type":"string","description":"Color associated with the tool","example":"#FF5733"},"iconSrc":{"type":"string","nullable":true,"description":"Source URL for the tool's icon","example":"https://example.com/icons/date.png"},"schema":{"type":"string","nullable":true,"description":"JSON schema associated with the tool"},"func":{"type":"string","nullable":true,"description":"Functionality description or code associated with the tool"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the tool was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the tool was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Tool"}}}}},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/tools/get-tool-by-id.api.mdx">
---
id: get-tool-by-id
title: "Get a tool by ID"
description: "Retrieve a specific tool by ID"
sidebar_label: "Get a tool by ID"
hide_title: true
hide_table_of_contents: true
api: eJy1Vm1v2zYQ/ivE7cM2QLZsx0k6fZq3LIWLog0SBwUaBAVNniK2Esnwxakh6L8PJymu37IMA/ZJInm85+Hdw+PVINELp2xQRkMG1xicwhUyzrxFoXIlWDCmZMs1m19AAsai42Q8l5DBA4aFMeUf67mEBAJ/8JDdAW3wcJ+ARxGdCmvI7mpYInfoZjEUkN3dN/cJWO54hQGdbw0UEbA8FJCA5hVCBorcOnyMyqGELLiICXhRYMUhqyGsLVn54JR+gCbZOwxRI9aE5dBboz162jcZjeiza30ThUDv81iyzSEhAWF0QB3InltbKtEupF89baoPyZjlVxQBErCO3ATVQSp5SDiB3LiKB8ggxvaou4xutXqMyJREHVSu0LHcOBYKbFMCCeB3XtmSPIpcnp6McTR4M8nFYDzG3wZLMZ0MTidnPD8/P5vmZ1MKUBfXQyK7wB94hczkx6EkD/glqAq/tEv7UX/N98WP0YsQs05z0aNsj0yQjGvJCPZHdjyBC1Ma9zrsn2TGuPdGKB5QsicViuPoP11enp6fnJBzJYy+ceKYex3Lki9pQ6fKPTGZ6ASy2+v3Ozn72TNyuQNXhGB9lqb9zFCYKiUjn9Kxh7ZT9kuif43Iu5uPH1i3+eXTNwnkUf+HY15GLeiXlyqsmdxOrWPCSPxHTOGQFi54OKrJzeWgOAwo9Ydi2hHGU4F6A8CeuGc9wk68J6PJdDB6M5hMF+NpNj7NJpPPxCZa+T+zKbkPrIf5F5SaBIIKrcGijVhDU9NjtWuuV7xUks0vmHVmpSRKaG2nh7ZtVdQmsNxE3ZqdHncZ0GleMo9uhY6hc8a1pCoMhemrP9U5qtgZpG3VT2slG6DKT5u6uh5duSVz64wc+hClMsNQINf+Cd2Qq5Rbla7GB0G9ckbGVmQ9E2iSbZ9ZmpZG8LIwPmTT0Wj0kqP3ZMUkrrA0tkIdNv62Xqobuildxd5+rzZyIETobyONOyNI+p/LZ5G8+7RoY6V0btrtW4n0bHY1P6A3u5q3paLimj8o/cBE9MFUrHtME6Bwdpbj4Wg46l8mLtqXqX8uZ20wZ3N2E601jrKzG/ztgJMCK65o2Xfmv+8sE31rfKi43oJ4i4Hx3ZZgr/xvHszXm4k+qAG/h9SWXOn2HhLjupfVczORQKYktRSUZpqu6yX3eOvKpqHpx4iO2oz7BFbcqa5i3dUglad/CVnOS79fv7bJ/nLdtxm/sue+4QWC/STXa0oLLyONIIFvuO5alua+SaBALtG1JLqFmRBow9aWg3aChLi5XG//WkACfFd+e3JrvR+lU9edxcJ8Q900G3aBxkSwaf4GnHtjOA==
sidebar_class_name: "get api-method"
info_path: docs/api/tools/tools-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get a tool by ID"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/tools/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a specific tool by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Tool ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the tool","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the tool","example":"date_time_tool"},"description":{"type":"string","description":"Description of the tool","example":"A tool used for date and time operations"},"color":{"type":"string","description":"Color associated with the tool","example":"#FF5733"},"iconSrc":{"type":"string","nullable":true,"description":"Source URL for the tool's icon","example":"https://example.com/icons/date.png"},"schema":{"type":"string","nullable":true,"description":"JSON schema associated with the tool"},"func":{"type":"string","nullable":true,"description":"Functionality description or code associated with the tool"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the tool was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the tool was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Tool"}}}},"400":{"description":"Invalid ID provided"},"404":{"description":"Tool not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/tools/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/tools/tools-api",
    },
    {
      type: "category",
      label: "tools",
      items: [
        {
          type: "doc",
          id: "api/tools/create-tool",
          label: "Create a new tool",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/tools/get-all-tools",
          label: "List all tools",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/tools/get-tool-by-id",
          label: "Get a tool by ID",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/tools/update-tool",
          label: "Update a tool by ID",
          className: "api-method put",
        },
        {
          type: "doc",
          id: "api/tools/delete-tool",
          label: "Delete a tool by ID",
          className: "api-method delete",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/tools/tools-api.info.mdx">
---
id: tools-api
title: "Tools API"
description: "API for managing custom tools"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Tools API

This section contains the API endpoints for the tools service.

## Overview

The Tools API provides endpoints for managing tools resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/tools/update-tool.api.mdx">
---
id: update-tool
title: "Update a tool by ID"
description: "Update a specific tool by ID"
sidebar_label: "Update a tool by ID"
hide_title: true
hide_table_of_contents: true
api: eJztV9tu3DYQ/RVi+tAW0F69TlI91Y1rYIMgMXxBgRpGwCVHXiYSSZPUOgtB/14MKa/3GhsFCvTBTxLJIefCM4czDUj0wikblNGQw7WVPCDjzFsUqlCCBWNKNluy6SlkYCw6TqJTCTnUUfjKmBIyCPzOQ34DJO/hNgOPonYqLCG/aWCG3KE7qcMc8pvb9jYDyx2vMKDzUUCRdsvDHDLQvELIQUnIwOF9rRxKyIOrMQMv5lhxyBsIS0tSPjil76DNtjwhs8ho0kWHoA9/GLmkncLogDrQL7e2VCK6NPjqaWOzq8LMvqIIkIF1FICg0NOqkrtmZFAYV/FA0amjA1vx1eq+RqYk6qAKhY4VxrEwxxhnyAC/88qWdKIo5PHRCIe9d+NC9EYj/K03E5Nx73j8hhdv376ZFG8m5HaK1q4hm4o/8QqZKfaromv8ElSFX+LSdiyfO/v0aXRQxUkCUu1RRpcTzLRkpJatcOVJuTClcc+rfU9ijHtvhOIBJXtQYb5f+09nZ8dvj47ocCWMvnRi3/G6Lks+ow0Ja5vqLk3tBLLri48bd/azZ3Tkhrp5CNbng0E30xemGpCQH5DbfZvwegjKzxny4fLzJ5Y2H/a+zaCo9b9w86zWgn55qcKSyfWrdUwYiT/UKRzSwikPezG5Sg6KQ4+ufhdMG8B4mKNeKWAP3LNOw0a8x8PxpDd81xtPrkaTfHScj8d/kzWJoP5La0ruA+vUvMCkNoOgQhSItNnSzCbFxQlvjfaJZcbDIX32kFunlvlaCPS+qMtyCdkru72y2yu7vbLb/4Pd2gwm+whsqhe8VJJNTynwXX3GZkYumXVmoSRKiHsnB8hPm8AKU+sodrxfRUCneck8ugU6hs4ZF42sMMwNFbC2jrxHVWcOg1i5DholW6DqlTal2rR25RrsrTOy70MtlemHOXLtH9D1uRpwqwaL0U6Qz52RdQRdZwm02fqZ+WBQGsHLufEhnwyHw0MHfSQpJnGBpbEV6rA6b63avqTMSQy+XnOv4EEaoctOGichyLqfs0fQfPjrKsaKno2Lp/L5z0cIpAfiZTS+qui3mHjbwRez6IpEV9S3xnwv4acnelrLl0goT+ONZN+P+K0cPJipShcm3sFadnh2cj7dDcH5NLpecc3vlL5jovbBVCx1VRkQJpPkqD/sD7vnnov43HdRPomIPJmyy9pa4wjimwheRy2ldcUVLfsk/vvGMmHAGh8qrtdUrNrEje5w61ldFSLPdZUdNAN+DwNbcqUju5HJTZecj21lBrmS1FxSstB008y4x2tXti1N39foqOG8zWDBnUrvwE0DUnn6l5AXvPTbr8K6qb9cdNXYr+yxgzxgYDfJNRVdC17WNIIMvuEyNa/tbZvBHLlEF41IC++Tqt4VbX/auFOqEUmkHSdCoA0/lL1dI7Xz6ytK567XrYykLY4/UDPNH5KNxqZEyps010DJ9V3N70g2HUkXzze5Y4srolN7o9A0SeLKfEPdtqugBBpTXNr2HwvYkCY=
sidebar_class_name: "put api-method"
info_path: docs/api/tools/tools-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update a tool by ID"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/tools/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Update a specific tool by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Tool ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the tool","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the tool","example":"date_time_tool"},"description":{"type":"string","description":"Description of the tool","example":"A tool used for date and time operations"},"color":{"type":"string","description":"Color associated with the tool","example":"#FF5733"},"iconSrc":{"type":"string","nullable":true,"description":"Source URL for the tool's icon","example":"https://example.com/icons/date.png"},"schema":{"type":"string","nullable":true,"description":"JSON schema associated with the tool"},"func":{"type":"string","nullable":true,"description":"Functionality description or code associated with the tool"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the tool was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the tool was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Tool"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Tool updated successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the tool","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the tool","example":"date_time_tool"},"description":{"type":"string","description":"Description of the tool","example":"A tool used for date and time operations"},"color":{"type":"string","description":"Color associated with the tool","example":"#FF5733"},"iconSrc":{"type":"string","nullable":true,"description":"Source URL for the tool's icon","example":"https://example.com/icons/date.png"},"schema":{"type":"string","nullable":true,"description":"JSON schema associated with the tool"},"func":{"type":"string","nullable":true,"description":"Functionality description or code associated with the tool"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the tool was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the tool was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Tool"}}}},"400":{"description":"Invalid ID or request body provided"},"404":{"description":"Tool not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/upsert-history/get-all-upsert-history.api.mdx">
---
id: get-all-upsert-history
title: "Get all upsert history records"
description: "Retrieve all upsert history records with optional filters"
sidebar_label: "Get all upsert history records"
hide_title: true
hide_table_of_contents: true
api: eJy9lllv4zYQgP8KMU+7gBwf61x6qpHsbr0o2iAHCjT2Ay2OIm4kUiFHyRqC/nsxlOLYsZNs2qJ5cSiSMx/nrkGhT5wuSVsDMZwjOY33KGSei6r06Ehk2pN1S+EwsU558aApEzbckLlIdU7oPERgS3SSv04VxHCDNMnzqyDi11YCREDyxkN8Da3oXica5hF4TCqnaQnxdQ0LlA7dpKIM4ut5M4+glE4WGDTxAc2wpaQMIjCyQIhBK4jA4V2lHSqIyVUYgU8yLCTENdCy5FOenDY30ETPHn6SSUpz+yCmp4Js96rVkxdLaKJO612F4SmdWusUug3Nqcz9K6ojQFMVbITJxQlEcPr54oQNoDCVVU4Qh+9bgBfWkQjKhE0FZSgc+ionLz5In6BR2twI6wTfalcfX2T2JB2dSsL3cafWFZIBlSTskS5wG/PLpuFSZwtBmfYi6BR8U3zQJskrr+/xZUQ06n8BrAzpvCVEo3bwzZnAl9Z49KxzNBjwzzPnVEmC3qdVLlZZABEk1hAa4vOyLHOdhI3+d8+X6u2HSOck20ATFn7tu118x4QggtKxeNItilavGqGqQkpskl4ZfVeh0AoN6VSjE6l1IZx2ZjtH6w9ZlDmrSFK1/2mIg97RKE16wyEe9xbJeNTbHx3I9PDwYJwejNngSZdLu/k2eaanj+H8eEtI722iJaFqS8023AbVYTLGo8Wh7B0ujmVvvBir3nH6CXsjNTpKh7hAORgxVZsubxOdh3OPVJ3ilVcjwQSohPRCim8Xf/wunvJ6xVTPOMeo8jOIZ+Db6JhBNOPIlDOI6xnc4jLs3su8whk0DUOyAU4l7YzuZ3HMpmJpr9nr/djGKmTq63k0A1Q33SLAcXL8ZNo9p+VcFtIowbviIUOzE1M8SC9KdCwRN4NvNBiNe4Oj3mh8ORzHw/14NPoLmiYC0hQObLSa8y5loeG/CPZ3Ze3UEDruYR7dPTqBzlkXZBZIme26GKcdN5oY+ptdq19r1QB3Lr7d9qXK5RBDRlT6uN8vnVV7niql7R5lKI1/QLcndV+Wun8/3LLSmbOqSoIhWqGhPD7JjPv93CYyz6yneDwYDF4S9BufEgrvMbdlgYZW8tY67QWXn7aSrPfblX9ZI3TlltftIYi6f748ev3bn5fBaNqkNlzfcIjoPCImZ9MtzsnZNNSfQhp5ww1s98gBEbCF2zvDvcHeoCuuMgkJ3fWMSbDvZCouqrK0jj236Y91H3BwFVLztm+P/7KxzS8qradCmjUVX5FeGY2ev69+6gD/arLqHEL4g/plLrXhbAxPq7vY3JqoIoi14rGCY4X363ohPV65vGn4c9tuOWSV9nKRr/XXF5/wT0aQF+Bvcbk2O4USCDGEcP95nvfOGq+yrM9E/w3PW6PFqzhP888TzJwXTjPNOz334bwboj6KNwfdF7AeRxSzXGd6xNUKmnkTQYaSXcp07cYkSbCktStbgxA/a1Vvv36+hAjkZiF6VniC9J04dd2euLS3aJpmRUe8ZsCm+RssVHQR
sidebar_class_name: "get api-method"
info_path: docs/api/upsert-history/upsert-history-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Get all upsert history records"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/upsert-history/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve all upsert history records with optional filters

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID to filter records by"},{"in":"query","name":"order","required":false,"schema":{"type":"string","enum":["ASC","DESC"],"default":"ASC"},"description":"Sort order of the results (ascending or descending)"},{"in":"query","name":"startDate","required":false,"schema":{"type":"string","format":"date-time"},"description":"Filter records from this start date (inclusive)"},{"in":"query","name":"endDate","required":false,"schema":{"type":"string","format":"date-time"},"description":"Filter records until this end date (inclusive)"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the upsert history record","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"chatflowid":{"type":"string","description":"ID of the chatflow associated with the upsert history","example":"7c4e8b7a-7b9a-4b4d-9f3e-2d28f1ebea02"},"result":{"type":"string","description":"Result of the upsert operation, stored as a JSON string","example":"{\"status\":\"success\",\"data\":{\"key\":\"value\"}}"},"flowData":{"type":"string","description":"Flow data associated with the upsert operation, stored as a JSON string","example":"{\"nodes\":[],\"edges\":[]}"},"date":{"type":"string","format":"date-time","description":"Date and time when the upsert operation was performed","example":"2024-08-24T14:15:22Z"}},"title":"UpsertHistoryResponse"}}}}},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/upsert-history/patch-delete-upsert-history.api.mdx">
---
id: patch-delete-upsert-history
title: "Delete upsert history records"
description: "Soft delete upsert history records by IDs"
sidebar_label: "Delete upsert history records"
hide_title: true
hide_table_of_contents: true
api: eJyVVE1v2zgQ/SvEnLYAbTlf7q5O6012US96CJoUezB8GIsjiy1FsiTl1BD03xcjKa5jJwXqi0lx3ny9N9OColgE7ZN2FnJ4cGUSigwlEo2PFJKodEwu7EWgwgUVxWYvlncRJDhPARm3VJCDx1RUdz3ycw/8MOBAQsJthHwFg8PJ6BDWEiIVTdBpD/mqhQ1hoLBoUgX5at2tJXgMWFOiEHsDbYc4FUiwWBPkoBVICPSt0YEU5Ck0JCEWFdUIeQtp79kqpqDtFjp5Uu1thak07kks74DjsSOK6S+n9owunE1kEx/Re6OLvtrsS2Rwex7Gbb5QkUCCD9ybpCnyq1bxyAhDQG6KTlTH8xwllC7UmCCHptHqPOePOibhytfZYWZEciOD0PU/rit6Z+OQzuVsxn8nvDdFQTGWjTH7Ea2eGeccrl8DLe0OjWa7vm1iw33rJNy8bpwoWDQiUthREBSCC8DZ1ZQqd5AQt48pziF7qZes1aoD1gzjB0U0wUAOVUo+5lnmg1PTmBql3TRVhDY+UZiiztDrbHcBp628D041BV/GpKCTxz7zLDOuQFO5mPLr2Wz2lqOPbCUU7cg4X5NNB39HGn9guQwUHCv9IACOCKN4+T4YgRwP/zzL4t//Hvu2sQw//RDs39+x9oYOglvBVYm/35Tz68nN+4v3k+ub+eVkc1UWk8vij/lVOZ9jiXNYdxK0LV2fiE7sAIb5FeMAi8X98qzixf1SlC6IGi1utd2+sSxAAnM1YC6ms+kMZD9WWPRjNU7xomdqsRQPjfcu8Ai9ZPaYTZBANWp+joP5ny+euTfexVSjPQpx97Oddlre0ej/0kYcmUz0PWXeoLY8DX0l7SjqsyUoIdeKVyGLjN/bdoORPgfTdfz5W0OB1+Nawg6Dxg0ztGpB6chnBXmJJtJPCvjt07ge34njffdGts9bynJqOzQN30DCV9oP67ZjxVSEikKfyPBwO4SbPDL8B/BsbfJAHOb9fvF4+4EFPu7b2ikGBXzipY5PQ2TX19TPTf+tBYN22+CWbQenTDm+nKaT6elTfbW2th0sHt1Xsl13KDXxnavtuv8BvGF1cg==
sidebar_class_name: "patch api-method"
info_path: docs/api/upsert-history/upsert-history-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete upsert history records"}
>
</Heading>

<MethodEndpoint
  method={"patch"}
  path={"/upsert-history/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Soft delete upsert history records by IDs

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"ids":{"type":"array","items":{"type":"string","format":"uuid"},"description":"List of upsert history record IDs to delete"}}}}}}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successfully deleted records"},"400":{"description":"Invalid request body"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/upsert-history/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/upsert-history/upsert-history-api",
    },
    {
      type: "category",
      label: "upsert-history",
      items: [
        {
          type: "doc",
          id: "api/upsert-history/get-all-upsert-history",
          label: "Get all upsert history records",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/upsert-history/patch-delete-upsert-history",
          label: "Delete upsert history records",
          className: "api-method patch",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/upsert-history/upsert-history-api.info.mdx">
---
id: upsert-history-api
title: "Upsert History API"
description: "API for managing upsert history records"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Upsert-history API

This section contains the API endpoints for the upsert-history service.

## Overview

The Upsert-history API provides endpoints for managing upsert-history resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/variables/create-variable.api.mdx">
---
id: create-variable
title: "Create a new variable"
description: "Create a new environment variable"
sidebar_label: "Create a new variable"
hide_title: true
hide_table_of_contents: true
api: eJztVk2P2zYQ/SvEnBpAsmXVu0l1qpukgNuiNbpOgnZhFLQ4splIpJak7BiC/nsxlOyVbG+7vfS0l12ZfPNB8r2ZqUGgTY0sndQKEnhrkDtknCncM1Q7abQqUDm240bydY4QgC7RcMLPBSSQeouPj9uObywk93C0sLAKwGJaGekOkNzXsEZu0Mwqt4XkftWsAjD4UKF1P2hxgKSGVCuHytEnL8tcpj7c+LOlHGuw6RYLTl/uUCIkoNefMXUQQGkoOSfR0q4UPYx1RqoNBJBpU3AHCVSVFBCcXcAHJR8qZFKgcjKTaFimDXNb7N8AfuVFmZPXNBM3304wCt/EWRpOJvhduE6ncXgT3/Ls9evbaXY7hSYAxQu8lsww+K+8QKazp8PNFvO/fn7/B3nc8bx6hsuPBLviU1V57j8TZyrsxygOocXUoAu/4IEitQH+LdDyUF7EYd/gaDMKWGsSMFUVazSvBkfq3DVBxyTxjrur4U7vJrjD0MkCL3J457mrBKNdtt+iGqaz55Z1UQY5xFE8DaM3YTxdTqbJ5CaJ4z8po6oU/0NGObeOdaGekRa9iHQecFJdQ6skImlQtE/qF2yplW3FEEcR/TtnR5dHdyvMVmmK1mZVnh8geBHiixBfhPgfhdgEML2mtbna8VwK1vU6tqZmR+A4vibMXAovNobGaEPAm+teHRrFc2bR7NAc0U0ABbqtpg5dauslyanfwvixL1NbJiPru3Jlckhg61xpk/G4NFqMrKuE1CO3Ra7sHs2IyzEv5Xg3ubjfhdGiSn3CrVNogr7PZDzOdcrzrbYumUZR9JSjXwjFBO4w16UfPY7+emPEHRWetrb0h4kTMygiHc/DIOlAEHQfPx758tOnpb8rKmi/P84g748v35au5xWXY2051YZTaThTcnDB34HgrrPtTANPKkWqTPuLOGOmZbPF/OKyZ4u5L6sFV3wj1ebqyEdEIZa0JpNRNIq61sBT3xqO5/Ycmc3ZXVWW2hDnhpzq84j0VXBJ27aFfz/Yplch4hZc9UIMxtNeER2cqjc/Pmue7V7D4Vc3LnMula84lHndieZsmCUO02Jdr7nFDyZvGlp+qNDQgLsKTnA/3wawRS7QeJURARJ42yYYUq3u8eSix5KGWotZmmLp/hG76ml+8dvdkujeDdSFFmRj+B4C/zcBP8iTtVeRX6sh52pT8Q1hW5/0DHyorTMt+VN1W1wdehnWdYtY6i+omgaC7iiOfkOzaprmb7EnMNQ=
sidebar_class_name: "post api-method"
info_path: docs/api/variables/variables-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Create a new variable"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/variables"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Create a new environment variable

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the variable","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the variable","example":"API_KEY"},"value":{"type":"string","description":"Value of the variable","nullable":true,"example":"my-secret-key"},"type":{"type":"string","description":"Type of the variable (e.g., string, number)","example":"string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the variable was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the variable was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Variable"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Variable created successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the variable","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the variable","example":"API_KEY"},"value":{"type":"string","description":"Value of the variable","nullable":true,"example":"my-secret-key"},"type":{"type":"string","description":"Type of the variable (e.g., string, number)","example":"string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the variable was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the variable was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Variable"}}}},"400":{"description":"Invalid request body"},"422":{"description":"Validation error"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/variables/delete-variable.api.mdx">
---
id: delete-variable
title: "Delete a variable by ID"
description: "Delete a specific environment variable by ID"
sidebar_label: "Delete a variable by ID"
hide_title: true
hide_table_of_contents: true
api: eJyVVD1v2zAQ/SvETS1AWEqRLpxqwCngIkPQpO0QeKDFs02EIpkj5dYQ9N+LkxRXdpwAnSSS793nu2vBYKrIxmyDBwULdJhRaJEiVnZjK4F+byn4Gn0We01Wrx2K9UEsFyAhRCTN1KUBBaYn/xxBICHrbQL1CC+8BCsJCauGbD6AemxhjZqQ5k3egXpcdSsJUZOuMSOlHmA5qqjzDiR4XSMosAYkED43ltCAytSghFTtsNagWsiHyKiUyfotdPIsw5fwOAH2R5hi8AkTcz+VJX/eYAz5GZGaqsKUNo1zB3ZwfYm19HvtrBHLhYgU9tagGbDX73jwIYtNaHwP/XzZbEby2omEtEcSSBQIuk5CjXkX/rUB5FA2BcWx/EVrTQfcAuYOBW7IgYJdzjGpoogUzCzlxtgwyzvUPv1Gmmlb6GiL/RWcV/OOgmkqPowBQSenNlVRuFBptwspq+uyLN8ydMsoYXCPLsRebC/2JpK55yYPnZoK59hz9gijFvg8gECOP18D1TqDgm+/HvqSWb8JPd1mh5M+JDG/W74KcX63FJtAotZeb63fXhyNBBK4tgPlalbOSpBQBZ91ldnXKOJ5X9n5Utw3MQbKIM86Ma0+SMBaW35OA/zLyTPnEkPKtfYTF8dZfjW2J3m1fXTo8/+P/1j2jH9yEZ22nmXbp9GO4pvOvgRlDW8AFgM/te1aJ/xBruv4+rlB4q2wkkdSr1BjE/8bUBvtEr4T/ofv41b4KKZj/kag46X2B+6Zdg2fQMITHoYt062mY7W4ub15uAEJ+lR0ZyLrQ75oum0HxEN4Qt91R0+Zz+ys6/4CvD7mmg==
sidebar_class_name: "delete api-method"
info_path: docs/api/variables/variables-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Delete a variable by ID"}
>
</Heading>

<MethodEndpoint
  method={"delete"}
  path={"/variables/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Delete a specific environment variable by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Variable ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Variable deleted successfully"},"400":{"description":"Invalid ID provided"},"404":{"description":"Variable not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/variables/get-all-variables.api.mdx">
---
id: get-all-variables
title: "List all variables"
description: "Retrieve a list of all environment variables"
sidebar_label: "List all variables"
hide_title: true
hide_table_of_contents: true
api: eJy9VVGP2zYM/isCnzZATpwsd239tADrhmzFcNilLbZDUDA2HauVJVWScw0M//eBji91Lndo97K8RJbIj59I6mMLBYXcKxeVNZDBXxS9oj0JFFqFKGwpUGtBZq+8NTWZKPboFW41BZBgHXlk11UBGewoLrV+NzqPuAuQ3cFXn42EQHnjVTxAdtfCltCTXzaxguxu020keArOmkABshbmacp/5yRvmzynEMpGixMBkJBbE8lEtkfntMr7g+nHwE4thLyiGnkVD44gA/QeDyBBRarDaN9uP1IeQYLzDB/VkYoqRjYhemV2IKG0vsYIGTSNKkA+YvrWqM8NCVWQiapU5EVpvYgVnbIIEugL1k4zal4WVz/NKE1ezss8mc3oVbLNF/Pkan6N5YsX14vyegGdBIM1PUXmPPifWBMX8Nlwy5vVhz9e/82Ie9TNd0C+Y7MnME2jdb/Mom9oHKM+JIFyTzH5RAeOdAzwrUDrg7uII36gyW4ixdFFCtPUW/I/nl1pgOsk5J4wUvELxifDnepWYKQkqpouOLCrQFMIPhX3FZlzOvcYxBDljMM8nS+S9GUyX6xni2x2lc3n/zCjxhX/AyONIYoh1HfQ4oqo2Bs8PFzo+Cfh6qm3tzKRvEEtAvk9eUHeW9/D1BQrO8gAPx7kJw3TsVwcfUL/8BuvIYMqRhey6dR5W0xCbAplJ7EiNOGe/ATVFJ2a7mcXmbjxtmhy/hiIQCfHmNl0qm2OurIhZos0TZ8DesNWoqA9aet6fXvAGynVLUvHUQXGenWqIUfk6/VmkA1GIIfFrw+V/f39uk+VMqXt3R9lPojlzeqC4vJm1ctGjQZ3yuyeVWPO7dFlNkkn6SCJmPeSeNQMWPaZXa7EbeOc9Vyp80qMs8/9U6Pi43A0//nsmO/ibIg1mlGINzw4eGqMuZ1dqf2q1f914gwZj/QlTp1GZfqXxTdoh5Z7NG24A3izbbcY6K3XXcfbnxvyPIE28mTeDyAJFWFBvu9RlqwMlnlOjhM1iOTldOFeObX/b6/XIAHPO+RRR/ToD2PIHEbYbXu0WNtPZLoO5EAi8jd0m67r/gUNt59M
sidebar_class_name: "get api-method"
info_path: docs/api/variables/variables-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"List all variables"}
>
</Heading>

<MethodEndpoint
  method={"get"}
  path={"/variables"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Retrieve a list of all environment variables

<ParamsDetails
  parameters={undefined}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={undefined}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Successful operation","content":{"application/json":{"schema":{"type":"array","items":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the variable","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the variable","example":"API_KEY"},"value":{"type":"string","description":"Value of the variable","nullable":true,"example":"my-secret-key"},"type":{"type":"string","description":"Type of the variable (e.g., string, number)","example":"string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the variable was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the variable was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Variable"}}}}},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/variables/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/variables/variables-api",
    },
    {
      type: "category",
      label: "variables",
      items: [
        {
          type: "doc",
          id: "api/variables/create-variable",
          label: "Create a new variable",
          className: "api-method post",
        },
        {
          type: "doc",
          id: "api/variables/get-all-variables",
          label: "List all variables",
          className: "api-method get",
        },
        {
          type: "doc",
          id: "api/variables/update-variable",
          label: "Update a variable by ID",
          className: "api-method put",
        },
        {
          type: "doc",
          id: "api/variables/delete-variable",
          label: "Delete a variable by ID",
          className: "api-method delete",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/variables/update-variable.api.mdx">
---
id: update-variable
title: "Update a variable by ID"
description: "Update a specific environment variable by ID"
sidebar_label: "Update a variable by ID"
hide_title: true
hide_table_of_contents: true
api: eJztV1tv2zYU/ivEeWoB+jon7fQ0b+kAb8NgtEmHNjAGWjyy2UokQ1JODUH/vTiU7MiXZOnD3vJkijx3nu/wcwUSfeqUDcpoSODGShGQCeYtpipTKUO9Uc7oAnVgG+GUWObIlls2uwIOxqITpDqTkEAZlT+2QsAhiJWH5BZ2eh4WHDympVNhC8ltBUsUDt20DGtIbhf1goMVThQY0PkooCgqK8IaOGhRICSgJHBweFcqhxKS4Erk4NM1FgKSCsLWkpQPTukV1Pwow114lAD5I0Pow69Gbkk7NTqgDrQU1uYqjekNvnhSrk7dmOUXTANwsI6KERR6OlXyNBQOmXGFCFSpMiZxVHut7kpkSqIOKlPoWGYcC2vc1x044DdR2Jysppm8+GmEw97bcZb2RiP8ubdMJ+PexfhSZG/eXE6yywml31TtNJhD53+LApnJHnc3nc/+/fPdJ7K4EXn5DJMfSeyMTV3meVy2V/fgo9j2PKYOQ+8rbslT4+C/HF1v7Ykf9gr7qz5njQpnuiyW6F4fpPTQIqlDEVBeiXDW3f7eqL97QRV4EsNVhI2WjE7Z/Rr1YTj3wrPWy0EM4+F40hu+7Y0n16NJMrpIxuPPFFGDpf87olz4wFpXzwiLbkSFKLBHeU27h2iMG94a7RswjIdD+nkEh6175ss0Re+zMs+3wF+A+ALEFyD+IBBrDpNzWJvpjciVZLMrZhxrXzy2NHLLrDMbJVFC1J08gVNtAstMqaPoxXk3AZ0WOfPoNugYOmdcDLbAsDZEEWwZIUrveQKDPS8YVErWQNyAFJuXv3Q5JLAOwfpkMLDOyL4PpVSmH9YotL9H1xdqIKwabEYnRZ87I8uUPtpooOZdm8lgkJtU5GvjQzIZDoePGfqLpJjEDebGRha0s9fhMh9oGjUDp8to9u1CHqElKfTdCAFvF7/vmuiPf65jvWjKvX8gJu927dDMs+dNnD1f2g2M/bw4gjc/aeoDFJ5vwSNgPAofpTMTC3HUrp5N57OTYk/nszhrC6HFSunVWfbpKRN0vlEZ9Yf9YfteiDS+F7u8Y49MZ+xDaa1x1HiHPdXtIwJdIRQd+0b8l4NjuhVrfCiE7rjY0+UTZnyQV4dW/ijDbq8m4LcwsLlQOs4kSqNqYdSl1xwSJYlkU1vTUVUthccbl9c1bd+V6Ih4L/heKWJNKk9rCUkmco9PhP/qffvUv2ZdJv1IoO2m0NtO9wEHartI5OtFzWGNQqKLgTQHvzXuevSedBRPeABButGYpina8KTsojOG5jfXBL6W8xdGkooT9/THQtw3MZqYfcR03KsgF3pVihXJNiapKcQh0o+QHZM6W4WqaiSuzVfUdb0vSqBvqktdfwdvno+N
sidebar_class_name: "put api-method"
info_path: docs/api/variables/variables-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Update a variable by ID"}
>
</Heading>

<MethodEndpoint
  method={"put"}
  path={"/variables/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Update a specific environment variable by ID

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Variable ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the variable","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the variable","example":"API_KEY"},"value":{"type":"string","description":"Value of the variable","nullable":true,"example":"my-secret-key"},"type":{"type":"string","description":"Type of the variable (e.g., string, number)","example":"string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the variable was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the variable was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Variable"}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Variable updated successfully","content":{"application/json":{"schema":{"type":"object","properties":{"id":{"type":"string","format":"uuid","description":"Unique identifier for the variable","example":"cfd531e0-82fc-11e9-bc42-526af7764f64"},"name":{"type":"string","description":"Name of the variable","example":"API_KEY"},"value":{"type":"string","description":"Value of the variable","nullable":true,"example":"my-secret-key"},"type":{"type":"string","description":"Type of the variable (e.g., string, number)","example":"string"},"createdDate":{"type":"string","format":"date-time","description":"Date and time when the variable was created","example":"2024-08-24T14:15:22Z"},"updatedDate":{"type":"string","format":"date-time","description":"Date and time when the variable was last updated","example":"2024-08-24T14:15:22Z"}},"title":"Variable"}}}},"400":{"description":"Invalid ID or request body provided"},"404":{"description":"Variable not found"},"500":{"description":"Internal server error"}}}
>
  
</StatusCodes>
</file>

<file path="api/variables/variables-api.info.mdx">
---
id: variables-api
title: "Variables API"
description: "API for managing environment variables"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Variables API

This section contains the API endpoints for the variables service.

## Overview

The Variables API provides endpoints for managing variables resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/vector-upsert/sidebar.ts">
import type { SidebarsConfig } from "@docusaurus/plugin-content-docs";

const sidebar: SidebarsConfig = {
  apisidebar: [
    {
      type: "doc",
      id: "api/vector-upsert/vector-upsert-api",
    },
    {
      type: "category",
      label: "vector",
      items: [
        {
          type: "doc",
          id: "api/vector-upsert/vector-upsert",
          label: "Upsert vector embeddings",
          className: "api-method post",
        },
      ],
    },
  ],
};

export default sidebar.apisidebar;
</file>

<file path="api/vector-upsert/vector-upsert-api.info.mdx">
---
id: vector-upsert-api
title: "Vector Upsert API"
description: "API for upserting vector embeddings of documents in a chatflow"
sidebar_label: Introduction
sidebar_position: 0
hide_title: true
custom_edit_url: null
---

# Vector-upsert API

This section contains the API endpoints for the vector-upsert service.

## Overview

The Vector-upsert API provides endpoints for managing vector-upsert resources.

## Authentication

All API requests require authentication using an API key or OAuth token.

## Rate Limiting

API calls are subject to rate limiting to ensure fair usage and system stability.

## Endpoints

Explore the available endpoints in the sidebar to learn more about specific operations.
</file>

<file path="api/vector-upsert/vector-upsert.api.mdx">
---
id: vector-upsert
title: "Upsert vector embeddings"
description: "Upsert vector embeddings of documents in a chatflow"
sidebar_label: "Upsert vector embeddings"
hide_title: true
hide_table_of_contents: true
api: eJztWNtu2zgQ/RVinlpAiZ1sdtvqad26Bdwt0mwSp1gEQUGLY4uNRHJ5cWIY+vfFkLItX7Jogz72yRI118Mz5IyX4PnMQX4Lcyy8tnCXgcMiWOkXkN8uYYLcoh0EX0J+e9fcZaANWu6lViMBeas2Ng6thwwEusJKQ58hh7TMkgzDeoJCSDVzTE+Z0EWoUXnHpGKcFSX300o/QAaGW16jR+tiBJIsGe5LyEDxGiEHKSADi/8GaVFA7m3ADFxRYs0hX4JfGJJy3ko1g2Y3qnetKzYaAiVEhtD5t1osSLvQyqPy9MiNqWQRk+19c6S83HejJ9+woNyNJWi8RBflvDbnWiChtBvSbkQjxQru0LGHEhVb6MBKPkdWh8pLU+EKQOe1Raa0QJdFqYIr5gwWcrpgvkyf2GjIvG5laTEpO8gAH3ltKoqDBL+eEDR6jtZKge+0msrZobS2Q70ukRVRNiQWkLOVkehP4JSHar3rIXHAofdx61/oaIpXL6FpmiaDlCW3vjfVtj4S3PMfwXkqq/TQinFr+QIykB5rdwh68sI95DCRitvFPj8+kEVKa4IsmEpzgYKkai2wOo8M3LeqQlXxCYGb2LjBeg/Cz75Eu8FsC00HBMmG2rdtfnfN9jo5iQvOaOUSAKf9Pv1sO7vZq720ISiYC0WBzk1DVRFgP4n3KtQDQYhtJFWoJ2j3cDiPy3QWtBRlPCp2wDtpCNl6iBX655kUreq+0bER/JlGQ6u6b/TqXhrzPKMuqbIXSvuERLaKPmOxkKLTl7teo+hQF99TBE/smeEzOgBW27/L7Q2Xr0vpmHSxzlu+UAr0SjaOY5mg56si3nXLhZCp+i+2Atg7sdcul8CDL7WFHD7qUrGhRsKSe5I/7Z+eHfVfH52eQawPL30Mc9jeLdurN52r6rItnPYMOjtUOiM155UUTCoTPDNWz2Wi50+qlBqd47ODp8l2IO+t1ZatxLv78USIDcUovttylN0ye34z+DQafh2dX4yvoYthVFiDdrYP2vpyJRZPdVC/ANsAdnp64ICmeNJNilH2F1wtXL8fLkqPVvGKObRztL8w62KWTt9SU2NutIvpUeOcQy/dMr10+feWUjRAnT5hmNrsYCvIofTeuLzXM1aLY+eDkPrYl8iVe0B7zGWPG9mbn+wFfWG1CEVkcTIKTda1mfd6lS54VWrn87N+v/+UoU8kxQTOsdKGzvC1vc5kckW7mTasO5+skSWP0E4E9J6EIGsfPqw6wI9frvcCGFyM2D0uGF07qHxLJPYgfcmm8hEFm/MqIDt7c3JZvj6tZ+OxfPebmt+EV/Lh0+eHV28e31Zf/uBHH6Ynj38Nxn+7f+JuERMvN5PG+80F150UVp35gcacjEg11THPrSuNtXPW4GJ0MJ3pug2Xavaj0xgRJJk6Oe4f99tS40UstXYeG0R6DEbsKhij4yC4TacuhYjVNZf02SXxP7c+U55E3pqrjounRsndhDvz2zPHz5ZDHh99z1RcKirdmM6yrab1sJxBWA2+uRQ0OxO/SWC5nHCHY1s1DS3/G9DSPH2XwZxbmQaF2yUI6ehZQD7llcP/yeXFZdv7v2Td+fWJcFdtoKImMPI1DSL3uEjjc3PXZFAiF2hjIOlD2wMeXZP6RvHQhEblnZQGRYHGd8T3zl8q3fW5dPH5iopu0k7bdTwT41AW7WabR/q/IYvtX6e0d0o5Bn8w2+UySVzre1RNs07e0zvl3zT/AfOG5QY=
sidebar_class_name: "post api-method"
info_path: docs/api/vector-upsert/vector-upsert-api
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Upsert vector embeddings"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/vector/upsert/{id}"}
  context={"endpoint"}
>
  
</MethodEndpoint>



Upsert vector embeddings of documents in a chatflow

<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"path","name":"id","required":true,"schema":{"type":"string"},"description":"Chatflow ID"}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"content":{"application/json":{"schema":{"type":"object","properties":{"stopNodeId":{"type":"string","description":"In cases when you have multiple vector store nodes, you can specify the node ID to store the vectors","example":"node_1"},"overrideConfig":{"type":"object","description":"The configuration to override the default vector upsert settings (optional)"}}}},"multipart/form-data":{"schema":{"type":"object","properties":{"files":{"type":"array","items":{"type":"string","format":"binary"},"description":"Files to be uploaded"},"modelName":{"type":"string","nullable":true,"example":"","description":"Other override configurations"}},"required":["files"]}}},"required":true}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"Vector embeddings upserted successfully","content":{"application/json":{"schema":{"type":"object","properties":{"numAdded":{"type":"number","description":"Number of vectors added","example":1},"numDeleted":{"type":"number","description":"Number of vectors deleted","example":1},"numUpdated":{"type":"number","description":"Number of vectors updated","example":1},"numSkipped":{"type":"number","description":"Number of vectors skipped (not added, deleted, or updated)","example":1},"addedDocs":{"type":"array","items":{"type":"object","properties":{"pageContent":{"type":"string","example":"This is the content of the page."},"metadata":{"type":"object","additionalProperties":{"type":"string"},"example":{"author":"John Doe","date":"2024-08-24"}}},"title":"Document"}}},"title":"VectorUpsertResponse"}}}},"400":{"description":"Invalid input provided","content":{"application/json":{"schema":{"type":"object","properties":{"message":{"type":"string","description":"Error message","example":"Invalid input provided"},"code":{"type":"string","description":"Error code","example":"INVALID_INPUT"}},"title":"Error"}}}},"404":{"description":"Chatflow not found","content":{"application/json":{"schema":{"type":"object","properties":{"message":{"type":"string","description":"Error message","example":"Invalid input provided"},"code":{"type":"string","description":"Error code","example":"INVALID_INPUT"}},"title":"Error"}}}},"422":{"description":"Validation error","content":{"application/json":{"schema":{"type":"object","properties":{"message":{"type":"string","description":"Error message","example":"Invalid input provided"},"code":{"type":"string","description":"Error code","example":"INVALID_INPUT"}},"title":"Error"}}}},"500":{"description":"Internal server error","content":{"application/json":{"schema":{"type":"object","properties":{"message":{"type":"string","description":"Error message","example":"Invalid input provided"},"code":{"type":"string","description":"Error code","example":"INVALID_INPUT"}},"title":"Error"}}}}}}
>
  
</StatusCodes>
</file>

<file path="api/full-api-spec.mdx">
---
title: Full API Specification
description: Complete OpenAPI specification for AnswerAgentAI
---

# AnswerAgentAI API Specification

This page provides the complete OpenAPI specification for all AnswerAgentAI API endpoints.

## OpenAPI Specification

The full API specification is available in the following formats:

-   [OpenAPI YAML](/api/full-api-spec.yaml)
-   [OpenAPI JSON](/api/full-api-spec.json)

These specifications follow the [OpenAPI 3.0](https://swagger.io/specification/) standard and can be imported into API tools such as Postman, Insomnia, or SwaggerUI for easier API exploration.

## API Categories

The API is organized into the following categories:

-   [Assistants API](/docs/api/assistants/assistants-api)
-   [Attachments API](/docs/api/attachments/attachments-api)
-   [Chat Message API](/docs/api/chat-message/chat-message-api)
-   [Chatflows API](/docs/api/chatflows/chatflows-api)
-   [Document Store API](/docs/api/document-store/document-store-api)
-   [Feedback API](/docs/api/feedback/feedback-api)
-   [Leads API](/docs/api/leads/leads-api)
-   [Ping API](/docs/api/ping/ping-api)
-   [Prediction API](/docs/api/prediction/prediction-api)
-   [Tools API](/docs/api/tools/tools-api)
-   [Upsert History API](/docs/api/upsert-history/upsert-history-api)
-   [Variables API](/docs/api/variables/variables-api)
-   [Vector Upsert API](/docs/api/vector-upsert/vector-upsert-api)

Each category provides specific endpoints for working with different aspects of the AnswerAgentAI platform.
</file>

<file path="api/index.mdx">
---
title: API Reference
description: Overview of the AnswerAgentAI API
---

# AnswerAgentAI API Reference

Welcome to the AnswerAgentAI API reference documentation. This section provides detailed information about the API endpoints available for integrating with AnswerAgentAI.

## Available APIs

AnswerAgentAI offers various API categories to interact with different aspects of the platform:

-   **Assistants API**: Manage AI assistants
-   **Attachments API**: Handle file attachments
-   **Chat Message API**: Work with chat messages
-   **Chatflows API**: Create and manage chat flows
-   **Document Store API**: Manage document storage and retrieval
-   **Feedback API**: Handle user feedback
-   **Leads API**: Manage lead information
-   **Ping API**: Check server status
-   **Prediction API**: Work with predictions
-   **Tools API**: Integrate with custom tools
-   **Upsert History API**: Track document updates
-   **Variables API**: Manage system variables
-   **Vector Upsert API**: Work with vector embeddings

## Authentication

Most API endpoints require authentication. Please refer to the [Authorization documentation](/docs/developers/authorization/app-level) for details on how to authenticate your API requests.

## API Specification

For a complete overview of all available endpoints, request/response formats, and parameters, see the [Full API Specification](/docs/api/full-api-spec).
</file>

<file path="browser/README.md">
---
title: Your Guide to AnswerAgent Sidekick
sidebar_label: Introduction
slug: /browser
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# AnswerAgent Sidekick Browser Extension

## Installation Guide

The AnswerAgent Sidekick is a browser extension that integrates your AnswerAgent flows directly into your web browsing experience, enhancing productivity and streamlining workflows across multiple platforms.

### Installing from Chrome Web Store (Recommended)

1. **[Install from Chrome Web Store](https://chromewebstore.google.com/detail/answeragent-sidekick/cpepciclppmfljkeiodifodfkpicfaim)** - Click the link to go directly to the Chrome Web Store

2. **Add to Chrome:** Click the "Add to Chrome" button on the Chrome Web Store page

3. **Confirm Installation:** Click "Add extension" when prompted to confirm the installation

4. **Pin for easy access:** Click the puzzle piece icon in your Chrome toolbar, then click the pin icon next to AnswerAgent Sidekick to keep it visible in your toolbar.

## What is AnswerAgent Sidekick?

AnswerAgent Sidekick is a sophisticated browser extension designed to bring the intelligence of your AnswerAgent flows directly into your daily web activities. It functions as an AI assistant that integrates with your existing tools and workflows to increase efficiency and productivity.

Key capabilities include:

-   **Web page summarization** for efficient information consumption
-   **AI-powered web search** for enhanced information retrieval
-   **Image generation with DALL-E** for creative projects
-   **Multiple AI model integration** including OpenAI, Google Gemini, Anthropic, Deepseek, and Llama models
-   **Extensive tool connectivity** with your existing enterprise systems

## Enterprise Use Cases

AnswerAgent Sidekick creates a unified workspace that provides a comprehensive view of your organization's information ecosystem through natural language prompts:

### Project Management

-   Extract key updates from Jira tickets
-   Summarize sprint progress reports
-   Draft project status updates based on ticket information

### Knowledge Management

-   Generate summaries of Confluence documentation
-   Extract relevant information across multiple knowledge bases
-   Create documentation templates based on existing content

### Customer Relationship Management

-   Retrieve customer interaction history from Salesforce
-   Generate account summaries with recent activities
-   Draft follow-up communications based on customer data

### Content Management

-   Query and update Contentful entries through natural language
-   Generate content drafts for website updates
-   Validate content structure across platforms

### Development Workflow

-   Analyze GitHub repositories and code
-   Summarize pull request changes
-   Generate code documentation

### Team Communication

-   Draft Slack messages based on information from other systems
-   Schedule meetings with context from relevant tools
-   Create update summaries for team communications

### Creative Workflows

-   Generate images with DALL-E based on project requirements
-   Create visual assets from text descriptions
-   Modify and iterate on visual concepts

### Research and Information Gathering

-   Conduct enhanced web searches with AI analysis
-   Compile information from multiple sources
-   Generate research summaries with citations

## AI Model Integration

AnswerAgent Sidekick provides access to leading AI models through a single interface:

-   **OpenAI:** Access GPT-4, GPT-3.5, and specialized models
-   **Google Gemini:** Leverage Google's advanced AI capabilities
-   **Anthropic:** Utilize Claude models for sophisticated understanding
-   **Deepseek:** Explore specialized AI capabilities
-   **Llama:** Access open-source AI models for various tasks

This integration allows you to select the optimal AI model for specific tasks while maintaining a consistent user experience.

## Business Value

AnswerAgent Sidekick transforms your browsing experience into a productivity hub by:

-   **Automating routine administrative tasks** across multiple platforms
-   **Centralizing information access** from disparate systems
-   **Providing a comprehensive 360-degree view** of work contexts with simple prompts
-   **Reducing context switching** between different tools and platforms
-   **Enhancing decision-making** with rapid information synthesis

By bringing AI capabilities directly into your browser and connecting to your essential tools, AnswerAgent Sidekick creates a seamless workflow that significantly reduces time spent on administrative tasks while improving information quality and accessibility.

## Supported Integrations

AnswerAgent Sidekick connects with a growing ecosystem of enterprise tools:

-   **CRM:** Salesforce (SFDC)
-   **Project Management & Collaboration:** Jira, Notion, Confluence
-   **Communication:** Slack
-   **Data & Content Management:** Airtable, PostgreSQL, Contentful
-   **Development:** GitHub

These integrations enable comprehensive workflows that span multiple systems through simple natural language prompts.

## Why You'll Absolutely Adore It (Value Proposition)

"Okay, sounds neat," you might be thinking, "but what's in it for _me_?" Oh, honey, let me count the ways:

-   **Boost Your Productivity:** Automate routine tasks, get information faster, and spend less time searching and more time _doing_.
-   **Unlock AI Superpowers:** Access cutting-edge AI models without needing a degree in computer science. It's like having a genius on call!
-   **Seamless Workflow:** Integrates directly into your browser and connects with tools like Salesforce, Jira, Slack, and so many more. Your workflow just got a major upgrade.
-   **Personalized Experience:** Choose the AI model that best suits your needs and customize how the Sidekick works for you.
-   **Creative Spark:** Generate images, draft content, and brainstorm ideas with AI assistance.

## Dazzling Features at Your Fingertips

Here's a little peek at what the AnswerAgent Sidekick can do for you:

-   **Instant Page Summaries:** Get the gist of any article or web page in seconds.
-   **AI-Powered Web Search:** Go beyond basic searches and find what you need with greater precision.
-   **DALL-E Image Generation:** Create unique images from text descriptions.
-   **Flexible AI Model Selection:** You're in control! Pick from leading AI providers.
-   **Rich Tool Integrations:** Connect to your CRM, project management, communication platforms, databases, and more.

## Unleash Your Inner Power User (Use Cases)

Wondering how this all translates into real-world awesomeness? Here are just a few ideas:

-   **Research Rockstar:** Quickly summarize articles, gather information from various sources, and even ask clarifying questions about the content you're browsing.
-   **Content Creation Whiz:** Draft emails, blog posts, social media updates, or product descriptions with AI assistance. Stuck for an image? Generate one on the fly!
-   **Productivity Ninja:**
    -   "Hey Sidekick, summarize the key updates on this Jira ticket for Project Phoenix."
    -   "Draft a Slack message to the team about the new marketing campaign, based on this Notion page."
    -   "Find recent customer interactions in Salesforce for Account XYZ."
-   **Data Detective:** Query your PostgreSQL database or look up information in Contentful without leaving your browser tab.
-   **Code Companion:** Get insights from GitHub repositories or ask for explanations of code snippets.

The possibilities are truly endless!

## Getting Started: Let the Magic Begin!

Ready to welcome the AnswerAgent Sidekick into your digital life? Setting it up is a breeze!

### First-Time Setup

Once installed, click on the AnswerAgent Sidekick icon in your browser toolbar. You might be guided through a quick welcome or login process to connect to your AnswerAgent account.

<!-- ![Placeholder for the initial welcome screen or login interface of the AnswerAgent Sidekick extension.](/img/screenshot-initial-setup.png) -->

_Caption: The initial setup or welcome screen for AnswerAgent Sidekick._

Follow the on-screen prompts, and you'll be ready to roll in no time!

## Choosing Your AI Brainiac (Supported AI Models)

The AnswerAgent Sidekick lets you choose your preferred AI model, because who doesn't love options? You can typically switch between models depending on the task at hand or your personal preference.

We support a range of powerful models from leading providers:

-   **OpenAI:** Access models like GPT-4, GPT-3.5, and others.
-   **Google Gemini:** Tap into Google's advanced AI capabilities.
-   **Anthropic:** Utilize models like Claude for sophisticated text generation and understanding.
-   **Deepseek:** Explore the power of Deepseek's AI.
-   **Llama:** Leverage Llama models for various tasks.

Look for a settings or configuration area within the extension to select your active AI model.

<!-- ![Placeholder for the AI model selection interface within the AnswerAgent Sidekick extension.](/img/screenshot-ai-model-selection.png) -->

_Caption: Selecting your preferred AI model within the AnswerAgent Sidekick._

## Connecting Your Universe (Tool Integrations)

This is where the AnswerAgent Sidekick truly shines, by talking to the tools you rely on every day! Imagine asking questions or getting summaries that pull information directly from your systems.

Here are some of the tool types you can connect (with more always on the way!):

-   **CRM:**
    -   Salesforce (SFDC) [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/salesforce-mcp)
-   **Project Management & Collaboration:**
    -   Jira [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/jira-mcp)
    -   Confluence [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/confluence-mcp)
-   **Communication:**
    -   Slack [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/slack-mcp)
-   **Data & Content Management:**
    -   PostgreSQL [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/postgresql-mcp)
    -   Contentful [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/contentful-mcp)
-   **Development:**
    -   GitHub [Learn More](/docs/sidekick-studio/chatflows/tools-mcp/github-mcp)

And many more! The setup for each integration might vary slightly, but generally involves authorizing the Sidekick to access your tool's data (securely, of course!).

<!-- ![Placeholder for the tool integration management page within the AnswerAgent Sidekick, showing a list of connectable tools.](/img/screenshot-tool-integrations.png) -->

_Caption: Connecting your favorite tools and services to AnswerAgent Sidekick._

## What's Next?

Go ahead, explore! Play around with the features, connect your tools, and see how the AnswerAgent Sidekick can revolutionize your browsing. We're thrilled to have you on board, and we can't wait to see what amazing things you'll do.

If you have questions or run into any hiccups, don't hesitate to reach out to our support channels (you'll usually find a link in the extension's settings).

Happy browsing, superstar!
</file>

<file path="chat/README.md">
---
description: Learn how to use the Answer Chat and Sidekicks
---

# Chat

The Answer Chat is the central interface for interacting with various AI-powered assistants. It provides a versatile platform for analyzing data, creating content, and getting specialized help across different domains.

## Selecting a Sidekick

At the top of the chat window, you'll find a dropdown menu that allows you to select a Sidekick. Sidekicks are specialized chatbots designed to assist you with specific tasks or areas of expertise.

### Types of Sidekicks

Sidekicks can come in various forms, such as:

-   Data Analysts: Help you interpret and visualize complex datasets
-   Content Creators: Assist in generating articles, blog posts, or marketing copy
-   Code Assistants: Provide programming help and code reviews
-   Research Aids: Support literature reviews and academic research
-   Business Strategists: Offer insights on market trends and business planning

## Using the Chat

1. Select your desired Sidekick from the dropdown menu.
2. Type your question or request in the chat input field.
3. Press Enter or click the send button to submit your message.
4. The Sidekick will process your input and provide a tailored response.

## Features

-   **Context Awareness**: Sidekicks can maintain context throughout the conversation, allowing for more natural and productive interactions.
-   **File Upload**: Some Sidekicks may support file uploads for analysis or processing.
-   **Customization**: Depending on the Sidekick, you may be able to adjust parameters or preferences to fine-tune the assistance you receive.

## Getting More Sidekicks

To expand your collection of Sidekicks, you can use the Sidekick Studio. Here's how to access and use it:

1. Navigate to the Sidekick Studio within the AnswerAgentAI platform.
2. In the Sidekick Studio, you'll find three tabs:

    - **My Sidekicks**: View and manage your current Sidekicks (chatflows).
    - **AnswerAgentAI Suggested**: Explore Sidekicks recommended by AnswerAgentAI.
    - **Shared**: Access Sidekicks shared by your organization or the community.

3. Browse through the available Sidekicks in each tab.
4. To add a new Sidekick, simply select it and follow the prompts to integrate it into your chat interface.

This feature allows you to continuously expand your toolkit of AI assistants, ensuring you have the right Sidekick for any task or project.

## Best Practices

-   Be specific in your requests to get the most accurate and helpful responses.
-   Provide necessary context when switching topics or starting a new task.
-   Experiment with different Sidekicks to find the best fit for your current needs.
-   Regularly check the Sidekick Studio for new and updated Sidekicks that might enhance your workflow.

Remember, while Sidekicks are powerful tools, they are AI assistants and may have limitations. Always review and verify important information or decisions.
</file>

<file path="community/getting-help.md">
---
description: Learn how to get help with AnswerAgentAI
---

# Getting Help with AnswerAgentAI

We understand that you might need assistance while using AnswerAgentAI. We've provided several ways for you to get help, report issues, and connect with our community. Here are the primary methods to get support:

## 1. In-App Support Sidekick

The quickest way to get help is by using our in-app Support Sidekick. This AI-powered assistant is available directly within the AnswerAgentAI application and can answer many of your questions instantly.

-   Look for the Support Sidekick icon in the bottom right corner of the application.
-   Click on it to open the chat interface.
-   Type your question or describe your issue, and the Sidekick will provide immediate assistance.

## 2. Join Our Discord Community

For more in-depth discussions, community support, and direct interaction with our team, join our Discord server:

-   Join the AnswerAgentAI Discord: [Discord Invite Link](https://discord.gg/X54ywt8pzj) <!-- TODO: Add actual Discord invite link -->
-   Introduce yourself in the #general channel.
-   Ask questions in the #support channel for community and team assistance.
-   Share your ideas and feedback in the #feedback channel.

## 3. Submit Feedback and Bug Reports

We value your feedback and want to know about any bugs you encounter. You can submit these through our Airtable form:

-   Access our feedback and bug report form: [Airtable Form Link](https://airtable.com/appIQM0aGhytwZPAO/pagWTeVKuMvGPcK2F/form) <!-- TODO: Add actual Airtable form link -->
-   Provide as much detail as possible about your feedback or the bug you've encountered.
-   Include screenshots or screen recordings if applicable.

## 4. GitHub Issues

For technical issues or feature requests, you can also use our GitHub repository:

-   Visit our GitHub Issues page: [AnswerAgentAI GitHub Issues](https://github.com/the-answerai/theanswer/) <!-- TODO: Add actual GitHub Issues link -->
-   Search existing issues to see if your problem has already been reported.
-   If not, create a new issue with a clear title and detailed description.
-   Follow the issue template if provided.

## Best Practices for Getting Help

1. Be specific: Clearly describe your problem or question.
2. Provide context: Include relevant details about your environment, AnswerAgentAI version, and steps to reproduce the issue.
3. Be patient: Our community and team are here to help, but responses may not always be immediate.
4. Pay it forward: Once you've received help, consider assisting others in the community when you can.

We're committed to providing you with the best possible support. Don't hesitate to reach out through any of these channels when you need assistance!
</file>

<file path="community/README.md">
---
description: Learn how to contribute to this project
---

# Community Guide

---

We appreciate our community! No matter your skill level or technical background, you can help this project grow. Here are a few ways to contribute:

##  Star

Star and share the [Github Repo](https://github.com/the-answerai/).

##  Share Chatflow

Yes! Sharing how you use AnswerAgentAI is a way of contribution. Export your chatflow as JSON, attach a screenshot and share it in [Show and Tell section](https://github.com/the-answerai).

##  Report Bugs

Found an issue? [Report it](https://airtable.com/appIQM0aGhytwZPAO/pagWTeVKuMvGPcK2F/form).
</file>

<file path="developers/authorization/app-level.md">
---
description: Learn how to set up app-level access control for your Flowise instances
---

# App Level

---

App level authorization protects your Flowise instance by username and password. This protects your apps from being accessible by anyone when deployed online.

<figure><img src="/.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" /><figcaption></figcaption></figure>

## How to Set Username & Password

### Npm

1. Install Flowise

```bash
npm install -g flowise
```

2. Start Flowise with username & password

```bash
npx flowise start --FLOWISE_USERNAME=user --FLOWISE_PASSWORD=1234
```

3. Open [http://localhost:3000](http://localhost:3000)

### Docker

1. Navigate to `docker` folder

```
cd docker
```

2. Create `.env` file and specify the `PORT`, `FLOWISE_USERNAME`, and `FLOWISE_PASSWORD`

```sh
PORT=3000
FLOWISE_USERNAME=user
FLOWISE_PASSWORD=1234
```

3. Pass `FLOWISE_USERNAME` and `FLOWISE_PASSWORD` to the `docker-compose.yml` file:

```
environment:
    - PORT=${PORT}
    - FLOWISE_USERNAME=${FLOWISE_USERNAME}
    - FLOWISE_PASSWORD=${FLOWISE_PASSWORD}
```

4. `docker compose up -d`
5. Open [http://localhost:3000](http://localhost:3000)
6. You can bring the containers down by `docker compose stop`

### Git clone

To enable app level authentication, add `FLOWISE_USERNAME` and `FLOWISE_PASSWORD` to the `.env` file in `packages/server`:

```
FLOWISE_USERNAME=user
FLOWISE_PASSWORD=1234
```
</file>

<file path="developers/authorization/chatflow-level.md">
---
description: Learn how to set up chatflow-level access control for your AnswerAgentAI instances
---

# Chatflow Level

---

After you have a chatflow / agentflow constructed, you might want to allow certain people to be able to access and interact with it. You can achieve that by assigning an API key for that specific chatflow.

## API Key

In dashboard, navigate to API Keys section, and you should be able to see a DefaultKey created. You can also add or delete any keys.

<figure><img src="/.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" /><figcaption></figcaption></figure>

## Chatflow

Navigate to the chatflow, and now you can select the API Key you want to use to protect the chatflow.

<figure><img src="/.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" /><figcaption></figcaption></figure>

After assigning an API key, one can only access the chatflow API when the Authorization header is provided with the correct API key specified during a HTTP call.

```json
"Authorization": "Bearer <your-api-key>"
```

An example of calling the API using POSTMAN

<figure><img src="/.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" /><figcaption></figcaption></figure>

You can specify the location where the api keys are stored by specifying `APIKEY_PATH` env variables. Read more [environment-variables.md](../environment-variables.md 'mention')
</file>

<file path="developers/authorization/google-oauth.md">
---
description: Complete guide to setting up Google OAuth for AnswerAgentAI integrations
---

# Google OAuth Setup

## Overview

Google OAuth 2.0 is required for integrating Google services with AnswerAgentAI, including Gmail, Google Drive, and Google Calendar. This guide covers the complete setup process from creating a Google Cloud Console project to configuring OAuth in your AnswerAgentAI instance.

## Prerequisites

-   A Google account
-   Access to Google Cloud Console
-   AnswerAgentAI instance (local or deployed)

## Step 1: Create Google Cloud Console Project

1. **Access Google Cloud Console**

    - Visit [Google Cloud Console](https://console.cloud.google.com/)
    - Sign in with your Google account

2. **Create a New Project**

    - Click on the project dropdown at the top of the page
    - Click "New Project"
    - Enter a project name (e.g., "AnswerAgentAI Integration")
    - Click "Create"

3. **Select Your Project**
    - Ensure your newly created project is selected in the project dropdown

## Step 2: Enable Required APIs

Navigate to **APIs & Services > Library** and enable the following APIs:

### Required APIs for AnswerAgentAI Integrations:

-   **Gmail API** - For Gmail document loader
-   **Google Drive API** - For Google Drive document loader
-   **Google Calendar API** - For calendar event tools
-   **Google Sheets API** - For spreadsheet integration (if using Drive)

For each API:

1. Search for the API name
2. Click on the API
3. Click "Enable"

## Step 3: Configure OAuth Consent Screen

1. **Navigate to OAuth Consent Screen**

    - Go to **APIs & Services > OAuth consent screen**

2. **Choose User Type**

    - Select "External" (unless you're using Google Workspace)
    - Click "Create"

3. **App Information**

    - App name: `AnswerAgentAI` (or your custom name)
    - User support email: Your email address
    - Developer contact information: Your email address

4. **App Domain (Optional but Recommended)**

    - Homepage URL: Your AnswerAgentAI instance URL
    - Privacy policy URL: Your privacy policy URL
    - Terms of service URL: Your terms of service URL

5. **Authorized Domains**

    - Add your domain (e.g., `yourdomain.com`)
    - For local development, you can skip this

6. **Save and Continue**

## Step 4: Add Required Scopes

1. **Click "Add or Remove Scopes"**

2. **Add the following scopes for AnswerAgentAI integrations:**

    ```
    https://www.googleapis.com/auth/gmail.readonly
    https://www.googleapis.com/auth/gmail.modify
    https://www.googleapis.com/auth/drive.readonly
    https://www.googleapis.com/auth/drive.file
    https://www.googleapis.com/auth/calendar
    https://www.googleapis.com/auth/calendar.events
    https://www.googleapis.com/auth/userinfo.email
    https://www.googleapis.com/auth/userinfo.profile
    ```

### Scope Descriptions:

| Scope              | Purpose                                     | Used By                      |
| ------------------ | ------------------------------------------- | ---------------------------- |
| `gmail.readonly`   | Read Gmail messages and labels              | Gmail Document Loader        |
| `gmail.modify`     | Modify Gmail messages (for marking as read) | Gmail Document Loader        |
| `drive.readonly`   | Read Google Drive files                     | Google Drive Document Loader |
| `drive.file`       | Access files created by the app             | Google Drive Document Loader |
| `calendar`         | Full calendar access                        | Calendar Tools               |
| `calendar.events`  | Manage calendar events                      | Calendar Tools               |
| `userinfo.email`   | Access user's email address                 | All integrations             |
| `userinfo.profile` | Access user's profile information           | All integrations             |

3. **Save and Continue**

## Step 5: Create OAuth 2.0 Credentials

1. **Navigate to Credentials**

    - Go to **APIs & Services > Credentials**

2. **Create Credentials**

    - Click "Create Credentials"
    - Select "OAuth 2.0 Client IDs"

3. **Configure OAuth Client**

    - Application type: "Web application"
    - Name: `AnswerAgentAI OAuth Client`

4. **Authorized Redirect URIs**
   Add the following URIs based on your setup:

    **For Local Development:**

    ```
    http://localhost:3000/api/v1/callback/googleoauth
    ```

    **For Production:**

    ```
    https://yourdomain.com/api/v1/callback/googleoauth
    ```

5. **Create and Download**
    - Click "Create"
    - Download the JSON file with your credentials
    - **Important:** Keep this file secure and never commit it to version control

## Step 6: Configure Environment Variables

Add the following environment variables to your AnswerAgentAI instance:

```bash
# Google OAuth Configuration
GOOGLE_CLIENT_ID=your_client_id_here
GOOGLE_CLIENT_SECRET=your_client_secret_here
GOOGLE_CALLBACK_URL=http://localhost:3000/api/v1/callback/googleoauth

# For production, use your domain:
# GOOGLE_CALLBACK_URL=https://yourdomain.com/api/v1/callback/googleoauth
```

## Step 7: Credential Configuration in AnswerAgentAI

### Creating Google OAuth Credential

1. **Navigate to Credentials**

    - In AnswerAgentAI, go to the Credentials section
    - Click "Add Credential"

2. **Select Google OAuth**

    - Choose "Google OAuth" from the credential types

3. **Configure Credential**

    - **Credential Name:** Give it a descriptive name (e.g., "My Google Account")
    - The system will redirect you to Google for authorization

4. **Authorize Access**
    - Sign in to your Google account
    - Review and accept the requested permissions
    - You'll be redirected back to AnswerAgentAI

## End User OAuth Flow

### For End Users Connecting Their Accounts

Once developers have set up the Google OAuth application, end users can connect their Google accounts through the following process:

1. **Access Credentials Section**

    - Navigate to the Credentials section in AnswerAgentAI
    - Click "Add Credential"

2. **Select Google OAuth**

    - Choose "Google OAuth" from the available credential types

3. **Authorization Process**

    - Click the authorization button
    - You'll be redirected to Google's authorization page
    - Sign in with your Google account if not already signed in

4. **Grant Permissions**

    - Review the requested permissions
    - These permissions allow AnswerAgentAI to access specific Google services
    - Click "Allow" to grant access

5. **Confirmation**

    - You'll be redirected back to AnswerAgentAI
    - Your Google account is now connected and ready to use

6. **Using the Credential**
    - Select your Google OAuth credential when configuring:
        - Gmail Document Loader
        - Google Drive Document Loader
        - Google Calendar Tools

### Troubleshooting User Authorization

**Common Issues:**

1. **"Error 400: redirect_uri_mismatch"**

    - Ensure the redirect URI in Google Console matches your AnswerAgentAI instance URL
    - Check that GOOGLE_CALLBACK_URL environment variable is correct

2. **"Access Blocked: This app's request is invalid"**

    - Verify all required scopes are added in Google Console
    - Ensure OAuth consent screen is properly configured

3. **"Refresh Token Issues"**
    - Tokens automatically refresh, but if issues persist:
    - Re-authorize the credential
    - Check token expiration in credential settings

## Security Best Practices

1. **Environment Variables**

    - Never expose client secrets in frontend code
    - Use environment variables for all sensitive configuration

2. **Scope Minimization**

    - Only request the minimum scopes required for your use case
    - Regularly review and remove unused scopes

3. **Token Management**

    - AnswerAgentAI automatically handles token refresh
    - Monitor token usage and expiration

4. **Domain Verification**
    - For production, verify your domain in Google Console
    - This removes the "unverified app" warning for users

## Testing Your Setup

1. **Create a Test Credential**

    - Follow the end user flow to create a Google OAuth credential

2. **Test Integration**

    - Try using Gmail Document Loader with your credential
    - Verify Google Drive access works
    - Test calendar event creation

3. **Check Permissions**
    - Ensure all required scopes are working
    - Verify data is loading correctly

## Advanced Configuration

### Custom Scopes

If you need additional Google API access, add the required scopes to your OAuth consent screen and update your application accordingly.

### Multi-Domain Setup

For multiple domains, add all authorized redirect URIs to your OAuth client configuration.

### Workspace Integration

For Google Workspace customers, you can use "Internal" user type for enhanced security and reduced approval requirements.

## Troubleshooting

### Common Setup Issues

1. **APIs Not Enabled**

    - Verify all required APIs are enabled in Google Console
    - Check API quotas and limits

2. **Incorrect Redirect URI**

    - Ensure redirect URIs match exactly (including http/https and trailing slashes)
    - Check environment variable configuration

3. **Scope Issues**

    - Verify all required scopes are added to OAuth consent screen
    - Some scopes may require Google verification for production use

4. **Token Expiration**
    - AnswerAgentAI handles automatic token refresh
    - If issues persist, re-authorize the credential

### Getting Help

-   Check Google Cloud Console error logs
-   Review AnswerAgentAI application logs
-   Ensure environment variables are correctly set
-   Verify network connectivity to Google APIs

---

**Next Steps:** Once OAuth is configured, you can set up specific Google integrations:

-   [Gmail Document Loader](../../sidekick-studio/chatflows/document-loaders/gmail.md)
-   [Google Drive Document Loader](../../sidekick-studio/chatflows/document-loaders/google-drive.md)
-   [Google Calendar Tools](../../sidekick-studio/chatflows/tools/google-calendar.md)
</file>

<file path="developers/authorization/README.md">
---
description: Learn how to secure your Flowise Instances
---

# Auth

---

This section guides you through configuring security with Flowise, focusing on authentication mechanisms at the application and chatflow levels.&#x20;

By implementing robust authentication, you can protect your Flowise instances and ensure only authorized users can access and interact with your chatflows.

## Supported Methods

-   [App level](app-level.md)
-   [Chatflow level](chatflow-level.md)
-   [Google OAuth](google-oauth.md)
</file>

<file path="developers/deployment/aws.md">
---
description: Learn how to deploy AnswerAgentAI on AWS using Copilot
---

# AWS

## Overview

AWS Copilot is a command-line interface (CLI) tool that simplifies the process of deploying and managing containerized applications on AWS. This guide will walk you through the steps to deploy AnswerAgentAI using AWS Copilot.

## Key Benefits

-   Simplified deployment process
-   Automatic infrastructure provisioning
-   Easy management of multiple environments

## Prerequisites

Before you begin, ensure you have the following:

-   An AWS account
-   AWS CLI installed and configured
-   Docker installed
-   AWS Copilot CLI installed
-   AnswerAgentAI application code
-   Environment variables for your application

## How to Use

### 1. Install AWS Copilot

If you haven't already installed AWS Copilot, you can do so by running:

```bash
brew install aws/tap/copilot-cli   # For macOS
```

For other operating systems, refer to the [AWS Copilot installation guide](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Copilot.html#copilot-install).

### 2. Configure AWS CLI

Ensure your AWS CLI is configured with the correct credentials:

```bash
aws configure
```

### 3. Clone the AnswerAgentAI Repository

Clone the AnswerAgentAI repository to your local machine:

```bash
git clone https://github.com/the-answeri/theanswer
cd theanswer
```

### 4. Initialize the Copilot Application

In the root directory of your AnswerAgentAI project, run:
Ensure yo uhave setup a hosted zone in AWS.
// TODO: Add Details

```bash
copilot app init --domain <your-domain>
```

Replace `<your-domain>` with your desired domain name, e.g., `myapp.flowise.theanswer.ai`.

### 5. Create a New Environment

Create a new environment for your application:

```bash
copilot env init --name <env-name> --profile default
```

Replace `<env-name>` with your desired environment name (e.g., "production" or "staging").

### 6. Set Up Environment Variables

Create an environment file named `<env-name>.env` in your project directory. Add all required environment variables to this file. For example:

```
PORT=3000
APIKEY_PATH=/path/to/api/key
SECRETKEY_PATH=/path/to/secret/key
LOG_PATH=/path/to/logs
DISABLE_FLOWISE_TELEMETRY=true
IFRAME_ORIGINS=https://example.com
MY_APP_VITE_AUTH_DOMAIN=your-auth-domain
MY_APP_VITE_AUTH_CLIENT_ID=your-auth-client-id
...
```

For a list of all environment variables, please refer to the [Environment Variables](../environment-variables.md) page.

### 7. Deploy the Service

Deploy your AnswerAgentAI service to the created environment:

```bash
copilot svc deploy --env <env-name>
```

This command will build your Docker image, push it to Amazon ECR, and deploy it to Amazon ECS.

### 8. Access Your Deployed Service

To get the URL of your deployed service, run:

```bash
copilot svc show
```

This will display information about your service, including its public URL.

## Tips and Best Practices

-   Use separate environments (e.g., "staging" and "production") to test changes before deploying to production.
-   Regularly update your AWS Copilot CLI to access the latest features and improvements.
-   Use AWS Copilot's built-in commands to manage your application, such as `copilot svc logs` to view service logs.
-   Implement a CI/CD pipeline to automate deployments using AWS Copilot commands.

## Troubleshooting

### Issue: Deployment Fails

If your deployment fails, check the following:

1. Ensure all required environment variables are correctly set in your `<env-name>.env` file.
2. Verify that your AWS CLI is configured with the correct credentials and permissions.
3. Check the Copilot logs for detailed error messages:

```bash
copilot svc logs --env <env-name>
```

### Issue: Unable to Access the Deployed Service

If you can't access your deployed service:

1. Verify that the service is running:

```bash
copilot svc status --env <env-name>
```

2. Check if the correct ports are exposed in your Dockerfile and Copilot configuration.
3. Ensure your domain's DNS settings are correctly configured to point to the AWS-provided URL.

Remember to regularly update your AnswerAgentAI application and redeploy using Copilot to ensure you have the latest features and security updates.

<!-- TODO: Add a screenshot of the successful deployment -->
</file>

<file path="developers/deployment/azure.md">
---
description: Learn how to deploy AnswerAgentAI on Azure using Terraform and Azure Container Instances
---

# Azure

## Overview

This guide covers two methods for deploying AnswerAgentAI on Azure:

1. Using Terraform to deploy AnswerAgentAI as an Azure App Service with Postgres
2. Using Azure Container Instances (ACI) through the Azure Portal UI or Azure CLI

## Method 1: AnswerAgentAI as Azure App Service with Postgres Using Terraform

### Prerequisites

1. **Azure Account**: Ensure you have an Azure account with an active subscription. If you don't have one, sign up at [Azure Portal](https://portal.azure.com/).
2. **Terraform**: Install Terraform CLI on your machine. Download it from [Terraform's website](https://www.terraform.io/downloads.html).
3. **Azure CLI**: Install Azure CLI. Instructions can be found on the [Azure CLI documentation page](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli).

### Setting Up Your Environment

1. **Login to Azure**: Open your terminal or command prompt and login to Azure CLI:

```bash
az login --tenant <Your Subscription ID> --use-device-code
```

2. **Set Subscription**:

```bash
az account set --subscription <Your Subscription ID>
```

3. **Initialize Terraform**:

Create a `terraform.tfvars` file in your Terraform project directory with the following content:

```hcl
subscription_name = "subscription_name"
subscription_id = "subscription id"
project_name = "webapp_name"
db_username = "PostgresUserName"
db_password = "strongPostgresPassword"
flowise_username = "flowiseUserName"
flowise_password = "strongFlowisePassword"
flowise_secretkey_overwrite = "longandStrongSecretKey"
webapp_ip_rules = [
  {
    name = "AllowedIP"
    ip_address = "X.X.X.X/32"
    headers = null
    virtual_network_subnet_id = null
    subnet_id = null
    service_tag = null
    priority = 300
    action = "Allow"
  }
]
postgres_ip_rules = {
  "ValbyOfficeIP" = "X.X.X.X"
}
source_image = "flowiseai/flowise:latest"
tagged_image = "flow:v1"
```

Replace the placeholders with your actual values.

4. **Initialize Terraform**:

Navigate to your Terraform project directory and run:

```bash
terraform init
```

### Deploying with Terraform

1. **Plan the Deployment**:

```bash
terraform plan
```

2. **Apply the Deployment**:

```bash
terraform apply
```

3. **Verify the Deployment**: Once Terraform has completed, check the Azure Portal to verify that the resources are correctly deployed.

## Method 2: Azure Container Instance

### Prerequisites

1. _(Optional)_ [Install Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) if you'd like to use CLI commands

### Option A: Create a Container Instance without Persistent Storage

#### Using Azure Portal

1. Search for Container Instances in Marketplace and click Create.
2. Configure basic settings:
    - Select or create a Resource group
    - Set Container name, Region, Image source (Other registry)
    - Set Image to `flowiseai/flowise`
    - Choose OS type and Size
3. Configure networking:
    - Add port `3000 (TCP)` in addition to the default `80 (TCP)`
4. Configure advanced settings:
    - Set Restart policy to `On failure`
    - Add Environment variables: `FLOWISE_USERNAME` and `FLOWISE_PASSWORD`
    - Set Command override to `["/bin/sh", "-c", "flowise start"]`
5. Review and create the container instance.

<!-- TODO: Add a screenshot of the successful deployment in Azure Portal -->

#### Using Azure CLI

1. Create a resource group:

```bash
az group create --name flowise-rg --location "West US"
```

2. Create a Container Instance:

```bash
az container create -g flowise-rg \
    --name flowise \
    --image flowiseai/flowise \
    --command-line "/bin/sh -c 'flowise start'" \
    --environment-variables FLOWISE_USERNAME=flowise-user FLOWISE_PASSWORD=flowise-password \
    --ip-address public \
    --ports 80 3000 \
    --restart-policy OnFailure
```

3. Access AnswerAgentAI at the IP address (including port :3000) provided in the output.

### Option B: Create a Container Instance with Persistent Storage

This option is only possible using Azure CLI:

1. Create a resource group:

```bash
az group create --name flowise-rg --location "West US"
```

2. Create a Storage Account and File share. Refer to [Azure documentation](https://learn.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-portal?tabs=azure-portal) for detailed steps.

3. Create a Container Instance with persistent storage:

```bash
az container create -g flowise-rg \
    --name flowise \
    --image flowiseai/flowise \
    --command-line "/bin/sh -c 'flowise start'" \
    --environment-variables FLOWISE_USERNAME=flowise-user FLOWISE_PASSWORD=flowise-password DATABASE_PATH=/opt/flowise/.flowise APIKEY_PATH=/opt/flowise/.flowise SECRETKEY_PATH=/opt/flowise/.flowise LOG_PATH=/opt/flowise/.flowise/logs BLOB_STORAGE_PATH=/opt/flowise/.flowise/storage \
    --ip-address public \
    --ports 80 3000 \
    --restart-policy OnFailure \
    --azure-file-volume-share-name <your-file-share-name> \
    --azure-file-volume-account-name <your-storage-account-name> \
    --azure-file-volume-account-key <your-storage-account-key> \
    --azure-file-volume-mount-path /opt/flowise/.flowise
```

Replace the placeholders with your actual values.

4. Access AnswerAgentAI at the IP address (including port :3000) provided in the output.

## Tips and Best Practices

-   Use separate resource groups for different environments (e.g., development, staging, production).
-   Regularly update your AnswerAgentAI image to get the latest features and security updates.
-   Monitor your Azure resources for performance and cost optimization.
-   Implement proper security measures, such as network security groups and Azure Private Link, for production deployments.

## Troubleshooting

-   If you encounter issues with Terraform deployment, check the Azure activity log for detailed error messages.
-   For Container Instances, use `az container logs` to view container logs for debugging.
-   Ensure that all required environment variables are correctly set for AnswerAgentAI to function properly.

Remember to comply with Azure's best practices for security and cost management when deploying your AnswerAgentAI instance.

<!-- TODO: Add a video tutorial on deploying to Azure Container Instance -->
</file>

<file path="developers/deployment/gcp.md">
---
description: Learn how to deploy Flowise on GCP
---

# GCP

---

## Prerequisites

1. Notedown your Google Cloud \[ProjectId]
2. Install [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
3. Install the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install-sdk)
4. Install [Docker Desktop](https://docs.docker.com/desktop/)

## Setup Kubernetes Cluster

1. Create a Kubernetes Cluster if you don't have one.

<figure><img src="/.gitbook/assets/gcp/1.png" alt="" /><figcaption><p>Click `Clusters` to create one.</p></figcaption></figure>

2. Name the Cluster, choose the right resource location, use `Autopilot` mode and keep all other default configs.
3. Once the Cluster is created, Click the 'Connect' menu from the actions menu

<figure><img src="/.gitbook/assets/gcp/2.png" alt="" /><figcaption></figcaption></figure>

4. Copy the command and paste into your terminal and hit enter to connect your cluster.
5. Run the below command and select correct context name, which looks like `gke_[ProjectId]_[DataCenter]_[ClusterName]`

```
kubectl config get-contexts
```

6. Set the current context

```
kubectl config use-context gke_[ProjectId]_[DataCenter]_[ClusterName]
```

## Build and Push the Docker image

Run the following commands to build and push the Docker image to GCP Container Registry.

1. Clone the Flowise

```
git clone https://github.com/FlowiseAI/Flowise.git
```

2. Build the Flowise

```
cd Flowise
pnpm install
pnpm build
```

3. Update the `Dockerfile` file a little.

> Specify the platform of nodejs
>
> ```
> FROM --platform=linux/amd64 node:18-alpine
> ```
>
> Add python3, make and g++ to install
>
> ```
> RUN apk add --no-cache python3 make g++
> ```

3. Build as Docker image, make sure the Docker desktop app is running

```
docker build -t gcr.io/[ProjectId]/flowise:dev .
```

4. Push the Docker image to GCP container registry.

```
docker push gcr.io/[ProjectId]/flowise:dev
```

## Deployment to GCP

1. Create a `yamls` root folder in the project.
2. Add the `deployment.yaml` file into that folder.

```
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flowise
  labels:
    app: flowise
spec:
  selector:
    matchLabels:
      app: flowise
  replicas: 1
  template:
    metadata:
      labels:
        app: flowise
    spec:
      containers:
      - name: flowise
        image: gcr.io/[ProjectID]/flowise:dev
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
```

3. Add the `service.yaml` file into that folder.

```
# service.yaml
apiVersion: "v1"
kind: "Service"
metadata:
  name: "flowise-service"
  namespace: "default"
  labels:
    app: "flowise"
spec:
  ports:
  - protocol: "TCP"
    port: 80
    targetPort: 3000
  selector:
    app: "flowise"
  type: "LoadBalancer"

```

It will be look like below.

<figure><img src="/.gitbook/assets/gcp/3.png" alt="" /><figcaption></figcaption></figure>

4. Deploy the yaml files by running following commands.

```
kubectl apply -f yamls/deployment.yaml
kubectl apply -f yamls/service.yaml
```

5. Go to `Workloads` in the GCP, you can see your pod is running.

<figure><img src="/.gitbook/assets/gcp/4.png" alt="" /><figcaption></figcaption></figure>

6. Go to `Services & Ingress`, you can click the `Endpoint` where the Flowise is hosted.

<figure><img src="/.gitbook/assets/gcp/5.png" alt="" /><figcaption></figcaption></figure>

## Congratulations!

You have successfully hosted the Flowise apps on GCP [](https://emojipedia.org/partying-face/)

## Timeout

By default, there is a 30 seconds timeout assigned to the proxy by GCP. This caused issue when the response is taking longer than 30 seconds threshold to return. In order to fix this issue, make the following changes to YAML files:

Note: To set the timeout to be 10 minutes (for example) -- we specify 600 seconds below.

1. Create a `backendconfig.yaml` file with the following content:

```yaml
apiVersion: cloud.google.com/v1
kind: BackendConfig
metadata:
    name: flowise-backendconfig
    namespace: your-namespace
spec:
    timeoutSec: 600
```

2. Issue: `kubectl apply -f backendconfig.yaml`
3. Update your `service.yaml` file with the following reference to the `BackendConfig`:

```yaml
apiVersion: v1
kind: Service
metadata:
    annotations:
        cloud.google.com/backend-config: '{"default": "flowise-backendconfig"}'
    name: flowise-service
    namespace: your-namespace
```

4. Issue: `kubectl apply -f service.yaml`
</file>

<file path="developers/deployment/README.md">
---
description: Learn how to deploy AnswerAgentAI to the cloud
---

# Deployment

AnswerAgentAI is designed with a platform-agnostic architecture, ensuring compatibility with a wide range of deployment environments to suit your infrastructure needs.

## Cloud Providers

The following cloud providers offer robust platforms for deploying AnswerAgentAI. These established providers require a higher level of technical expertise to manage and optimize for your specific needs, but grant greater flexibility and control over your cloud environment.

-   [AWS](aws.md)
-   [Azure](azure.md)
-   [GCP](gcp.md)
-   [Render](render.md)

Each of these platforms provides unique features and capabilities. Choose the one that best aligns with your organization's requirements, existing infrastructure, and technical expertise.

For detailed deployment instructions for each platform, please refer to the respective links above.

## Local Machine

To deploy AnswerAgentAI locally, follow our [Get Started Running Locally](../running-locally.md) guide.
</file>

<file path="developers/deployment/render.md">
---
description: Learn how to deploy Flowise on Render
---

# Render

---

1. Fork [Flowise Official Repository](https://github.com/FlowiseAI/Flowise)
2. Visit your github profile to assure you have successfully made a fork
3. Sign in to [Render](https://dashboard.render.com)
4. Click **New +**

<figure><img src="/.gitbook/assets/render/1.png" alt="" width="563" /><figcaption></figcaption></figure>

5. Select **Web Service**

<figure><img src="/.gitbook/assets/render/2.png" alt="" /><figcaption></figcaption></figure>

6. Connect Your GitHub Account
7. Select your forked Flowise repo and click **Connect**

<figure><img src="/.gitbook/assets/render/3.png" alt="" width="563" /><figcaption></figcaption></figure>

8. Fill in your preferred **Name** and **Region.**
9. Select `Docker` as your **Runtime**

<figure><img src="/.gitbook/assets/render/4.png" alt="" /><figcaption></figcaption></figure>

9. Select an **Instance**

<figure><img src="/.gitbook/assets/render/5.png" alt="" /><figcaption></figcaption></figure>

10. _(Optional)_ Add app level authorization, click **Advanced** and add `Environment Variable`

-   FLOWISE_USERNAME
-   FLOWISE_PASSWORD

<figure><img src="/.gitbook/assets/render/6.png" alt="" /><figcaption></figcaption></figure>

Add `NODE_VERSION` with value `18.18.1` as the node version to run the instance.

There are list of env variables you can configure. Refer to [environment-variables.md](../environment-variables.md 'mention')

11. Click **Create Web Service**

<figure><img src="/.gitbook/assets/render/7.png" alt="" /><figcaption></figcaption></figure>

12. Navigate to the deployed URL and that's it [](https://emojipedia.org/rocket/)[](https://emojipedia.org/rocket/)

<figure><img src="/.gitbook/assets/render/8.png" alt="" /><figcaption></figcaption></figure>

## Persistent Disk

The default filesystem for services running on Render is ephemeral. Flowise data isnt persisted across deploys and restarts. To solve this issue, we can use [Render Disk](https://render.com/docs/disks).

1. On the left hand side bar, click **Disks**
2. Name your disk, and specify the **Mount Path** to `/opt/render/.flowise`

<figure><img src="/.gitbook/assets/render/9.png" alt="" /><figcaption></figcaption></figure>

3. Click the **Environment** section, and add these new environment variables:

-   DATABASE_PATH - `/opt/render/.flowise`
-   APIKEY_PATH - `/opt/render/.flowise`
-   LOG_PATH - `/opt/render/.flowise/logs`
-   SECRETKEY_PATH - `/opt/render/.flowise`
-   BLOB_STORAGE_PATH - `/opt/render/.flowise/storage`

<figure><img src="/.gitbook/assets/image (1) (5).png" alt="" /><figcaption></figcaption></figure>

4. Click **Manual Deploy** then select **Clear build cache & deploy**

<figure><img src="/.gitbook/assets/render/11.png" alt="" /><figcaption></figcaption></figure>

5. Now try creating a flow and save it in Flowise. Then try restarting service or redeploy, you should still be able to see the flow you have saved previously.

Watch how to deploy to Render

<iframe src="https://www.youtube.com/embed/Fxyc6-frgrI"></iframe>

<iframe src="https://www.youtube.com/embed/l-0NzOMeCco"></iframe>
</file>

<file path="developers/use-cases/calling-children-flows.md">
---
description: Learn how to effectively use the Chatflow Tool and the Custom Tool
---

# Calling Children Flows

---

One of the powerful features of AnswerAgentAI is that you can turn flows into tools. For example, having a main flow to orchestrate which/when to use the necessary tools. And each tool is designed to perform a niece/specific thing.

This offers a few benefits:

-   Each children flow as tool will execute on its own, with separate memory to allow cleaner output
-   Aggregating detailed outputs from each children flow to a final agent, often results in higher quality output

You can achieve this by using the following tools:

-   Chatflow Tool
-   Custom Tool

## Chatflow Tool

1. Have a chatflow ready. In this case, we create a Chain of Thought chatflow that can go through multiple chainings.

<figure><img src="/.gitbook/assets/image (169).png" alt="" /><figcaption></figcaption></figure>

2. Create another chatflow with Tool Agent + Chatflow Tool. Select the chatflow you want to call from the tool. In this case, it was Chain of Thought chatflow. Give it a name, and an appropriate description to let LLM knows when to use this tool:

<figure><img src="/.gitbook/assets/image (35).png" alt="" width="245" /><figcaption></figcaption></figure>

3. Test it out!

<figure><img src="/.gitbook/assets/image (168).png" alt="" /><figcaption></figcaption></figure>

4. From the response, you can see the input and output from the Chatflow Tool:

<figure><img src="/.gitbook/assets/image (170).png" alt="" /><figcaption></figcaption></figure>

## Custom Tool

With the same example as above, we are going to create a custom tool that will calls the [Prediction API](/docs/api/prediction/create-prediction) of the Chain of Thought chatflow.

1. Create a new tool:

<table><thead><tr><th width="180">Tool Name</th><th>Tool Description</th></tr></thead><tbody><tr><td>ideas_flow</td><td>Use this tool when you need to achieve certain objective</td></tr></tbody></table>

Input Schema:

<table><thead><tr><th>Property</th><th>Type</th><th>Description</th><th data-type="checkbox">Required</th></tr></thead><tbody><tr><td>input</td><td>string</td><td>input question</td><td>true</td></tr></tbody></table>

<figure><img src="/.gitbook/assets/image (95) (1).png" alt="" /><figcaption></figcaption></figure>

Javascript Function of the tool:

```javascript
const fetch = require('node-fetch')
const url = 'http://localhost:3000/api/v1/prediction/<chatflow-id>' // replace with specific chatflow id

const body = {
    question: $input
}

const options = {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json'
    },
    body: JSON.stringify(body)
}

try {
    const response = await fetch(url, options)
    const resp = await response.json()
    return resp.text
} catch (error) {
    console.error(error)
    return ''
}
```

2. Create a Tool Agent + Custom Tool. Specify the tool we've created in Step 1 in the Custom Tool.

<figure><img src="/.gitbook/assets/image (97).png" alt="" /><figcaption></figcaption></figure>

3. From the response, you can see the input and output from the Custom Tool:

<figure><img src="/.gitbook/assets/image (99).png" alt="" /><figcaption></figcaption></figure>

## Conclusion

In this example, we have successfully demonstrate 2 ways of turning other chatflows into tools, via Chatflow Tool and Custom Tool. Both are using the same code logic under the hood.
</file>

<file path="developers/use-cases/interacting-with-api.md">
---
description: Learn how to use external API integrations with AnswerAgentAI
---

# Interacting with API

---

The OpenAPI Specification (OAS) defines a standard, language-agnostic interface to HTTP APIs. The goal of this use case is to have the LLM automatically figure out which API to call, while still having a stateful conversation with user.

## OpenAPI Chain

1. In this tutorial, we are going to use [Klarna OpenAPI](https://www.klarna.com/us/shopping/public/openai/v0/api-docs/)

```json
{
    "openapi": "3.0.1",
    "info": {
        "version": "v0",
        "title": "Open AI Klarna product Api"
    },
    "servers": [
        {
            "url": "https://www.klarna.com/us/shopping"
        }
    ],
    "tags": [
        {
            "name": "open-ai-product-endpoint",
            "description": "Open AI Product Endpoint. Query for products."
        }
    ],
    "paths": {
        "/public/openai/v0/products": {
            "get": {
                "tags": ["open-ai-product-endpoint"],
                "summary": "API for fetching Klarna product information",
                "operationId": "productsUsingGET",
                "parameters": [
                    {
                        "name": "countryCode",
                        "in": "query",
                        "description": "ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.",
                        "required": true,
                        "schema": {
                            "type": "string"
                        }
                    },
                    {
                        "name": "q",
                        "in": "query",
                        "description": "A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!",
                        "required": true,
                        "schema": {
                            "type": "string"
                        }
                    },
                    {
                        "name": "size",
                        "in": "query",
                        "description": "number of products returned",
                        "required": false,
                        "schema": {
                            "type": "integer"
                        }
                    },
                    {
                        "name": "min_price",
                        "in": "query",
                        "description": "(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.",
                        "required": false,
                        "schema": {
                            "type": "integer"
                        }
                    },
                    {
                        "name": "max_price",
                        "in": "query",
                        "description": "(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.",
                        "required": false,
                        "schema": {
                            "type": "integer"
                        }
                    }
                ],
                "responses": {
                    "200": {
                        "description": "Products found",
                        "content": {
                            "application/json": {
                                "schema": {
                                    "$ref": "#/components/schemas/ProductResponse"
                                }
                            }
                        }
                    },
                    "503": {
                        "description": "one or more services are unavailable"
                    }
                },
                "deprecated": false
            }
        }
    },
    "components": {
        "schemas": {
            "Product": {
                "type": "object",
                "properties": {
                    "attributes": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "name": {
                        "type": "string"
                    },
                    "price": {
                        "type": "string"
                    },
                    "url": {
                        "type": "string"
                    }
                },
                "title": "Product"
            },
            "ProductResponse": {
                "type": "object",
                "properties": {
                    "products": {
                        "type": "array",
                        "items": {
                            "$ref": "#/components/schemas/Product"
                        }
                    }
                },
                "title": "ProductResponse"
            }
        }
    }
}
```

<figure><img src="/.gitbook/assets/image (133).png" alt="" /><figcaption></figcaption></figure>

3. However, if you want to have a normal conversation chat, it is not able to do so. You will see the following error:

<figure><img src="/.gitbook/assets/image (134).png" alt="" width="361" /><figcaption></figcaption></figure>

## Tool Agent + OpenAPI Chain

In order to solve the above error, we can use Agent.

1. Connect **OpenAPI Chain** with a **Chain Tool**. This allow the chain to be used as tool. Under the tool, we give an appropriate description as in when should the LLM uses this tool. For example:

```
useful when you need to search and return answer about tshirts
```

<figure><img src="/.gitbook/assets/image (135).png" alt="" /><figcaption></figcaption></figure>

2. Connect the **Chain Tool** with a **Tool Agent**:

<figure><img src="/.gitbook/assets/image (136).png" alt="" /><figcaption></figcaption></figure>

3. Let's try it!

<figure><img src="/.gitbook/assets/image (137).png" alt="" width="563" /><figcaption></figcaption></figure>

## Conclusion

We've successfully created an agent that can interact with API when necessary, and still be able handle stateful conversations with users. Below is the template:

<a href="/.gitbook/assets/OpenAPI Chatflow.json" download>Download /.gitbook/assets/OpenAPI Chatflow.json</a>
</file>

<file path="developers/use-cases/multiple-documents-qna.md">
---
description: Learn how to query multiple documents correctly
---

# Multiple Documents QnA

---

From the last [Web Scrape QnA](web-scrape-qna.md) example, we are only upserting and querying 1 website. What if we have multiple websites, or multiple documents? Let's take a look and see how we can achieve that.

In this example, we are going to perform QnA on 2 PDFs, which are FORM-10K of APPLE and TESLA.

<div align="left" data-full-width="false">

<figure><img src="/.gitbook/assets/image (93).png" alt="" width="375" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (94).png" alt="" width="375" /><figcaption></figcaption></figure>

</div>

## Upsert

1. Find the example flow called - **Conversational Retrieval QA Chain** from the marketplace templates.
2. We are going to use [PDF File Loader](../../sidekick-studio/chatflows/document-loaders/pdf-file.md), and upload the respective files:

<figure><img src="/.gitbook/assets/multi-docs-upload.png" alt="" /><figcaption></figcaption></figure>

3. Click the **Additional Parameters** of PDF File Loader, and specify metadata object. For instance, PDF File with Apple FORM-10K uploaded can have a metadata object `{source: apple}`, whereas PDF File with Tesla FORM-10K uploaded can have `{source: tesla}` . This is done to seggregate the documents during retrieval time.

<div align="left">

<figure><img src="/.gitbook/assets/multi-docs-apple.png" alt="" width="563" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/multi-docs-tesla.png" alt="" width="563" /><figcaption></figcaption></figure>

</div>

4. After filling in the credentials for Pinecone, click Upsert:

<figure><img src="/.gitbook/assets/multi-docs-upsert.png" alt="" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (98).png" alt="" /><figcaption></figcaption></figure>

5. On the [Pinecone console](https://app.pinecone.io) you will be able to see the new vectors that were added.

<figure><img src="/.gitbook/assets/multi-docs-console.png" alt="" /><figcaption></figcaption></figure>

## Query

1. After verifying data has been upserted to Pinecone, we can now start asking question in the chat!

<figure><img src="/.gitbook/assets/image (100).png" alt="" /><figcaption></figcaption></figure>

2. However, the context retrieved used to return the answer is a mix of both APPLE and TESLA documents. As you can see from the Source Documents:

<div align="left">

<figure><img src="/.gitbook/assets/Untitled (7).png" alt="" width="563" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/Untitled (8).png" alt="" width="563" /><figcaption></figcaption></figure>

</div>

3. We can fix this by specifying a metadata filter from the Pinecone node. For example, if we only want to retrieve context from APPLE FORM-10K, we can look back at the metadata we have specified earlier in the [#upsert](multiple-documents-qna.md#upsert 'mention') step, then use the same in the Metadata Filter below:

<figure><img src="/.gitbook/assets/image (102).png" alt="" /><figcaption></figcaption></figure>

4. Let's ask the same question again, we should now see all context retrieved are indeed from APPLE FORM-10K:

<figure><img src="/.gitbook/assets/image (103).png" alt="" /><figcaption></figcaption></figure>

:::info
Each vector databse provider has different format of filtering syntax, recommend to read through the respective vector database documentation
:::

5. However, the problem with this is that metadata filtering is sort of _**"hard-coded"**_. Ideally, we should let the LLM to decide which document to retrieve based on the question.

## Tool Agent

We can solve the _**"hard-coded"**_ metadata filter problem by using [Tool Agent](../../sidekick-studio/chatflows/agents/tool-agent.md).

By providing tools to agent, we can let the agent to decide which tool is suitable to be used depending on the question.

1. Create a [Retriever Tool](../../sidekick-studio/chatflows/tools/retriever-tool.md) with following name and description:

<table><thead><tr><th width="178">Name</th><th>Description</th></tr></thead><tbody><tr><td>search_apple</td><td>Use this function to answer user questions about Apple Inc (APPL). It contains a SEC Form 10K filing describing the financials of Apple Inc (APPL) for the 2022 time period.</td></tr></tbody></table>

2. Connect to Pinecone node with metadata filter `{source: apple}`

<figure><img src="/.gitbook/assets/image (104).png" alt="" width="563" /><figcaption></figcaption></figure>

3. Repeat the same for Tesla:

<table><thead><tr><th width="175">Name</th><th width="322">Description</th><th>Pinecone Metadata Filter</th></tr></thead><tbody><tr><td>search_tsla</td><td>Use this function to answer user questions about Tesla Inc (TSLA). It contains a SEC Form 10K filing describing the financials of Tesla Inc (TSLA) for the 2022 time period.</td><td><code>`{source: tesla}`</code></td></tr></tbody></table>

:::info
It is important to specify a clear and concise description. This allows LLM to better decide when to use which tool
:::

Your flow should looks like below:

<figure><img src="/.gitbook/assets/image (154).png" alt="" /><figcaption></figcaption></figure>

4. Now, we need to create a general instruction to Tool Agent. Click **Additional Parameters** of the node, and specify the **System Message**. For example:

```
You are an expert financial analyst that always answers questions with the most relevant information using the tools at your disposal.
These tools have information regarding companies that the user has expressed interest in.
Here are some guidelines that you must follow:
* For financial questions, you must use the tools to find the answer and then write a response.
* Even if it seems like your tools won't be able to answer the question, you must still use them to find the most relevant information and insights. Not using them will appear as if you are not doing your job.
* You may assume that the users financial questions are related to the documents they've selected.
* For any user message that isn't related to financial analysis, respectfully decline to respond and suggest that the user ask a relevant question.
* If your tools are unable to find an answer, you should say that you haven't found an answer but still relay any useful information the tools found.
* Dont ask clarifying questions, just return answer.

The tools at your disposal have access to the following SEC documents that the user has selected to discuss with you:
- Apple Inc (APPL) FORM 10K 2022
- Tesla Inc (TSLA) FORM 10K 2022

The current date is: 2024-01-28
```

5. Save the Chatflow, and start asking question!

<figure><img src="/.gitbook/assets/image (110).png" alt="" /><figcaption></figcaption></figure>

<div align="left">

<figure><img src="/.gitbook/assets/Untitled (9).png" alt="" width="375" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/Untitled (10).png" alt="" width="375" /><figcaption></figcaption></figure>

</div>

6. Follow up with Tesla:

<figure><img src="/.gitbook/assets/image (111).png" alt="" /><figcaption></figcaption></figure>

7. We are now able to ask questions about any documents that we've previously upserted to vector database without "hard-coding" the metadata filtering by using tools + agent.

## XML Agent

For some LLMs, function callings capabilities are not supported. In this case, we can use XML Agent to prompt the LLM in a more structured format/syntax, with the goal of using the provided tools.

It has the underlying prompt:

```xml
You are a helpful assistant. Help the user answer any questions.

You have access to the following tools:

{tools}

In order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>
For example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:

<tool>search</tool><tool_input>weather in SF</tool_input>
<observation>64 degrees</observation>

When you are done, respond with a final answer between <final_answer></final_answer>. For example:

<final_answer>The weather in SF is 64 degrees</final_answer>

Begin!

Previous Conversation:
{chat_history}

Question: {input}
{agent_scratchpad}
```

<figure><img src="/.gitbook/assets/image (20).png" alt="" /><figcaption></figcaption></figure>

## Conclusion

We've covered using Conversational Retrieval QA Chain and its limitation when querying multiple documents. And we were able to overcome the issue by using OpenAI Function Agent/XML Agent + Tools. You can find the templates below:

<a href="/.gitbook/assets/ToolAgent Chatflow.json" download>Download ToolAgent Chatflow.json</a>

<a href="/.gitbook/assets/XMLAgent Chatflow.json" download>Download XMLAgent Chatflow.json</a>
</file>

<file path="developers/use-cases/README.md">
---
description: Learn to build your own Flowise solutions through practical examples
---

# Use Cases

---

This section provides a collection of practical examples to demonstrate how AnswerAgentAI can be used to build a variety of solutions.

Each use case will guide you through the process of designing, building, and deploying real-world applications using AnswerAgentAI.

## Guides

-   [Calling Children Flows](calling-children-flows.md)
-   [Calling Webhook](webhook-tool.md)
-   [Interacting with API](interacting-with-api.md)
-   [Multiple Documents QnA](multiple-documents-qna.md)
-   [SQL QnA](sql-qna.md)
-   [Upserting Data](upserting-data.md)
-   [Web Scrape QnA](web-scrape-qna.md)
</file>

<file path="developers/use-cases/sql-qna.md">
---
description: Learn how to query structured data
---

# SQL QnA

---

Unlike previous examples like [Web Scrape QnA](web-scrape-qna.md) and [Multiple Documents QnA](multiple-documents-qna.md), querying structured data does not require a vector database. At the high-level, this can be achieved with following steps:

1. Providing the LLM:
    - overview of the SQL database schema
    - example rows data
2. Return a SQL query with few shot prompting
3. Validate the SQL query using an [If Else](../../sidekick-studio/chatflows/utilities/if-else.md) node
4. Create a custom function to execute the SQL query, and get the response
5. Return a natural response from the executed SQL response

<figure><img src="/.gitbook/assets/image (113).png" alt="" /><figcaption></figcaption></figure>

In this example, we are going to create a QnA chatbot that can interact with a SQL database stored in SingleStore

<figure><img src="/.gitbook/assets/image (116).png" alt="" /><figcaption></figcaption></figure>

## TL;DR

You can find the chatflow template:

<a href="/.gitbook/assets/SQL Chatflow.json" download>Download /.gitbook/assets/SQL Chatflow.json</a>

## 1. SQL Database Schema + Example Rows

Use a Custom JS Function node to connect to SingleStore, retrieve database schema and top 3 rows.

From the [research paper](https://arxiv.org/abs/2204.00498), it is recommended to generate a prompt with following example format:

```
CREATE TABLE samples (firstName varchar NOT NULL, lastName varchar)
SELECT * FROM samples LIMIT 3
firstName lastName
Stephen Tyler
Jack McGinnis
Steven Repici
```

<figure><img src="/.gitbook/assets/image (114).png" alt="" /><figcaption></figcaption></figure>

<details>

<summary>Full Javascript Code</summary>

```javascript
const HOST = 'singlestore-host.com'
const USER = 'admin'
const PASSWORD = 'mypassword'
const DATABASE = 'mydb'
const TABLE = 'samples'
const mysql = require('mysql2/promise')

let sqlSchemaPrompt

function getSQLPrompt() {
    return new Promise(async (resolve, reject) => {
        try {
            const singleStoreConnection = mysql.createPool({
                host: HOST,
                user: USER,
                password: PASSWORD,
                database: DATABASE
            })

            // Get schema info
            const [schemaInfo] = await singleStoreConnection.execute(
                `SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name = "${TABLE}"`
            )

            const createColumns = []
            const columnNames = []

            for (const schemaData of schemaInfo) {
                columnNames.push(`${schemaData['COLUMN_NAME']}`)
                createColumns.push(
                    `${schemaData['COLUMN_NAME']} ${schemaData['COLUMN_TYPE']} ${schemaData['IS_NULLABLE'] === 'NO' ? 'NOT NULL' : ''}`
                )
            }

            const sqlCreateTableQuery = `CREATE TABLE samples (${createColumns.join(', ')})`
            const sqlSelectTableQuery = `SELECT * FROM samples LIMIT 3`

            // Get first 3 rows
            const [rows] = await singleStoreConnection.execute(sqlSelectTableQuery)

            const allValues = []
            for (const row of rows) {
                const rowValues = []
                for (const colName in row) {
                    rowValues.push(row[colName])
                }
                allValues.push(rowValues.join(' '))
            }

            sqlSchemaPrompt = sqlCreateTableQuery + '\n' + sqlSelectTableQuery + '\n' + columnNames.join(' ') + '\n' + allValues.join('\n')

            resolve()
        } catch (e) {
            console.error(e)
            return reject(e)
        }
    })
}

async function main() {
    await getSQLPrompt()
}

await main()

return sqlSchemaPrompt
```

</details>

Once finished, click Execute:

<figure><img src="/.gitbook/assets/image (117).png" alt="" /><figcaption></figcaption></figure>

We can now see the correct format has been generated. Next step is to bring this into Prompt Template.

## 2. Return a SQL query with few shot prompting

Create a new Chat Model + Prompt Template + LLMChain

<figure><img src="/.gitbook/assets/image (118).png" alt="" /><figcaption></figcaption></figure>

Specify the following prompt in the Prompt Template:

```
Based on the provided SQL table schema and question below, return a SQL SELECT ALL query that would answer the user's question. For example: SELECT * FROM table WHERE id = '1'.
------------
SCHEMA: \{schema\}
------------
QUESTION: \{question\}
------------
SQL QUERY:
```

Since we are using 2 variables: \{schema\} and \{question\}, specify their values in **Format Prompt Values**:

<figure><img src="/.gitbook/assets/image (122).png" alt="" width="563" /><figcaption></figcaption></figure>

:::info
You can provide more examples to the prompt (i.e few-shot prompting) to let the LLM learns better. Or take reference from [dialect-specific prompting](https://js.langchain.com/docs/use_cases/sql/prompting#dialect-specific-prompting)
:::

## 3. Validate the SQL query using [If Else](../../sidekick-studio/chatflows/utilities/if-else.md) node

Sometimes the SQL query is invalid, and we do not want to waste resources the execute an invalid SQL query. For example, if a user is asking a general question that is irrelevant to the SQL database. We can use an `If Else` node to route to different path.

For instance, we can perform a basic check to see if SELECT and WHERE are included in the SQL query given by the LLM.

<!-- {% tabs %}
{% tab title="If Function" %}
```javascript
const sqlQuery = $sqlQuery.trim();

if (sqlQuery.includes("SELECT") && sqlQuery.includes("WHERE")) {
    return sqlQuery;
}
```
{% endtab %}

{% tab title="Else Function" %}
```javascript
return $sqlQuery;
```
{% endtab %}
{% endtabs %} -->

<figure><img src="/.gitbook/assets/image (119).png" alt="" width="327" /><figcaption></figcaption></figure>

In the Else Function, we will route to a Prompt Template + LLMChain that basically tells LLM that it is unable to answer user query:

<figure><img src="/.gitbook/assets/image (120).png" alt="" /><figcaption></figcaption></figure>

## 4. Custom function to execute SQL query, and get the response

If it is a valid SQL query, we need to execute the query. Connect the _**True**_ output from **If Else** node to a **Custom JS Function** node:

<figure><img src="/.gitbook/assets/image (123).png" alt="" width="563" /><figcaption></figcaption></figure>

<details>

<summary>Full Javascript Code</summary>

```javascript
const HOST = 'singlestore-host.com'
const USER = 'admin'
const PASSWORD = 'mypassword'
const DATABASE = 'mydb'
const TABLE = 'samples'
const mysql = require('mysql2/promise')

let result

function getSQLResult() {
    return new Promise(async (resolve, reject) => {
        try {
            const singleStoreConnection = mysql.createPool({
                host: HOST,
                user: USER,
                password: PASSWORD,
                database: DATABASE
            })

            const [rows] = await singleStoreConnection.execute($sqlQuery)

            result = JSON.stringify(rows)

            resolve()
        } catch (e) {
            console.error(e)
            return reject(e)
        }
    })
}

async function main() {
    await getSQLResult()
}

await main()

return result
```

</details>

## 5. Return a natural response from the executed SQL response

Create a new Chat Model + Prompt Template + LLMChain

<figure><img src="/.gitbook/assets/image (124).png" alt="" /><figcaption></figcaption></figure>

Write the following prompt in the Prompt Template:

```
Based on the question, and SQL response, write a natural language response, be details as possible:
------------
QUESTION: {question}
------------
SQL RESPONSE: {sqlResponse}
------------
NATURAL LANGUAGE RESPONSE:
```

Specify the variables in **Format Prompt Values**:

<figure><img src="/.gitbook/assets/image (125).png" alt="" width="563" /><figcaption></figcaption></figure>

Voila! Your SQL chatbot is now ready for testing!

## Query

First, let's ask something related to the database.

<figure><img src="/.gitbook/assets/image (128).png" alt="" width="434" /><figcaption></figcaption></figure>

Looking at the logs, we can see the first LLMChain is able to give us a SQL query:

**Input:**

```
Based on the provided SQL table schema and question below, return a SQL SELECT ALL query that would answer the user's question. For example: SELECT * FROM table WHERE id = '1'.\n------------\nSCHEMA: CREATE TABLE samples (id bigint(20) NOT NULL, firstName varchar(300) NOT NULL, lastName varchar(300) NOT NULL, userAddress varchar(300) NOT NULL, userState varchar(300) NOT NULL, userCode varchar(300) NOT NULL, userPostal varchar(300) NOT NULL, createdate timestamp(6) NOT NULL)\nSELECT * FROM samples LIMIT 3\nid firstName lastName userAddress userState userCode userPostal createdate\n1125899906842627 Steven Repici 14 Kingston St. Oregon NJ 5578 Thu Dec 14 2023 13:06:17 GMT+0800 (Singapore Standard Time)\n1125899906842625 John Doe 120 jefferson st. Riverside NJ 8075 Thu Dec 14 2023 13:04:32 GMT+0800 (Singapore Standard Time)\n1125899906842629 Bert Jet 9th, at Terrace plc Desert City CO 8576 Thu Dec 14 2023 13:07:11 GMT+0800 (Singapore Standard Time)\n------------\nQUESTION: what is the address of John\n------------\nSQL QUERY:
```

**Output**

```sql
SELECT userAddress FROM samples WHERE firstName = 'John'
```

After executing the SQL query, the result is passed to the 2nd LLMChain:

**Input**

```
Based on the question, and SQL response, write a natural language response, be details as possible:\n------------\nQUESTION: what is the address of John\n------------\nSQL RESPONSE: [{\"userAddress\":\"120 jefferson st.\"}]\n------------\nNATURAL LANGUAGE RESPONSE:
```

**Output**

```
The address of John is 120 Jefferson St.
```

Now, we if ask something that is irrelevant to the SQL database, the Else route is taken.

<figure><img src="/.gitbook/assets/image (132).png" alt="" width="428" /><figcaption></figcaption></figure>

For first LLMChain, a SQL query is generated as below:

```sql
SELECT * FROM samples LIMIT 3
```

However, it fails the `If Else` check because it doesn't contains both `SELECT` and `WHERE`, hence entering the Else route that has a prompt that says:

```
Politely say "I'm not able to answer query"
```

And the final output is:

```
I apologize, but I'm not able to answer your query at the moment.
```

## Conclusion

In this example, we have successfully created a SQL chatbot that can interact with your database, and is also able to handle questions that are irrelevant to database. Further improvement includes adding memory to provide conversation history.

You can find the chatflow below:

<a href="/.gitbook/assets/SQL Chatflow.json" download>Download /.gitbook/assets/SQL Chatflow.json</a>
</file>

<file path="developers/use-cases/upserting-data.md">
---
description: Learn how to upsert data to Vector Stores with AnswerAgentAI
---

# Upserting Data

---

There are two fundamental ways to upsert your data into a [Vector Store](../../sidekick-studio/chatflows/vector-stores/) using AnswerAgentAI, either via [API calls](http://localhost:4242/docs/api/vector-upsert/vector-upsert) or by using a set of dedicated nodes we have ready for this purpose.

In this guide, even though it is **highly recommended** that you prepare your data using the [Document Stores](../../sidekick-studio/documents) before upserting to a Vector Store, we will go through the entire process by using the specific nodes required for this end, outlining the steps, advantages of this approach, and optimization strategies for efficient data handling.

## Understanding the upserting process

The first thing we need to understand is that the upserting data process to a [Vector Store](../../sidekick-studio/chatflows/vector-stores/) is a fundamental piece for the formation of a [Retrieval Augmented Generation (RAG)](multiple-documents-qna.md) system. However, once this process is finished, the RAG can be executed independently.

In other words, in AnswerAgentAI you can upsert data without a full RAG setup, and you can run your RAG without the specific nodes used in the upsert process, meaning that although a well-populated vector store is crucial for RAG to function, the actual retrieval and generation processes don't require continuous upserting.

<figure><img src="/.gitbook/assets/ud_01.png" alt="" /><figcaption><p>Upsert vs. RAG</p></figcaption></figure>

## Setup

Let's say we have a long dataset in PDF format that we need to upsert to our [Upstash Vector Store](../../sidekick-studio/chatflows/vector-stores/upstash-vector.md) so we could instruct an LLM to retrieve specific information from that document.

In order to do that, and for illustrating this tutorial, we would need to create an **upserting flow** with 5 different nodes:

<figure><img src="/.gitbook/assets/UD_02.png" alt="" /><figcaption><p>Upserting Flow</p></figcaption></figure>

## 1. Document Loader

The first step is to **upload our PDF data into the AnswerAgentAI instance** using a [Document Loader node](../../sidekick-studio/chatflows/document-loaders/). Document Loaders are specialized nodes that handle the ingestion of various document formats, including **PDFs**, **TXT**, **CSV**, Notion pages, and more.

It is important to mention that every Document Loader comes with two important **additional parameters** that allow us to add and omit metadata from our dataset at will.

<figure><img src="/.gitbook/assets/UD_03.png" alt="" width="375" /><figcaption><p>Additional Parameters</p></figcaption></figure>

:::info
**Tip**: The add/omit metadata parameters, although they are optional, are very useful for targeting our dataset once it is upserted in a Vector Store or for removing unnecessary metadata from it.
:::

## 2. Text Splitter

Once we have uploaded our PDF or datset, we need to **split it into smaller pieces, documents, or chunks**. This is a crucial preprocessing step for 2 main reasons:

-   **Retrieval speed and relevance:** Storing and querying large documents as single entities in a vector database can lead to slower retrieval times and potentially less relevant results. Splitting the document into smaller chunks allows for more targeted retrieval. By querying against smaller, more focused units of information, we can achieve faster response times and improve the precision of the retrieved results.
-   **Cost-effective:** Since we only retrieve relevant chunks rather than the entire document, the number of tokens processed by the LLM is significantly reduced. This targeted retrieval approach directly translates to lower usage costs for our LLM, as billing is typically based on token consumption. By minimizing the amount of irrelevant information sent to the LLM, we also optimize for cost.

### Nodes

In AnswerAgentAI, this splitting process is accomplished using the [Text Splitter nodes](../../sidekick-studio/chatflows/text-splitters/). Those nodes provide a range of text segmentation strategies, including:

-   **Character Text Splitting:** Dividing the text into chunks of a fixed number of characters. This method is straightforward but may split words or phrases across chunks, potentially disrupting context.
-   **Token Text Splitting:** Segmenting the text based on word boundaries or tokenization schemes specific to the chosen embedding model. This approach often leads to more semantically coherent chunks, as it preserves word boundaries and considers the underlying linguistic structure of the text.
-   **Recursive Character Text Splitting:** This strategy aims to divide text into chunks that maintain semantic coherence while staying within a specified size limit. It's particularly well-suited for hierarchical documents with nested sections or headings. Instead of blindly splitting at the character limit, it recursively analyzes the text to find logical breakpoints, such as sentence endings or section breaks. This approach ensures that each chunk represents a meaningful unit of information, even if it slightly exceeds the target size.
-   **Markdown Text Splitter:** Designed specifically for markdown-formatted documents, this splitter logically segments the text based on markdown headings and structural elements, creating chunks that correspond to logical sections within the document.
-   **Code Text Splitter:** Tailored for splitting code files, this strategy considers code structure, function definitions, and other programming language-specific elements to create meaningful chunks that are suitable for tasks like code search and documentation.
-   **HTML-to-Markdown Text Splitter:** This specialized splitter first converts HTML content to Markdown and then applies the Markdown Text Splitter, allowing for structured segmentation of web pages and other HTML documents.

The Text Splitter nodes provide granular control over text segmentation, allowing for customization of parameters such as:

-   **Chunk Size:** The desired maximum size of each chunk, usually defined in characters or tokens.
-   **Chunk Overlap:** The number of characters or tokens to overlap between consecutive chunks, useful for maintaining contextual flow across chunks.

:::info
**Tip:** Note that Chunk Size and Chunk Overlap values are not additive. Selecting `chunk_size=1200` and `chunk_overlap=400` does not result in a total chunk size of 1600. The overlap value determines the number of tokens from the preceding chunk included in the current chunk to maintain context. It does not increase the overall chunk size.
:::

### Undertanding Chunk Overlap

In the context of vector-based retrieval and LLM querying, chunk overlap plays an **important role in maintaining contextual continuity** and **improving response accuracy**, especially when dealing with limited retrieval depth or **top K**, which is the parameter that determines the maximum number of most similar chunks that are retrieved from the [Vector Store](../../sidekick-studio/chatflows/vector-stores/) in response to a query.

During query processing, the LLM executes a similarity search against the Vector Store to retrieve the most semantically relevant chunks to the given query. If the retrieval depth, represented by the top K parameter, is set to a small value, 4 for default, the LLM initially uses information only from these 4 chunks to generate its response.

This scenario presents us with a problem, since relying solely on a limited number of chunks without overlap can lead to incomplete or inaccurate answers, particularly when dealing with queries that require information spanning multiple chunks.

Chunk overlap helps with this issue by ensuring that a portion of the textual context is shared across consecutive chunks, **increasing the likelihood that all relevant information for a given query is contained within the retrieved chunks**.

In other words, this overlap serves as a bridge between chunks, enabling the LLM to access a wider contextual window even when limited to a small set of retrieved chunks (top K). If a query relates to a concept or piece of information that extends beyond a single chunk, the overlapping regions increase the likelihood of capturing all the necessary context.

Therefore, by introducing chunk overlap during the text splitting phase, we enhance the LLM's ability to:

1. **Preserve contextual continuity:** Overlapping chunks provide a smoother transition of information between consecutive segments, allowing the model to maintain a more coherent understanding of the text.
2. **Improve retrieval accuracy:** By increasing the probability of capturing all relevant information within the target top K retrieved chunks, overlap contributes to more accurate and contextually appropriate responses.

### Accuracy vs. Cost

So, to further optimize the trade-off between retrieval accuracy and cost, two primary strategies can be used:

1. **Increase/Decrease Chunk Overlap:** Adjusting the overlap percentage during text splitting allows for fine-grained control over the amount of shared context between chunks. Higher overlap percentages generally lead to improved context preservation but may also increase costs since you would need to use more chunks to encompass the entire document. Conversely, lower overlap percentages can reduce costs but risk losing key contextual information between chunks, potentially leading to less accurate or incomplete answers from the LLM.
2. **Increase/Decrease Top K:** Raising the default top K value (4) expands the number of chunks considered for response generation. While this can improve accuracy, it also increases cost.

:::info
**Tip:** The choice of optimal **overlap** and **top K** values depends on factors such as document complexity, embedding model characteristics, and the desired balance between accuracy and cost. Experimentation with these values is important for finding the ideal configuration for a specific need.
:::

## 3. Embedding

We have now uploaded our dataset and configured how our data is going to be split before it gets upserted to our [Vector Store](../../sidekick-studio/chatflows/vector-stores/). At this point, [the embedding nodes](../../sidekick-studio/chatflows/embeddings/) come into play, **converting all those chunks into a "language" that an LLM can easily understand**.

In this current context, embedding is the process of converting text into a numerical representation that captures its meaning. This numerical representation, also called the embedding vector, is a multi-dimensional array of numbers, where each dimension represents a specific aspect of the text's meaning.

These vectors allow LLMs to compare and search for similar pieces of text within the vector store by measuring the distance or similarity between them in this multi-dimensional space.

### Understanding Embeddings/Vector Store dimensions

The number of dimensions in a Vector Store index is determined by the embedding model used when we upsert our data, and vice versa. Each dimension represents a specific feature or concept within the data. For example, a **dimension** might **represent a particular topic, sentiment, or other aspect of the text**.

The more dimensions we use to embed our data, the greater the potential for capturing nuanced meaning from our text. However, this increase comes at the cost of higher computational requirements per query.

In general, a larger number of dimensions needs more resources to store, process, and compare the resulting embedding vectors. Therefore, embeddings models like the Google `embedding-001`, which uses 768 dimensions, are, in theory, cheaper than others like the OpenAI `text-embedding-3-large`, with 3072 dimensions.

It's important to note that the **relationship between dimensions and meaning capture isn't strictly linear**; there's a point of diminishing returns where adding more dimensions provides negligible benefit for the added unnecessary cost.

:::info
**Tip:** To ensure compatibility between an embedding model and a Vector Store index, dimensional alignment is essential. Both **the model and the index must utilize the same number of dimensions for vector representation**. Dimensionality mismatch will result in upsertion errors, as the Vector Store is designed to handle vectors of a specific size determined by the chosen embedding model.
:::

## 4. Vector Store

The [Vector Store node](../../sidekick-studio/chatflows/vector-stores/) is the **end node of our upserting flow**. It acts as the bridge between our AnswerAgentAI instance and our vector database, enabling us to send the generated embeddings, along with any associated metadata, to our target Vector Store index for persistent storage and subsequent retrieval.

It is in this node where we can set parameters like "**top K**", which, as we said previously, is the parameter that determines the maximum number of most similar chunks that are retrieved from the Vector Store in response to a query.

<figure><img src="/.gitbook/assets/UD_04.png" alt="" width="375" /><figcaption></figcaption></figure>

:::info
**Tip:** A lower top K value will yield fewer but potentially more relevant results, while a higher value will return a broader range of results, potentially capturing more information.
:::

## 5. Record Manager

The [Record Manager node](../../sidekick-studio/documents/record-manager.md) is an optional but incredibly useful addition to our upserting flow. It allows us to maintain records of all the chunks that have been upserted to our Vector Store, enabling us to efficiently add or delete chunks as needed.

For a more in-depth guide, we refer you to [this guide](../../sidekick-studio/documents/record-manager.md).

<figure><img src="/.gitbook/assets/UD_05.png" alt="" width="375" /><figcaption></figcaption></figure>

## 6. Full Overview

Finally, let's examine each stage, from initial document loading to the final vector representation, highlighting the key components and their roles in the upserting process.

<figure><img src="/.gitbook/assets/UD_06.png" alt="" /><figcaption></figcaption></figure>

1. **Document Ingestion**:
    - We begin by feeding our raw data into AnswerAgentAI using the appropriate **Document Loader node** for your data format.
2. **Strategic Splitting**
    - Next, the **Text Splitter node** divides our document into smaller, more manageable chunks. This is crucial for efficient retrieval and cost control.
    - We have flexibility in how this splitting happens by selecting the appropriate text splitter node and, importantly, by fine-tuning chunk size and chunk overlap to balance context preservation with efficiency.
3. **Meaningful Embeddings**
    - Now, just before our data is going to be recorded in the Vector Store, the **Embedding node** steps in. It transforms each text chunk and its meaning into a numerical representation that our LLM can understand.
4. **Vector Store Index**
    - Finally, the **Vector Store node** acts as the bridge between AnswerAgentAI and our database. It sends our embeddings, along with any associated metadata, to the designated Vector Store index.
    - Here, in this node, we can control the retrieval behavior by setting the **top K** parameter, which influences how many chunks are considered when answering a query.
5. **Data Ready**
    - Once upserted, our data is now represented as vectors within the Vector Store, ready for similarity search and retrieval.
6. **Record Keeping (Optional)**
    - For enhanced control and management data, the **Record Manager** node keeps track of all upserted chunks. This facilitates easy updates or removals as your data or needs evolve.

In essence, the upserting process transforms our raw data into an LLM-ready format, optimized for fast and cost-effective retrieval.
</file>

<file path="developers/use-cases/web-scrape-qna.md">
---
description: Learn how to scrape, upsert, and query a website
---

# Web Scrape QnA

---

Let's say you have a website (could be a store, an ecommerce site, a blog), and you want to scrap all the relative links of that website and have LLM answer any question on your website. In this tutorial, we are going to go through how to achieve that.

You can find the example flow called - **WebPage QnA** from the marketplace templates.

## Setup

We are going to use **Cheerio Web Scraper** node to scrape links from a given URL and the **HtmlToMarkdown Text Splitter** to split the scraped content into smaller pieces.

<figure><img src="/.gitbook/assets/image (86).png" alt="" /><figcaption></figcaption></figure>

If you do not specify anything, by default only the given URL page will be scraped. If you want to crawl the rest of relative links, click **Additional Parameters** of Cheerio Web Scraper.

## 1. Crawl Multiple Pages

1. Select `Web Crawl` or `Scrape XML Sitemap` in **Get Relative Links Method**.
2. Input `0` in **Get Relative Links Limit** to retrieve all links available from the provided URL.

<figure><img src="/.gitbook/assets/image (87).png" alt="" width="563" /><figcaption></figcaption></figure>

### Manage Links (Optional)

1. Input desired URL to be crawled.
2. Click **Fetch Links** to retrieve links based on the inputs of the **Get Relative Links Method** and **Get Relative Links Limit** in **Additional Parameters**.
3. In **Crawled Links** section, remove unwanted links by clicking **Red Trash Bin Icon**.
4. Lastly, click **Save**.

<figure><img src="/.gitbook/assets/image (88).png" alt="" width="563" /><figcaption></figcaption></figure>

## 2. Upsert

1. On the top right corner, you will notice a green button:

<figure><img src="/.gitbook/assets/Untitled (2).png" alt="" /><figcaption></figcaption></figure>

2. A dialog will be shown that allow users to upsert data to Pinecone:

<figure><img src="/.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2).png" alt="" /><figcaption></figcaption></figure>

**Note:** Under the hood, following actions will be executed:

-   Scraped all HTML data using Cheerio Web Scraper
-   Convert all scraped data from HTML to Markdown, then split it
-   Splitted data will be looped over, and converted to vector embeddings using OpenAI Embeddings
-   Vector embeddings will be upserted to Pinecone

3. On the [Pinecone console](https://app.pinecone.io) you will be able to see the new vectors that were added.

<figure><img src="/.gitbook/assets/web-scrape-pinecone.png" alt="" /><figcaption></figcaption></figure>

## 3. Query

Querying is relatively straight-forward. After you have verified that data is upserted to vector database, you can start asking question in the chat:

<figure><img src="/.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (2).png" alt="" /><figcaption></figcaption></figure>

In the Additional Parameters of Conversational Retrieval QA Chain, you can specify 2 prompts:

-   **Rephrase Prompt:** Used to rephrase the question given the past conversation history
-   **Response Prompt:** Using the rephrased question, retrieve the context from vector database, and return a final response

<figure><img src="/.gitbook/assets/image (91).png" alt="" /><figcaption></figcaption></figure>

:::info
It is recommended to specify a detailed response prompt message. For example, you can specify the name of AI, the language to answer, the response when answer its not found (to prevent hallucination).
:::

You can also turn on the Return Source Documents option to return a list of document chunks where the AI's response is coming from.

<figure><img src="/.gitbook/assets/Untitled (1) (1) (1) (1).png" alt="" width="563" /><figcaption></figcaption></figure>

## Additional Web Scraping

Apart from Cheerio Web Scraper, there are other nodes that can perform web scraping as well:

-   **Puppeteer:** Puppeteer is a Node.js library that provides a high-level API for controlling headless Chrome or Chromium. You can use Puppeteer to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.
-   **Playwright:** Playwright is a Node.js library that provides a high-level API for controlling multiple browser engines, including Chromium, Firefox, and WebKit. You can use Playwright to automate web page interactions, including extracting data from dynamic web pages that require JavaScript to render.
-   **Apify:** [Apify](https://apify.com/) is a cloud platform for web scraping and data extraction, which provides an [ecosystem](https://apify.com/store) of more than a thousand ready-made apps called _Actors_ for various web scraping, crawling, and data extraction use cases.

<figure><img src="/.gitbook/assets/image (92).png" alt="" /><figcaption></figcaption></figure>

:::info
The same logic can be applied to any document use cases, not just limited to web scraping!
:::

If you have any suggestion on how to improve the performance, we'd love your [contribution](../../community/)!
</file>

<file path="developers/use-cases/webhook-tool.md">
---
description: Learn how to call a webhook on Make
---

# Calling Webhook

---

In this use case tutorial, we are going to create a custom tool that will be able to call a webhook endpoint, and pass in the necessary parameters into the webhook body. We'll be using [Make.com](https://www.make.com/en) to create the webhook workflow.

## Make

Head over to Make.com, after registering an account, create a workflow that has a Webhook module and Discord module, which looks like below:

<figure><img src="/.gitbook/assets/screely-1691756705932.png" alt="" /><figcaption></figcaption></figure>

From the Webhook module, you should be able to see a webhook URL:

<figure><img src="/.gitbook/assets/image (46).png" alt="" width="563" /><figcaption></figcaption></figure>

From the Discord module, we are passing the `message` body from the Webhook as the message to send to Discord channel:

<figure><img src="/.gitbook/assets/image (47).png" alt="" width="563" /><figcaption></figcaption></figure>

To test it out, you can click Run once at the bottom left corner, and send a POST request with a JSON body

```json
{
    "message": "Hello Discord!"
}
```

<figure><img src="/.gitbook/assets/image (48).png" alt="" width="563" /><figcaption></figcaption></figure>

You'll be able to see a Discord message sent to the channel:

<figure><img src="/.gitbook/assets/image (49).png" alt="" width="249" /><figcaption></figcaption></figure>

Perfect! We have successfully configured a workflow that is able to pass a message and send to Discord channel [](https://emojiterra.com/party-popper/)[](https://emojiterra.com/party-popper/)

## AnswerAgentAI

In AnswerAgentAI, we are going to create a custom tool that is able to call the Webhook POST request, with the message body.

From the dashboard, click **Tools**, then click **Create**

<figure><img src="/.gitbook/assets/screely-1691758397783.png" alt="" /><figcaption></figcaption></figure>

We can then fill in the following fields (feel free to change this according to your needs):

-   **Tool Name**: make_webhook (must be in snake_case)
-   **Tool Description**: Useful when you need to send message to Discord
-   **Tool Icon Src**: [https://github.com/FlowiseAI/Flowise/assets/26460777/517fdab2-8a6e-4781-b3c8-fb92cc78aa0b](https://github.com/FlowiseAI/Flowise/assets/26460777/517fdab2-8a6e-4781-b3c8-fb92cc78aa0b)
-   **Input Schema**:

<figure><img src="/.gitbook/assets/image (167).png" alt="" /><figcaption></figcaption></figure>

-   **JavaScript Function**:

```javascript
const fetch = require('node-fetch')
const webhookUrl = 'https://hook.eu1.make.com/abcdef'
const body = {
    message: $message
}
const options = {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json'
    },
    body: JSON.stringify(body)
}
try {
    const response = await fetch(webhookUrl, options)
    const text = await response.text()
    return text
} catch (error) {
    console.error(error)
    return ''
}
```

Click **Add** to save the custom tool, and you should be able to see it now:

<figure><img src="/.gitbook/assets/image (51).png" alt="" width="279" /><figcaption></figcaption></figure>

Now, create a new canvas with following nodes:

-   **Buffer Memory**
-   **ChatOpenAI**
-   **Custom Tool** (select the make_webhook tool we just created)
-   **OpenAI Function Agent**

It should looks like below after connecting them up:

<figure><img src="/.gitbook/assets/screely-1691758990676.png" alt="" /><figcaption></figcaption></figure>

Save the chatflow, and start testing it!

For example, we can ask question like _"how to cook an egg"_

<figure><img src="/.gitbook/assets/image (52).png" alt="" width="563" /><figcaption></figcaption></figure>

Then ask the agent to send all of these to Discord:

<figure><img src="/.gitbook/assets/image (53).png" alt="" width="563" /><figcaption></figcaption></figure>

Go to the Discord channel, and you will be able to see the message:

<figure><img src="/.gitbook/assets/image (54).png" alt="" /><figcaption></figcaption></figure>

That's it! OpenAI Function Agent will be able to automatically figure out what to pass as the message and send it over to Discord. This is just a quick example of how to trigger a webhook workflow with dynamic body. The same idea can be applied to workflow that has a webhook and Gmail, GoogleSheets etc.

You can read more on how to pass chat information like `sessionId`, `flowid` and `variables` to custom tool - [#additional](../../sidekick-studio/chatflows/tools/custom-tool.md)

## Tutorials

-   Watch a step-by-step instruction video on using Webhooks with AnswerAgentAI custom tools.

<iframe src="https://www.youtube.com/embed/_K9xJqEgnrU"></iframe>

-   Watch how to connect AnswerAgentAI to Google Sheets using webhooks

<iframe src="https://www.youtube.com/embed/fehXLdRLJFo"></iframe>

-   Watch how to connect AnswerAgentAI to Microsoft Excel using webhooks

<iframe src="https://www.youtube.com/embed/cB2GC8JznJc"></iframe>
</file>

<file path="developers/building-node.md">
---
description: Learn how to build a custom node for AnswerAgentAI
---

# Custom Nodes

## Overview

This guide will walk you through the process of creating a custom node for AnswerAgentAI. We'll be building a simple Calculator tool as an example. Custom nodes can only be deployed to AnswerAgentAI if you self host.

## Prerequisites

-   Git installed on your system
-   AnswerAgentAI repository cloned locally
-   Familiarity with TypeScript and Node.js

## Step 1: Set Up the Project Structure

1. Navigate to the `packages/componentschatflowstools` directory in your AnswerAgentAI project.
2. Create a new folder named `Calculator`.

## Step 2: Create the Base Class

1. Inside the `Calculator` folder, create a new file named `Calculator.ts`.
2. Add the following code to `Calculator.ts`:

```typescript
import { INode } from '../../../src/Interface'
import { getBaseClasses } from '../../../src/utils'

class Calculator_Tools implements INode {
    label: string
    name: string
    version: number
    description: string
    type: string
    icon: string
    category: string
    author: string
    baseClasses: string[]

    constructor() {
        this.label = 'Calculator'
        this.name = 'calculator'
        this.version = 1.0
        this.type = 'Calculator'
        this.icon = 'calculator.svg'
        this.category = 'Tools'
        this.author = 'Your Name'
        this.description = 'Perform calculations on response'
        this.baseClasses = [this.type, ...getBaseClasses(Calculator)]
    }
}

module.exports = { nodeClass: Calculator_Tools }
```

### Understanding the Properties

| Property    | Description                                                                                   |
| ----------- | --------------------------------------------------------------------------------------------- |
| label       | The name of the node that appears on the UI                                                   |
| name        | The name used by code (must be camelCase)                                                     |
| version     | Version of the node                                                                           |
| type        | Usually the same as label, defines which node can be connected to this specific type on UI    |
| icon        | Icon of the node                                                                              |
| category    | Category of the node                                                                          |
| author      | Creator of the node                                                                           |
| description | Node description                                                                              |
| baseClasses | The base classes from the node, used to define which node can be connected to this node on UI |

## Step 3: Define the Core Functionality

1. Create a new file named `core.ts` in the `Calculator` folder.
2. Add the following code to `core.ts`:

```typescript
import { Parser } from 'expr-eval'
import { Tool } from '@langchain/core/tools'

export class Calculator extends Tool {
    name = 'calculator'
    description = `Useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator.`

    async _call(input: string) {
        try {
            return Parser.evaluate(input).toString()
        } catch (error) {
            return "I don't know how to do that."
        }
    }
}
```

This class defines the actual functionality of our Calculator tool.

## Step 4: Finalize the Node Class

1. Update `Calculator.ts` to include the initialization function:

```typescript
import { INode } from '../../../src/Interface'
import { getBaseClasses } from '../../../src/utils'
import { Calculator } from './core'

class Calculator_Tools implements INode {
    // ... (previous code remains the same)

    async init() {
        return new Calculator()
    }
}

module.exports = { nodeClass: Calculator_Tools }
```

The `init` function will be called when the flow is executed, and the `_call` function will be executed when the language model decides to use this tool.

## Step 5: Enable Community Nodes

1. Open the `.env` file in the `packages/server` directory.
2. Add the following line:

```
SHOW_COMMUNITY_NODES=true
```

This allows AnswerAgentAI to recognize and use your custom node.

## Step 6: Build and Run

1. In the root directory of your AnswerAgentAI project, run:

```bash
pnpm build
```

2. Once the build is complete, start AnswerAgentAI:

```bash
pnpm start
```

3. Open the AnswerAgentAI interface in your browser. You should now see your Calculator node available in the Tools category.

<figure><img src="/.gitbook/assets/screenshots/calculator node.png" alt="" /><figcaption><p>Calculator Node &#x26; Drop UI</p></figcaption></figure><!-- TODO: Add a screenshot of the Calculator node in the AnswerAgentAI interface -->

## Troubleshooting

-   If your node doesn't appear, ensure that `SHOW_COMMUNITY_NODES` is set to `true` in your `.env` file.
-   Check the console for any error messages during the build or start process.
-   Verify that your node's files are in the correct directory structure.

## Next Steps

-   Consider adding more complex functionality to your Calculator tool.
-   Explore creating nodes for other categories, such as Memory or Chains.
-   Share your custom node with the AnswerAgentAI community!

Remember to test your node thoroughly to ensure it interacts correctly with other components in AnswerAgentAI flows.
</file>

<file path="developers/contributing.md">
---
title: Contributing to AnswerAgentAI
description: Complete guide to contributing to the AnswerAgentAI Alpha Sprint - PR process, guidelines, and best practices
---

# Contributing to AnswerAgentAI

Welcome to the AnswerAgentAI Alpha Sprint! This guide will help you contribute effectively to our mission of building privacy-first AI tools.

##  Getting Started

### 1. Choose Your Repository

Visit our [organization repositories](https://github.com/orgs/the-answerai/repositories) and pick a project:

-   **[theanswer](https://github.com/the-answerai/theanswer)** - Main platform (21k)
-   **[aai-browser-sidekick](https://github.com/the-answerai/aai-browser-sidekick)** - Chrome extension
-   **[FlowiseChatEmbed](https://github.com/the-answerai/FlowiseChatEmbed)** - Chat embedding (1.5k)
-   **[mcp-inspector](https://github.com/the-answerai/mcp-inspector)** - MCP testing tool
-   **Various MCP servers** - HubSpot, Jira, Confluence integrations

### 2. Find Your First Issue

Look for issues tagged with:

-   `beginner` - Perfect for first-time contributors
-   `good first issue` - Well-documented starter tasks
-   `help wanted` - Community assistance needed

##  PR Process

### Before You Start

1. **Fork the repository** you want to contribute to
2. **Clone your fork** locally
3. **Create a new branch** for your feature/fix
4. **Read the existing code** to understand patterns

### Development Workflow

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/REPO_NAME.git
cd REPO_NAME

# Create feature branch
git checkout -b feature/your-feature-name

# Install dependencies
npm install
# or
pnpm install

# Start development server
npm run dev
```

### Making Changes

1. **Follow existing code style** - We use ESLint and Prettier
2. **Write descriptive commit messages**
3. **Test your changes** thoroughly
4. **Update documentation** if needed

### Commit Message Format

Use conventional commit format:

```
type: brief description

feat: add new chat sidekick for data analysis
fix: resolve extension popup sizing issue
docs: update API documentation for agents
style: format code according to prettier rules
refactor: improve database query performance
test: add unit tests for chat history
```

### Pull Request Guidelines

#### 1. Create a Quality PR

-   **Title**: Clear, descriptive summary
-   **Description**: Explain what you built and why
-   **Screenshots**: For UI changes
-   **Testing**: How you verified it works

#### 2. PR Template

```markdown
## What I Built

Brief description of your contribution

## Problem It Solves

Explain the specific issue or enhancement

## How It Serves the Mission

Connect to our privacy-first, developer-friendly goals

## Testing Done

-   [ ] Local testing completed
-   [ ] Edge cases considered
-   [ ] Documentation updated

## Screenshots/Video

[Include visual proof of your work]

## Additional Notes

Any special considerations or future improvements
```

### 3. Automated Review Process

Our system will automatically:

-    Run tests and linting
-    Provide AI-suggested improvements
-    Check code coverage
-    Security vulnerability scan

### 4. Video Requirement

**Every merged PR requires a 1-3 minute video explaining:**

1. **What you built/fixed**
2. **Problem it solves**
3. **How it serves our mission**
4. **Demo of functionality**

#### Video Guidelines

-   **Length**: 1-3 minutes maximum
-   **Format**: MP4, MOV, or YouTube link
-   **Quality**: Clear audio, visible screen
-   **Content**: Focus on your contribution's impact

##  Contribution Types

### Code Contributions

-   **Bug fixes** - Resolve existing issues
-   **New features** - Enhance platform capabilities
-   **Performance** - Optimize speed and efficiency
-   **Security** - Strengthen privacy and safety

### Non-Code Contributions

-   **Documentation** - Improve guides and examples
-   **Testing** - Write and improve test coverage
-   **Design** - UI/UX improvements
-   **Community** - Help other contributors

##  Recognition System

### Credit Earning Tiers

** Bronze Contributors**

-   First merged PR
-   Community recognition
-   Alpha sprint certificate

** Silver Contributors**

-   5+ merged PRs
-   Technical mentor status
-   Early access to new features

** Gold Contributors**

-   10+ merged PRs
-   Architecture decision input
-   Revenue sharing opportunities

##  Alpha Sprint Priorities

### High-Priority Areas

1. **Chrome Extension Polish**

    - Performance optimizations
    - Bug fixes
    - New tool integrations

2. **Web Application Features**

    - Chat improvements
    - Agent management
    - Studio enhancements

3. **Foundation Work**
    - Desktop app architecture
    - API improvements
    - Documentation

### Special Recognition

**Alpha Sprint Heroes** get:

-   Direct access to core team
-   Influence on roadmap decisions
-   First access to revenue sharing
-   Speaking opportunities at events

##  Development Environment

### Required Tools

-   **Node.js** (18.15.0+ or 20+)
-   **pnpm** (recommended) or npm
-   **Git** with SSH keys configured
-   **VS Code** (recommended) with extensions:
    -   ESLint
    -   Prettier
    -   TypeScript Hero

### Environment Setup

```bash
# Install dependencies
pnpm install

# Copy environment template
cp .env.example .env

# Start development servers
pnpm dev
```

### Testing

```bash
# Run all tests
pnpm test

# Run specific test suite
pnpm test:unit
pnpm test:e2e

# Watch mode for development
pnpm test:watch
```

##  Code Review Process

### What We Look For

1. **Code Quality**

    - Clean, readable code
    - Proper error handling
    - Performance considerations

2. **Alignment with Mission**

    - Privacy-first approach
    - User empowerment
    - Developer-friendly design

3. **Community Impact**
    - Solves real problems
    - Enhances user experience
    - Moves sprint goals forward

### Review Timeline

-   **Initial response**: Within 24 hours
-   **Full review**: Within 48 hours
-   **Merge decision**: Within 72 hours

##  After Your PR is Merged

1. **Record your video** (if not done already)
2. **Share on social media** with #AnswerAgentAIAlphaSprint
3. **Find your next issue** to continue contributing
4. **Help other contributors** in discussions

##  Getting Help

### Community Support

-   **Discord**: [Join our developer community](https://discord.gg/X54ywt8pzj)
-   **GitHub Discussions**: Ask questions in repo discussions
-   **Office Hours**: Weekly developer Q&A sessions

### Direct Support

-   **Stuck on setup?** Tag `@core-team` in discussions
-   **Architecture questions?** Create detailed GitHub issue
-   **Urgent blockers?** Message in #alpha-sprint Discord channel


---

Remember: We're not just building softwarewe're proving that committed developers can create better tools than billion-dollar corporations. Every contribution matters in showing the world what privacy-first, developer-owned AI looks like.

**Ready to start building?** [Browse our repositories](https://github.com/orgs/the-answerai/repositories) and find your first issue!
</file>

<file path="developers/databases.md">
---
description: Learn how to connect your AnswerAgentAI instance to a database
---

# Databases

## Overview

AnswerAgentAI supports four database types:

1. SQLite (Default)
2. MySQL
3. PostgreSQL
4. MariaDB

This guide will walk you through the process of configuring each database type for use with AnswerAgentAI.

## SQLite (Default)

SQLite is the default database for AnswerAgentAI. To configure it, use the following environment variables:

```sh
DATABASE_TYPE=sqlite
DATABASE_PATH=/root/.answerai #your preferred location
```

A `database.sqlite` file will be created and saved in the path specified by `DATABASE_PATH`. If not specified, the default storage path will be in your home directory -> .answerai

**Note:** If no database environment variables are specified, SQLite will be the fallback database choice.

## MySQL

To configure MySQL, use the following environment variables:

```sh
DATABASE_TYPE=mysql
DATABASE_PORT=3306
DATABASE_HOST=localhost
DATABASE_NAME=answerai
DATABASE_USER=user
DATABASE_PASSWORD=123
```

Ensure that you have created the database and granted the necessary permissions to the user before connecting AnswerAgentAI.

## PostgreSQL

For PostgreSQL configuration, use these environment variables:

```sh
DATABASE_TYPE=postgres
DATABASE_PORT=5432
DATABASE_HOST=localhost
DATABASE_NAME=answerai
DATABASE_USER=user
DATABASE_PASSWORD=123
PGSSLMODE=require
```

Make sure you have created the database and granted the appropriate permissions to the user before connecting AnswerAgentAI.

## MariaDB

To configure MariaDB, use the following environment variables:

```bash
DATABASE_TYPE=mariadb
DATABASE_PORT=3306
DATABASE_HOST=localhost
DATABASE_NAME=answerai
DATABASE_USER=answerai
DATABASE_PASSWORD=mypassword
```

As with the other database types, ensure that you have created the database and granted the necessary permissions to the user before connecting AnswerAgentAI.

## Database Synchronization in Production

AnswerAgentAI uses [TypeORM](https://typeorm.io/data-source-options#common-data-source-options) to configure database connections. By default, the `synchronize` option is set to `true`. This means that the database schema will be automatically created on every application launch.

**Warning:** Be cautious with this option in production environments. Enabling `synchronize` in production can lead to **data loss**. This option is primarily useful during development and debugging.

To override the `synchronize` value, set the following environment variable:

```sh
OVERRIDE_DATABASE=false
```

## Best Practices

1. **Development Environment:**

    - Use SQLite for quick setup and testing.
    - Enable `synchronize` to automatically update the schema during development.

2. **Production Environment:**

    - Use a more robust database like MySQL, PostgreSQL, or MariaDB.
    - Disable `synchronize` by setting `OVERRIDE_DATABASE=false`.
    - Implement proper database migration strategies for schema changes.

3. **Security:**

    - Use strong, unique passwords for your database users.
    - Limit database user permissions to only what's necessary for AnswerAgentAI.
    - Use SSL/TLS connections when possible, especially if your database is on a different server.

4. **Performance:**
    - Regularly maintain and optimize your database.
    - Monitor database performance and scale resources as needed.

## Troubleshooting

-   If you encounter connection issues, verify that the database server is running and accessible from your AnswerAgentAI instance.
-   Double-check your environment variables for typos or incorrect values.
-   Ensure that the specified database and user exist and have the necessary permissions.
-   Check AnswerAgentAI logs for any database-related error messages.

## Video Tutorial

For a visual guide on using SQLite and MySQL/MariaDB with AnswerAgentAI, watch this tutorial:

<!-- TODO: Update the video URL to an AnswerAgentAI-specific tutorial when available -->
<iframe src="https://www.youtube.com/embed/R-6uV1Cb8I8"></iframe>

Remember to always backup your data before making any changes to your database configuration or performing migrations.
</file>

<file path="developers/earn-credits.md">
---
title: Earn Credits - Developer Rewards
description: Detailed guide to earning credits through AnswerAgentAI contributions - transparent tracking, reward tiers, and future benefits
---

# Earn Credits Through Contributions

Your work has value, and we're committed to ensuring you're fairly compensated for helping build the future of privacy-first AI. Here's our transparent credit system.

##  Credit System Overview

We track every meaningful contribution transparently on the blockchain. Credits earned during the Alpha Sprint will unlock increasing benefits as AnswerAgentAI grows.

### Why Credits Matter

-   **Transparent ownership** - Your contributions are permanently recorded
-   **Future revenue sharing** - Credits convert to equity participation
-   **Community status** - Higher credits unlock exclusive access
-   **Career opportunities** - Top contributors get first access to positions

##  How to Earn Credits

### Code Contributions

| Contribution Type            | Credit Range    | Description                        |
| ---------------------------- | --------------- | ---------------------------------- |
| **Bug Fixes**                | 10-50 credits   | Based on complexity and impact     |
| **New Features**             | 50-500 credits  | Scaled by scope and innovation     |
| **Performance Improvements** | 25-100 credits  | Measured improvements required     |
| **Security Enhancements**    | 100-300 credits | Critical for privacy-first mission |
| **API Integrations**         | 75-200 credits  | External tool connections          |
| **Testing Coverage**         | 15-75 credits   | Unit, integration, and E2E tests   |

### Non-Code Contributions

| Contribution Type     | Credit Range   | Description                              |
| --------------------- | -------------- | ---------------------------------------- |
| **Documentation**     | 10-100 credits | Guides, tutorials, API docs              |
| **Code Reviews**      | 5-25 credits   | Thoughtful, constructive feedback        |
| **Bug Reports**       | 5-50 credits   | Detailed reports with reproduction steps |
| **Community Support** | 10-25 credits  | Helping other contributors               |
| **Design Assets**     | 25-150 credits | UI/UX improvements                       |
| **Video Tutorials**   | 50-200 credits | Educational content creation             |

### Multipliers & Bonuses

#### Quality Multipliers

-   **First-try merge**: +25% bonus
-   **Zero review comments**: +15% bonus
-   **Includes tests**: +20% bonus
-   **Comprehensive documentation**: +15% bonus

#### Impact Bonuses

-   **Fixes critical issue**: +50-100% bonus
-   **Enables new use case**: +30-75% bonus
-   **Improves accessibility**: +25% bonus
-   **Enhances privacy**: +50% bonus

##  Recognition Tiers

###  Bronze Contributors (1-100 credits)

**Benefits:**

-   Community recognition badge
-   Alpha Sprint certificate
-   Early access to announcements
-   Contributor Discord role

**Requirements:**

-   First meaningful contribution merged
-   Community code of conduct compliance

###  Silver Contributors (101-500 credits)

**Benefits:**

-   Technical mentor status
-   Monthly contributor calls
-   Early feature previews
-   Revenue sharing eligibility (pending)

**Requirements:**

-   Multiple high-quality contributions
-   Code review participation
-   Community support activities

###  Gold Contributors (501-1500 credits)

**Benefits:**

-   Architecture decision input
-   Direct core team access
-   Speaking opportunities
-   Higher revenue share percentage

**Requirements:**

-   Significant feature development
-   Technical leadership demonstrated
-   Mentoring other contributors

###  Platinum Contributors (1501+ credits)

**Benefits:**

-   Co-founder consideration
-   Equity participation discussions
-   Product roadmap influence
-   Board meeting attendance (advisory)

**Requirements:**

-   Major platform contributions
-   Community leadership
-   Strategic vision alignment

##  Transparent Tracking

### Real-Time Dashboard

We're building a public dashboard that shows:

-   **Individual credit balances** (with privacy controls)
-   **Contribution leaderboards** (weekly/monthly/all-time)
-   **Credit distribution analytics**
-   **Upcoming reward distributions**

### Blockchain Recording

All credits are recorded on:

-   **Immutable ledger** - Cannot be changed or lost
-   **Public verification** - Anyone can audit the system
-   **Smart contracts** - Automated distribution when revenue sharing begins

### Credit Verification

Check your credits through:

-   **GitHub integration** - Automatic tracking of merged PRs
-   **Discord bot** - Query your balance anytime
-   **Web dashboard** - Comprehensive contribution history
-   **API endpoint** - Programmatic access to your data

##  Future Value Propositions

### Revenue Sharing Program

_Details coming soon - following legal compliance review_

**Planned Structure:**

-   Credits convert to revenue share percentages
-   Monthly distributions based on platform growth
-   Minimum thresholds for payouts
-   International payment support

### Equity Participation

**For Top Contributors:**

-   Stock option programs
-   Phantom equity for international contributors
-   Vesting schedules aligned with long-term commitment
-   Exit event participation

### Career Opportunities

**Priority Access to:**

-   Full-time developer positions
-   Consulting and contract work
-   Speaking engagements and conferences
-   Advisory roles in the ecosystem

##  Video Requirements for Credit

### Mandatory Video Submission

To receive credits for merged PRs, you must submit a 1-3 minute video explaining:

1. **What you built** - Technical overview
2. **Problem it solves** - User impact
3. **Mission alignment** - How it serves privacy/empowerment
4. **Demo** - Working functionality

### Video Quality Guidelines

**Technical Requirements:**

-   **Resolution**: 720p minimum (1080p preferred)
-   **Audio**: Clear, understandable speech
-   **Length**: 1-3 minutes (strict enforcement)
-   **Format**: MP4, MOV, or unlisted YouTube link

**Content Requirements:**

-   **Screen recording** showing your contribution
-   **Clear explanation** of the technical implementation
-   **Impact statement** connecting to user benefits
-   **Mission connection** explaining privacy/empowerment value

### Video Submission Process

1. **Record your video** using screen recording software
2. **Upload to platform** (YouTube unlisted, or direct file)
3. **Submit with PR** or add to merged PR comments
4. **Await review** - credits processed within 48 hours

##  Credit Audit Process

### Automated Tracking

**GitHub Integration:**

-   Pull request analysis
-   Code complexity scoring
-   Impact measurement
-   Quality assessment

**Manual Review:**

-   Community impact evaluation
-   Subjective quality scoring
-   Bonus criteria assessment
-   Dispute resolution

### Appeals Process

If you believe your credits are incorrect:

1. **Document your case** with specific examples
2. **Submit appeal** through Discord or GitHub issue
3. **Community review** with core team arbitration
4. **Transparent resolution** with public explanation

##  Credit Economy Design

### Inflation Protection

-   **Capped total credits** per time period
-   **Quality over quantity** emphasis
-   **Long-term value preservation**
-   **Regular system audits**

### Fair Distribution

-   **No retroactive changes** to earned credits
-   **Equal opportunity** regardless of background
-   **Skill development support** for new contributors
-   **Mentorship credit bonuses**

##  Recognition Events

### Monthly Celebrations

**Top Contributor Spotlights:**

-   Public recognition across all channels
-   Detailed contribution stories
-   Technical deep-dives on their work
-   Community Q&A sessions

### Annual Contributors Conference

**Future Plans:**

-   All-expenses-paid contributor gathering
-   Technical presentations and workshops
-   Networking with AI industry leaders
-   Exclusive product announcements

##  Getting Started

### Immediate Actions

1. **[Browse repositories](https://github.com/orgs/the-answerai/repositories)** and find your first issue
2. **Join Discord** for real-time community support
3. **Read contribution guidelines** for quality standards
4. **Start building** - every contribution counts!

### Maximizing Your Credits

-   **Focus on quality** over quantity
-   **Include comprehensive tests** for bonus multipliers
-   **Write clear documentation** for your contributions
-   **Help other contributors** for community credits
-   **Record compelling videos** that tell your story

---

##  Important Notes

**Legal Disclaimer**: Credit system details may evolve as we finalize legal structures for revenue sharing and equity participation. All contributors will be grandfathered into any system improvements.

**Transparency Commitment**: We pledge to maintain full transparency in credit allocation, with public audits and community oversight of the system.

**Long-term Vision**: Credits are designed to recognize your early contributions to building the future of AI. Your work today creates lasting value for yourself and the community.

**Ready to earn your first credits?** [Start contributing now](https://github.com/orgs/the-answerai/repositories) and help us prove that developers can build better AI tools than Big Tech!
</file>

<file path="developers/embed.md">
---
description: Learn how to embed our in-house chat widget
---

# Embed

---

You can embed the chat widget on your website. Simply copy the provided code and paste it anywhere within the tag of your HTML file.

<figure><img src="/.gitbook/assets/image (8) (2) (1) (1).png" alt="" /><figcaption></figcaption></figure>

## Widget Setup

The following video shows how to inject the widget script into any webpage.

<iframe src="https://github.com/FlowiseAI/Flowise/assets/26460777/c128829a-2d08-4d60-b821-1e41a9e677d0"></iframe>

## Chatflow Config

You can pass `chatflowConfig` JSON object to override existing configuration. This is the same as [#override-config](/docs/api/prediction/create-prediction 'mention') in API.

```html
<script type="module">
    import Chatbot from 'https://cdn.jsdelivr.net/npm/aai-embed/dist/web.js'
    Chatbot.init({
        chatflowid: 'abc',
        apiHost: 'http://localhost:3000',
        chatflowConfig: {
            sessionId: '123',
            returnSourceDocuments: true
        }
    })
</script>
```

## Observer Config

This allows you to execute code in parent based upon signal observations within the chatbot.

```html
<script type="module">
    import Chatbot from 'https://cdn.jsdelivr.net/npm/aai-embed/dist/web.js'
    Chatbot.init({
        chatflowid: 'abc',
        apiHost: 'http://localhost:3000',
        observersConfig: {
            // User input has changed
            observeUserInput: (userInput) => {
                console.log({ userInput })
            },
            // The bot message stack has changed
            observeMessages: (messages) => {
                console.log({ messages })
            },
            // The bot loading signal changed
            observeLoading: (loading) => {
                console.log({ loading })
            }
        }
    })
</script>
```

## Theme

You can change the pop up button properties, as well as the chat window:

```html
<script type="module">
    import Chatbot from 'https://cdn.jsdelivr.net/npm/aai-embed/dist/web.js'
    Chatbot.init({
        chatflowid: '91e9c803-1234-5a6b7-8207-3c0915d71c5f',
        apiHost: 'https://prod.studio.theanswer.ai',
        chatflowConfig: {
            // topK: 2
        },
        observersConfig: {
            // (optional) Allows you to execute code in parent based upon signal observations within the chatbot.
            // The userinput field submitted to bot ("" when reset by bot)
            observeUserInput: (userInput) => {
                console.log({ userInput })
            },
            // The bot message stack has changed
            observeMessages: (messages) => {
                console.log({ messages })
            },
            // The bot loading signal changed
            observeLoading: (loading) => {
                console.log({ loading })
            }
        },
        theme: {
            button: {
                backgroundColor: '#3B81F6',
                right: 20,
                bottom: 20,
                size: 48, // small | medium | large | number
                dragAndDrop: true,
                iconColor: 'white',
                customIconSrc: 'https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/svg/google-messages.svg',
                autoWindowOpen: {
                    autoOpen: true, //parameter to control automatic window opening
                    openDelay: 2, // Optional parameter for delay time in seconds
                    autoOpenOnMobile: false //parameter to control automatic window opening in mobile
                }
            },
            tooltip: {
                showTooltip: true,
                tooltipMessage: 'Hi There !',
                tooltipBackgroundColor: 'black',
                tooltipTextColor: 'white',
                tooltipFontSize: 16
            },
            chatWindow: {
                showTitle: true,
                showAgentMessages: true,
                title: 'AnswerAgentAI Bot',
                titleAvatarSrc: 'https://raw.githubusercontent.com/walkxcode/dashboard-icons/main/svg/google-messages.svg',
                welcomeMessage: 'Hello! This is custom welcome message',
                errorMessage: 'This is a custom error message',
                backgroundColor: '#ffffff',
                backgroundImage: 'enter image path or link', // If set, this will overlap the background color of the chat window.
                height: 700,
                width: 400,
                fontSize: 16,
                starterPrompts: ['What is a bot?', 'Who are you?'], // It overrides the starter prompts set by the chat flow passed
                starterPromptFontSize: 15,
                clearChatOnReload: false, // If set to true, the chat will be cleared when the page reloads.
                botMessage: {
                    backgroundColor: '#f7f8ff',
                    textColor: '#303235',
                    showAvatar: true,
                    avatarSrc: 'https://raw.githubusercontent.com/zahidkhawaja/langchain-chat-nextjs/main/public/parroticon.png'
                },
                userMessage: {
                    backgroundColor: '#3B81F6',
                    textColor: '#ffffff',
                    showAvatar: true,
                    avatarSrc: 'https://raw.githubusercontent.com/zahidkhawaja/langchain-chat-nextjs/main/public/usericon.png'
                },
                textInput: {
                    placeholder: 'Type your question',
                    backgroundColor: '#ffffff',
                    textColor: '#303235',
                    sendButtonColor: '#3B81F6',
                    maxChars: 50,
                    maxCharsWarningMessage: 'You exceeded the characters limit. Please input less than 50 characters.',
                    autoFocus: true, // If not used, autofocus is disabled on mobile and enabled on desktop. true enables it on both, false disables it on both.
                    sendMessageSound: true,
                    // sendSoundLocation: "send_message.mp3", // If this is not used, the default sound effect will be played if sendSoundMessage is true.
                    receiveMessageSound: true
                    // receiveSoundLocation: "receive_message.mp3", // If this is not used, the default sound effect will be played if receiveSoundMessage is true.
                },
                feedback: {
                    color: '#303235'
                },
                footer: {
                    textColor: '#303235',
                    text: 'Powered by',
                    company: 'AnswerAgentAI',
                    companyLink: 'https://theanswer.ai'
                }
            }
        }
    })
</script>
```
</file>

<file path="developers/environment-variables.md">
---
description: Environment variables for AnswerAgentAI
---

# Environment Variables

:::info
Before you begin, ensure you have [NodeJS](https://nodejs.org/en/download) installed on your computer. AnswerAgentAI supports Node `v18.15.0` or `v20` and above.
:::

There are three different .env files you can set environment variables for AnswerAgentAI.

-   Root (.env)
-   Server (packages/server/.env)
-   UI (packages/ui/.env)

| Variable                      | Description                                              | File Location          |
| ----------------------------- | -------------------------------------------------------- | ---------------------- |
| PORT                          | The port number for the server to run on                 | Root, Server, UI       |
| API_PORT                      | The port number for the API server                       | UI                     |
| APIKEY_PATH                   | Path to the API key file                                 | Root, Server           |
| SECRETKEY_PATH                | Path to the secret key file                              | Root, Server           |
| LOG_PATH                      | Path to store log files                                  | Root, Server           |
| DISABLE_FLOWISE_TELEMETRY     | Disable Flowise telemetry when set to true               | Root                   |
| IFRAME_ORIGINS                | Allowed origins for iframes                              | Root                   |
| CHATFLOW_DOMAIN_OVERRIDE      | Overrides the chatflowDomain with a specified URL        | Root, Web              |
| VITE_AUTH_DOMAIN              | Auth0 domain for authentication                          | Root, UI               |
| VITE_AUTH_CLIENT_ID           | Auth0 client ID for authentication                       | Root, UI               |
| VITE_AUTH_AUDIENCE            | Auth0 audience for authentication                        | Root, UI               |
| VITE_AUTH_ORGANIZATION_ID     | Auth0 organization ID for authentication                 | Root, UI               |
| DOMAIN                        | Domain for the staging environment                       | Root                   |
| ANSWERAI_DOMAIN               | Domain for the beta environment                          | Root                   |
| DATABASE_TYPE                 | Type of database (e.g., postgres)                        | Root, Server           |
| DATABASE_USER                 | Database username                                        | Root, Server           |
| DATABASE_PASSWORD             | Database password                                        | Root, Server           |
| DATABASE_HOST                 | Database host address                                    | Root, Server           |
| DATABASE_PORT                 | Database port number                                     | Root, Server, AnswerAgentAI |
| AUTH0_JWKS_URI                | Auth0 JSON Web Key Set URI                               | Root                   |
| AUTH0_ISSUER_BASE_URL         | Auth0 issuer base URL                                    | Root                   |
| AUTH0_BASE_URL                | Base URL for Auth0 authentication                        | Root                   |
| AUTH0_CLIENT_ID               | Auth0 client ID                                          | Root                   |
| AUTH0_CLIENT_SECRET           | Auth0 client secret                                      | Root                   |
| AUTH0_AUDIENCE                | Auth0 audience                                           | Root                   |
| AUTH0_SCOPE                   | Auth0 scope for authentication                           | Root                   |
| AUTH0_TOKEN_SIGN_ALG          | Auth0 token signing algorithm                            | Root                   |
| AUTH0_ORGANIZATION_ID         | Auth0 organization ID                                    | Root                   |
| LANGFUSE_RELEASE              | Langfuse release identifier                              | Root                   |
| LANGFUSE_SECRET_KEY           | Langfuse secret key                                      | Root                   |
| LANGFUSE_PUBLIC_KEY           | Langfuse public key                                      | Root                   |
| LANGFUSE_HOST                 | Langfuse host URL                                        | Root                   |
| AUTH_AUTH0_CLIENT_ID          | Auth0 client ID for authentication                       | Root                   |
| AUTH_AUTH0_CLIENT_SECRET      | Auth0 client secret for authentication                   | Root                   |
| AUTH_AUTH0_ISSUER             | Auth0 issuer URL for authentication                      | Root                   |
| DATABASE_URL                  | Full database connection URL                             | AnswerAgentAI               |
| VITE_FLAGSMITH_ENVIRONMENT_ID | Flagsmith environment ID                                 | UI                     |
| NUMBER_OF_PROXIES             | Number of proxies (commented out)                        | Server                 |
| CORS_ORIGINS                  | Allowed CORS origins (commented out)                     | Server                 |
| DATABASE_NAME                 | Database name (commented out)                            | Server                 |
| DATABASE_SSL                  | Enable SSL for database connection (commented out)       | Server                 |
| DATABASE_SSL_KEY_BASE64       | Base64 encoded SSL key for database (commented out)      | Server                 |
| FLOWISE_USERNAME              | Flowise username (commented out)                         | Server                 |
| FLOWISE_PASSWORD              | Flowise password (commented out)                         | Server                 |
| FLOWISE_SECRETKEY_OVERWRITE   | Encryption key for Flowise (commented out)               | Server                 |
| FLOWISE_FILE_SIZE_LIMIT       | File size limit for Flowise (commented out)              | Server                 |
| DISABLE_CHATFLOW_REUSE        | Disable chatflow reuse when set to true (commented out)  | Server                 |
| DEBUG                         | Enable debug mode when set to true (commented out)       | Server                 |
| LOG_LEVEL                     | Logging level (commented out)                            | Server                 |
| TOOL_FUNCTION_BUILTIN_DEP     | Built-in dependencies for tool functions (commented out) | Server                 |
| TOOL_FUNCTION_EXTERNAL_DEP    | External dependencies for tool functions (commented out) | Server                 |
| LANGCHAIN_TRACING_V2          | Enable LangChain tracing v2 (commented out)              | Server                 |
| LANGCHAIN_ENDPOINT            | LangChain API endpoint (commented out)                   | Server                 |
| LANGCHAIN_API_KEY             | LangChain API key (commented out)                        | Server                 |
| LANGCHAIN_PROJECT             | LangChain project name (commented out)                   | Server                 |
| MODEL_LIST_CONFIG_JSON        | Path to model list configuration JSON (commented out)    | Server                 |
| STORAGE_TYPE                  | Storage type (local or s3) (commented out)               | Server                 |
| BLOB_STORAGE_PATH             | Path for local blob storage (commented out)              | Server                 |
| S3_STORAGE_BUCKET_NAME        | S3 storage bucket name (commented out)                   | Server                 |
| S3_STORAGE_ACCESS_KEY_ID      | S3 storage access key ID (commented out)                 | Server                 |
| S3_STORAGE_SECRET_ACCESS_KEY  | S3 storage secret access key (commented out)             | Server                 |
| S3_STORAGE_REGION             | S3 storage region (commented out)                        | Server                 |
| S3_ENDPOINT_URL               | Custom S3 endpoint URL (commented out)                   | Server                 |
| APIKEY_STORAGE_TYPE           | API key storage type (json or db) (commented out)        | Server                 |
| SHOW_COMMUNITY_NODES          | Show community nodes when set to true (commented out)    | Server                 |

This table provides a comprehensive overview of all the environment variables used across the different files in your project. The "File Location" column indicates which file(s) each variable is found in (Root, Server, UI, or AnswerAgentAI).
</file>

<file path="developers/README.md">
---
description: Learn how to run AnswerAgentAI locally, deploy it to various platforms and contribute to the project
---

# Developers

Welcome to the developers section of the AnswerAgentAI documentation. This area provides comprehensive guides and information for developers working with or contributing to AnswerAgentAI. Below you'll find links to various subsections, each covering specific aspects of development and deployment.

## API Reference

AnswerAgentAI provides a comprehensive set of APIs to help you integrate our AI capabilities into your applications:

-   [API Documentation](/docs/api) - Explore our complete API reference, including endpoints, request/response formats, and examples
-   [Getting Your API Key](/docs/developers/authorization/chatflow-level) - Learn how to generate and use your API key

## Deployment

Learn how to deploy AnswerAgentAI on various platforms:

-   [AWS Deployment](deployment/aws.md) - Deploy AnswerAgentAI on Amazon Web Services
-   [Azure Deployment](deployment/azure.md) - Set up AnswerAgentAI on Microsoft Azure
-   [Google Cloud Platform Deployment](deployment/gcp.md) - Use GCP to host AnswerAgentAI
-   [Render Deployment](deployment/render.md) - Deploy AnswerAgentAI using Render

## Development

Guides for developers working on AnswerAgentAI:

-   [Building Nodes](building-node.md) - Learn how to create custom nodes for AnswerAgentAI
-   [Databases](databases.md) - Information on database setup and management for AnswerAgentAI
-   [Environment Variables](environment-variables.md) - Comprehensive guide to configuring environment variables
-   [Running Locally](running-locally.md) - Instructions for setting up and running AnswerAgentAI in a local development environment

## Additional Resources

-   [Deployment README](deployment) - Overview of deployment options and considerations

This documentation is designed to help developers get started quickly, understand the architecture, and contribute effectively to the AnswerAgentAI project. If you have any questions or need further clarification, please don't hesitate to reach out to the development team or consult the project's issue tracker.

Happy coding!

<!-- TODO: Add links to contribution guidelines and code of conduct when available -->
</file>

<file path="developers/running-locally.md">
---
description: Learn how to deploy AnswerAgentAI locally
---

# Local Development

This guide will help you set up and run AnswerAgentAI in your local development environment for ultimate privacy and control.

## Prerequisites

Before you begin, ensure you have the following installed:

-   [Node.js](https://nodejs.org/en/download) (v18.15.0 or v20 and above)
-   [PNPM](https://pnpm.io/installation) (v9 or above)
-   [Git](https://git-scm.com/downloads)

## Project Structure

AnswerAgentAI is a monorepo project with multiple packages:

Certainly! I'll list out the rest of the @packages folders with descriptions based on the information available in the provided code blocks. Here's an expanded list of the packages:

-   `server`: Node.js backend for API logic
-   `ui`: React frontend
-   `components`: Integration components
-   `docs`: Documentation site
-   `embed`: Javascript library to display AnswerAgentAI chatbot on your website

Here's a more detailed breakdown of each package:

1. `server`:

    - Main backend server for AnswerAgentAI
    - Handles API logic, database interactions, and server-side operations
    - Contains marketplaces and chatflow configurations

2. `ui`:

    - React-based frontend for the AnswerAgentAI application
    - Provides the user interface for interacting with AnswerAgentAI

3. `components`:

    - Contains integration components and nodes for AnswerAgentAI
    - Includes various LLM models, tools, and utilities

4. `docs`:

    - Documentation site for AnswerAgentAI
    - Contains user guides, API documentation, and developer resources

5. `embed`:

    - Javascript library for embedding AnswerAgentAI chatbot on websites
    - Provides components and features for full chatbot integration

These packages work together to create the full AnswerAgentAI ecosystem, providing a comprehensive solution for building and deploying AI-powered chatbots and workflows.

## Setup

1. Clone the repository:

```bash
git clone https://github.com/AnswerAgentAI/AnswerAgentAI.git
cd AnswerAgentAI
```

2. Install dependencies:

```bash
pnpm install
```

3. Build the project:

```bash
pnpm build
```

## Running the Application

To start the application in development mode:

```bash
pnpm dev
```

This command will start both the backend server and the frontend development server. The application will be available at [http://localhost:8080](http://localhost:8080).

For production mode:

```bash
pnpm start
```

The application will be available at [http://localhost:3000](http://localhost:3000).

## Environment Configuration

1. Create `.env` files in both `packages/ui` and `packages/server` directories.
2. Use the `.env.example` files in each directory as a reference for the required environment variables.

Key environment variables:

-   `packages/ui/.env`: Set `VITE_PORT` for the frontend development server
-   `packages/server/.env`: Set `PORT` for the backend server

## Development Workflow

1. Make changes to the code in `packages/ui` or `packages/server`.
2. The development server will automatically reload with your changes.
3. For changes in `packages/components`, you need to rebuild the project:

```bash
pnpm build
```

## Project Configuration

The project uses the following key configurations:

1. TypeScript Configuration:

```1:22:packages/components/tsconfig.json
{
    "compilerOptions": {
        "lib": ["ES2020", "ES2021.String"],
        "experimentalDecorators": true /* Enable experimental support for TC39 stage 2 draft decorators. */,
        "emitDecoratorMetadata": true /* Emit design-type metadata for decorated declarations in source files. */,
        "target": "ES2020", // or higher
        "outDir": "./dist/",
        "resolveJsonModule": true,
        "esModuleInterop": true /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables `allowSyntheticDefaultImports` for type compatibility. */,
        "forceConsistentCasingInFileNames": true /* Ensure that casing is correct in imports. */,
        "strict": true /* Enable all strict type-checking options. */,
        "skipLibCheck": true /* Skip type checking all .d.ts files. */,
        "sourceMap": true,
        "strictPropertyInitialization": false,
        "useUnknownInCatchVariables": false,
        "declaration": true,
        "module": "commonjs"
    },
    "include": ["src", "nodes", "credentials"],
    "exclude": ["gulpfile.ts", "node_modules", "dist"]
}

```

2. Prettier Configuration:

```98:106:package.json
    "prettier": {
        "printWidth": 140,
        "singleQuote": true,
        "jsxSingleQuote": true,
        "trailingComma": "none",
        "tabWidth": 4,
        "semi": false,
        "endOfLine": "auto"
    },
```

3. Babel Configuration:

```107:119:package.json
    "babel": {
        "presets": [
            "@babel/preset-typescript",
            [
                "@babel/preset-env",
                {
                    "targets": {
                        "node": "current"
                    }
                }
            ]
        ]
    },
```

## Testing

To run end-to-end tests:

```bash
pnpm test:e2e
```

For component tests:

```bash
pnpm test:cmp
```

## Documentation

To run the documentation site locally:

```bash
cd packages/docs
pnpm start
```

The documentation will be available at [http://localhost:4242](http://localhost:4242).

## Troubleshooting

-   If you encounter memory issues during build, try increasing the Node.js heap size:

```bash
export NODE_OPTIONS="--max-old-space-size=4096"
pnpm build
```

-   Ensure all required environment variables are set correctly in your `.env` files.
-   For any other issues, check the console output for error messages or refer to the project's issue tracker on GitHub.

## Contributing

Please refer to the `CONTRIBUTING.md` file in the repository for guidelines on how to contribute to the project.

This guide should help you get started with running AnswerAgentAI locally. If you encounter any issues or have questions, please refer to the project's documentation or reach out to the development team.
</file>

<file path="developers/video-guide.md">
---
title: Video Contribution Guide
description: Complete guide to creating effective contribution videos for AnswerAgentAI Alpha Sprint - technical requirements, content guidelines, and examples
---

# Video Contribution Guide

Every merged contribution to AnswerAgentAI requires a video explanation. This creates a human connection to each contribution and reinforces our mission alignment. Here's how to create compelling videos that earn your credits.

##  Why Videos Matter

### Mission Alignment

Videos help us ensure every contribution serves our privacy-first, developer-empowering mission. They create accountability and shared understanding.

### Community Building

Your videos become part of our collective story - showing the world what happens when committed developers build for developers.

### Recognition

Great videos get featured across our channels, giving you recognition for your contributions and helping build your technical reputation.

##  Video Requirements

### Mandatory Content

Your 1-3 minute video must cover:

1. **What you built/fixed** (30-45 seconds)

    - Technical overview of your contribution
    - Code walkthrough or demo
    - Before/after comparison

2. **Problem it solves** (30-45 seconds)

    - User pain point addressed
    - Specific use case improvement
    - Impact on user experience

3. **Mission alignment** (30-45 seconds)

    - How it serves privacy goals
    - Developer empowerment aspect
    - Community benefit

4. **Working demo** (30-45 seconds)
    - Live functionality showcase
    - Edge cases handled
    - Integration with existing features

### Technical Specifications

**Video Requirements:**

-   **Resolution**: 720p minimum (1080p recommended)
-   **Frame Rate**: 30fps minimum
-   **Audio**: Clear, understandable speech (-12dB to -6dB levels)
-   **Length**: 1-3 minutes (strict enforcement)
-   **Format**: MP4 (H.264), MOV, or WebM

**Audio Guidelines:**

-   Use a decent microphone (headset mic is fine)
-   Record in a quiet environment
-   Speak clearly at moderate pace
-   Test audio levels before recording

##  Technical Setup

### Recommended Tools

**Free Options:**

-   **OBS Studio** - Professional recording and streaming
-   **Loom** - Easy browser-based recording
-   **ShareX** - Windows screen recording
-   **QuickTime** - macOS built-in screen recording

**Professional Options:**

-   **Camtasia** - Full editing suite
-   **ScreenFlow** - macOS professional recording
-   **Snagit** - Simple editing and recording

### Screen Recording Setup

**Display Settings:**

-   Use 1920x1080 resolution for recording
-   Increase font sizes for visibility
-   Hide sensitive information (keys, tokens)
-   Close unnecessary browser tabs/applications

**Recording Area:**

-   Capture specific application window (preferred)
-   Use full screen only when necessary
-   Avoid recording multiple monitors simultaneously

### Audio Recording

**Microphone Options:**

1. **Built-in laptop mic** - Acceptable for quiet environments
2. **Headset microphone** - Recommended minimum quality
3. **USB microphone** - Professional quality
4. **Lavalier mic** - Consistent audio levels

**Audio Tips:**

-   Test recording levels before starting
-   Use noise suppression if available
-   Record in a quiet space
-   Speak 6-12 inches from microphone

##  Content Structure

### Opening (15-30 seconds)

**Introduce yourself and your contribution:**

```
"Hi, I'm [Name], and I just contributed [brief description]
to the AnswerAgentAI [repository name]. This addresses [problem]
and helps users [benefit]."
```

### Technical Walkthrough (60-90 seconds)

**Show your code and explain the implementation:**

1. **Code Structure**

    - Navigate through files you changed
    - Highlight key functions/components
    - Explain architectural decisions

2. **Implementation Details**

    - Show before/after states
    - Demonstrate the solution
    - Point out important code segments

3. **Testing**
    - Run your tests if applicable
    - Show edge cases handled
    - Demonstrate error handling

### Demo & Impact (45-60 seconds)

**Show the feature working and its impact:**

1. **Live Demo**

    - Use the feature as an end user would
    - Show multiple use cases
    - Highlight user experience improvements

2. **Impact Explanation**
    - Explain who benefits
    - Quantify improvements if possible
    - Connect to broader platform goals

### Mission Connection (30-45 seconds)

**Connect your work to AnswerAgentAI's mission:**

1. **Privacy Benefits**

    - How it enhances user privacy
    - Data protection improvements
    - Local-first computing advantages

2. **Developer Empowerment**

    - Tools for better developer experience
    - Enabling new use cases
    - Reducing friction in workflows

3. **Community Impact**
    - How it helps other contributors
    - Building toward our shared vision
    - Advancing the platform's capabilities

##  Recording Best Practices

### Preparation Checklist

**Environment Setup:**

-   [ ] Quiet recording space
-   [ ] Good lighting on your face (if showing yourself)
-   [ ] Clean desktop/screen
-   [ ] Applications ready to demo
-   [ ] Script or talking points prepared

**Technical Prep:**

-   [ ] Audio levels tested
-   [ ] Recording software configured
-   [ ] Demo environment ready
-   [ ] Backup recording method available

### During Recording

**Speaking Tips:**

-   Speak slowly and clearly
-   Use natural, conversational tone
-   Pause briefly between sections
-   Explain technical terms for accessibility

**Screen Interaction:**

-   Move cursor deliberately
-   Highlight important elements
-   Use zoom for small details
-   Give viewers time to read code

**Common Mistakes to Avoid:**

-   Speaking too fast
-   Mumbling or unclear speech
-   Showing sensitive information
-   Rushing through complex code
-   Forgetting to demonstrate functionality

##  Example Video Structures

### Bug Fix Video Example

```
1. Introduction (20 seconds)
   "I fixed a critical bug in the chat interface where messages
   weren't displaying properly in dark mode..."

2. Problem Demonstration (45 seconds)
   [Show the bug occurring, explain impact on users]

3. Solution Walkthrough (60 seconds)
   [Show code changes, explain the fix]

4. Testing & Verification (30 seconds)
   [Demonstrate the fix working]

5. Mission Connection (25 seconds)
   "This improves accessibility and ensures our privacy-focused
   chat works for all users..."
```

### New Feature Video Example

```
1. Introduction (25 seconds)
   "I built a new document summarization feature for the
   browser extension..."

2. Feature Demo (75 seconds)
   [Show the feature working end-to-end]

3. Technical Implementation (45 seconds)
   [Explain key code components]

4. Impact & Mission (35 seconds)
   "This helps users quickly understand content while keeping
   their data local and private..."
```

##  Submission Process

### Upload Options

**YouTube (Recommended):**

-   Upload as "Unlisted" video
-   Use descriptive title: "AnswerAgentAI Contribution: [Brief Description]"
-   Include link in PR description

**Direct File Upload:**

-   Use GitHub release attachments
-   Discord file upload (if under 25MB)
-   Google Drive/Dropbox with public link

**Cloud Storage:**

-   Ensure public access enabled
-   Use stable, permanent links
-   Test download access before submitting

### PR Integration

**In Your Pull Request:**
Add a "Video Explanation" section:

```markdown
## Video Explanation

[YouTube Link] or [Download Link]

### Video Summary:

-   Demonstrates [functionality]
-   Explains [technical approach]
-   Shows [user impact]
-   Connects to [mission goals]
```

##  Video Quality Tiers

### Bronze Quality (Credit Eligible)

-   Meets all technical requirements
-   Covers required content areas
-   Clear audio and visual
-   Demonstrates functionality

### Silver Quality (Bonus Credits)

-   Excellent explanation clarity
-   Professional presentation
-   Engaging storytelling
-   Strong mission connection

### Gold Quality (Feature Worthy)

-   Outstanding production value
-   Compelling narrative
-   Educational for community
-   Potential for promotional use

##  Review & Feedback

### What We Look For

**Technical Accuracy:**

-   Correct explanation of implementation
-   Accurate description of problem/solution
-   Valid demonstration of functionality

**Mission Alignment:**

-   Clear privacy/empowerment connection
-   Understanding of community goals
-   Authentic commitment to values

**Communication Quality:**

-   Clear, understandable explanation
-   Logical flow of information
-   Engaging presentation style

### Feedback Process

**Approval Timeline:**

-   Initial review within 24 hours
-   Feedback provided within 48 hours
-   Credits awarded within 72 hours

**Common Feedback:**

-   "Great technical explanation, could strengthen mission connection"
-   "Excellent demo, audio quality needs improvement"
-   "Strong content, consider showing more edge cases"

### Resubmission

If your video needs improvement:

1. **Review feedback** carefully
2. **Record updated version** addressing concerns
3. **Resubmit** with original PR link
4. **No credit penalty** for resubmissions

##  Featured Video Examples

### Hall of Fame Contributors

_Videos will be linked here as exemplary contributions are submitted_

**Coming Soon:**

-   Outstanding bug fix explanations
-   Innovative feature demonstrations
-   Compelling mission alignment examples
-   Technical deep-dives worth showcasing

### Learning Resources

**Tutorial Series:**
We're creating a series based on the best contributor videos:

-   "How to Explain Technical Changes"
-   "Connecting Code to Mission"
-   "Effective Demo Techniques"
-   "Building Community Through Video"

---

##  Ready to Record?

Your video is more than just a requirementit's your chance to tell the story of how you're helping build the future of AI. Make it count!

**Quick Start Checklist:**

-   [ ] Review your contribution and plan key points
-   [ ] Set up recording environment
-   [ ] Practice your explanation once
-   [ ] Record your video (1-3 minutes)
-   [ ] Review and re-record if needed
-   [ ] Submit with your PR

**Questions?** Join our Discord #video-help channel for real-time support from the community.

**Ready to start building?** [Browse repositories](https://github.com/orgs/the-answerai/repositories) and find your first contribution to showcase!
</file>

<file path="integrations/README.md">
---
description: Learn how to integrate Flowise with third-party platforms
---

# External Integrations

---

Flowise can also be used in 3rd party platform. Here are some usage examples:

-   [Zapier Zaps](zapier-zaps.md)
</file>

<file path="integrations/zapier-zaps.md">
---
description: Learn how to integrate AnswerAgentAI and Zapier
---

# Zapier Zaps

---

## Prerequisite

1. [Log in](https://zapier.com/app/login) or [sign up](https://zapier.com/sign-up) to Zapier
2. Refer [deployment](../developers/deployment/) to create a cloud hosted version of Flowise.

## Setup

1. Go to [Zapier Zaps](https://zapier.com/app/zaps)
2. Click **Create**

<figure><img src="/.gitbook/assets/zapier/zap/1.png" alt="" /><figcaption></figcaption></figure>

### Receive Trigger Message

1. Click or Search for **Discord**

 <figure><img src="/.gitbook/assets/zapier/zap/2.png" alt="" width="563" /><figcaption></figcaption></figure>

2. Select **New Message Posted to Channel** as Event then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/3.png" alt="" width="563" /><figcaption></figcaption></figure>

3. **Sign in** your Discord account

 <figure><img src="/.gitbook/assets/zapier/zap/4.png" alt="" width="563" /><figcaption></figcaption></figure>

4. Add **Zapier Bot** to your prefered server

 <figure><img src="/.gitbook/assets/zapier/zap/5.png" alt="" width="272" /><figcaption></figcaption></figure>

5. Give appropriate permissions and click **Authorize** then click **Continue**

    <figure><img src="/.gitbook/assets/zapier/zap/6.png" alt="" width="292" /><figcaption></figcaption></figure>

    <figure><img src="/.gitbook/assets/zapier/zap/7.png" alt="" width="290" /><figcaption></figcaption></figure>

6. Select your **prefered channel** to interact with Zapier Bot then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/8.png" alt="" width="563" /><figcaption></figcaption></figure>

7. **Send a message** to your selected channel on step 8

 <figure><img src="/.gitbook/assets/zapier/zap/9.png" alt="" width="563" /><figcaption></figcaption></figure>

8. Click **Test trigger**

 <figure><img src="/.gitbook/assets/zapier/zap/10.png" alt="" width="563" /><figcaption></figcaption></figure>

9. Select your message then click **Continue with the selected record**

 <figure><img src="/.gitbook/assets/zapier/zap/11.png" alt="" width="563" /><figcaption></figcaption></figure>

### Filter out Zapier Bot's Message

1. Click or search for **Filter**

 <figure><img src="/.gitbook/assets/zapier/zap/12.png" alt="" width="563" /><figcaption></figcaption></figure>

2. Configure **Filter** to not continue if received message from **Zapier Bot** then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/13.png" alt="" width="563" /><figcaption></figcaption></figure>

### FlowiseAI generate Result Message

1. Click **+**, click or search for **FlowiseAI**

 <figure><img src="/.gitbook/assets/zapier/zap/14.png" alt="" width="563" /><figcaption></figcaption></figure>

2. Select **Make Prediction** as Event, then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/15.png" alt="" width="563" /><figcaption></figcaption></figure>

3. Click **Sign in** and insert your details, then click **Yes, Continue to FlowiseAI**

    <figure><img src="/.gitbook/assets/zapier/zap/16.png" alt="" width="563" /><figcaption></figcaption></figure>

    <figure><img src="/.gitbook/assets/zapier/zap/17.png" alt="" width="563" /><figcaption></figcaption></figure>

4. Select **Content** from Discord and your Flow ID, then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/18.png" alt="" width="563" /><figcaption></figcaption></figure>

5. Click **Test action** and wait for your result

 <figure><img src="/.gitbook/assets/zapier/zap/19.png" alt="" width="563" /><figcaption></figcaption></figure>

### Send Result Message

1. Click **+**, click or search for **Discord**

 <figure><img src="/.gitbook/assets/zapier/zap/20.png" alt="" width="563" /><figcaption></figcaption></figure>

2. Select **Send Channel Message** as Event, then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/21.png" alt="" width="563" /><figcaption></figcaption></figure>

3. Select the Discord's account that you signed in, then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/22.png" alt="" width="563" /><figcaption></figcaption></figure>

4. Select your prefered Channel for channel and select **Text** and **String Source** (if available) from FlowiseAI for Message Text, then click **Continue**

 <figure><img src="/.gitbook/assets/zapier/zap/23.png" alt="" width="563" /><figcaption></figcaption></figure>

5. Click **Test action**

 <figure><img src="/.gitbook/assets/zapier/zap/24.png" alt="" /><figcaption></figcaption></figure>

6. Voila [](https://emojipedia.org/party-popper/) you should see the message arrived in your Discord Channel

 <figure><img src="/.gitbook/assets/zapier/zap/25.png" alt="" /><figcaption></figcaption></figure>

7. Lastly, rename your Zap and publish it

 <figure><img src="/.gitbook/assets/zapier/zap/26.png" alt="" /><figcaption></figcaption></figure>
</file>

<file path="postlaunch/advanced-topics/best-practices-for-sidekick-design.md">
---
description: Best practices for Sidekick design
---

<!-- TODO: Add best practices for Sidekick design -->
</file>

<file path="postlaunch/advanced-topics/customization-and-extensibility.md">
---
description: Learn how to customize and extend AnswerAgentAI
---

<!-- TODO: Add customization and extensibility -->
</file>

<file path="postlaunch/analytic.md">
---
description: Learn how to analyze and troubleshoot your chatflows and agentflows
---

# Analytic

---

There are several analytic providers Flowise integrates with:

-   [Langsmith](https://smith.langchain.com/)
-   [Langfuse](https://langfuse.com/)
-   [LunaryAI](https://lunary.ai/)
-   [LangWatch](https://langwatch.ai/)

## Setup

1. At the top right corner of your Chatflow or Agentflow, click **Settings** > **Configuration**

<figure><img src="/.gitbook/assets/analytic-1.webp" alt="Screenshot of user clicking in the configuration menu" width="375" /><figcaption></figcaption></figure>

2. Then go to the Analyse Chatflow section

<figure><img src="/.gitbook/assets/analytic-2.png" alt="Screenshot of the Analyse Chatflow section with the different Analytics providers" /><figcaption></figcaption></figure>

3. You will see a list of providers, along with their configuration fields

<figure><img src="/.gitbook/assets/image (82).png" alt="Screenshot of an analytics provider with credentials fields expanded" /><figcaption></figcaption></figure>

3. Fill in the credentials and other configuration details, then turn the provider **ON**

<figure><img src="/.gitbook/assets/image (83).png" alt="Screenshot of analytics providers enabled" /><figcaption></figcaption></figure>

## API

Once the analytic has been turned ON from the UI, you can override or provide additional configuration in the body of the [Prediction API](#):

```json
{
    "question": "hi there",
    "overrideConfig": {
        "analytics": {
            "langFuse": {
                // langSmith, langFuse, lunary, langWatch
                "userId": "user1"
            }
        }
    }
}
```
</file>

<file path="postlaunch/troubleshooting-faqs.md">
---
description: Learn how to troubleshoot and get answers to FAQs
---

<!-- TODO: Add troubleshooting and FAQs -->
</file>

<file path="sidekick-studio/agentflows/sequential-agents/agent-memory-node.md">
---
description: Understanding the Agent Memory Node in Sequential Agents
---

# Agent Memory Node

The Agent Memory Node **provides a mechanism for persistent memory storage**, allowing the Sequential Agent workflow to retain the conversation history `state.messages` and any custom State previously defined across multiple interactions

This long-term memory is essential for agents to learn from previous interactions, maintain context over extended conversations, and provide more relevant responses.

<!-- ![](../../assets/seq-03.png) -->

## Where the data is recorded

By default, Flowise utilizes its **built-in SQLite database** to store conversation history and custom state data, creating a "**checkpoints**" table to manage this persistent information.

## Understanding the Agent Memory Node

The Agent Memory Node serves as the intermediary between the conversational workflow and the database, tracking the conversation history and custom state information for specific chat sessions.

-   It **assigns a unique session ID to each conversation**, allowing multiple chats to be managed simultaneously.
-   It **manages changes in State (conversation history and custom State)** throughout the workflow's execution, tracking these changes and committing them to the database.
-   It **retrieves the latest State** from the database for a chat session, ensuring continuity between conversations.

## Inputs

|                | Required | Description                                                                                 |
| -------------- | -------- | ------------------------------------------------------------------------------------------- |
| Memory Options | No       | A configuration object to **customize how memory is managed** (e.g., database connections). |

## Outputs

The Agent Memory Node has no direct outputs. Instead, it connects to the Start Node as its input, providing memory functionality to the entire Sequential Agent workflow.

## How the memory is used

When a user interacts with an Agent in which the **Start Node is connected to an Agent Memory Node**, a unique session ID is generated to identify the conversation. Each time the user engages with the workflow, the conversation history and any custom State are:

1. **Retrieved** from the database using the conversation's session ID.
2. **Updated** to include the latest user inputs and agent responses.
3. **Committed** back to the database for future use.

This allows the workflow to provide contextually relevant responses based on the complete conversation history, even for interactions spanning multiple sessions over extended periods.

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Leverage memory for personalization**

    Use the Agent Memory Node to store preferences, language patterns, and interaction history to tailor responses to individual users over time.

    **Balance context and performance**

    When working with large conversation histories, consider implementing strategies to manage context window limitations, such as summarizing or focusing on relevant portions of the conversation history.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Not connecting the Agent Memory Node to the Start Node**

    * **Problem:** The Agent Memory Node is not properly connected to the Start Node, resulting in a failure to preserve conversation context across user sessions.
    * **Example:** Users must reintroduce themselves and re-establish context in every new session because the conversation history is not being saved or retrieved correctly.
    * **Solution:** Ensure the Agent Memory Node is connected to the Start Node at the appropriate input field. Verify that the connection is active and functioning by testing the workflow across multiple sessions.

    **Misunderstanding memory persistence**

    * **Problem:** The State with a specific session ID is not being correctly saved or retrieved due to improper configuration or misunderstanding of the Agent Memory Node's functionality.
    * **Example:** Custom State variables or conversation history pieces appear to be "forgotten" in subsequent interactions.
    * **Solution:** Verify that the database is correctly set up and that the session ID generation mechanism is working properly. Test memory persistence by creating a simple workflow that reads and writes custom State variables.

    **Not configuring the correct storage method**

    * **Problem:** The chosen storage method is not appropriate for the application's requirements, leading to performance issues or data loss.
    * **Example:** The default SQLite database is used for a high-traffic application that requires more robust database solutions.
    * **Solution:** Assess your application's requirements and select the appropriate storage solution. For most use cases, the default SQLite database is sufficient, but consider alternatives for specific needs. Implement proper database maintenance practices to prevent performance degradation.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/agent-node.md">
---
description: Understanding the Agent Node in Sequential Agents
---

# Agent Node

The Agent Node is a **core component of the Sequential Agent architecture.** It acts as a decision-maker and orchestrator within our workflow.

<!-- ![](../../assets/sa-agent.png) -->

## Understanding the Agent Node

Upon receiving input from preceding nodes, which always includes the full conversation history `state.messages` and any custom State at that point in the execution, the Agent Node uses its defined "persona", established by the System Prompt, to determine if external tools are necessary to fulfill the user's request.

-   If tools are required, the Agent Node autonomously selects and executes the appropriate tool. This execution can be automatic or, for sensitive tasks, require human approval before proceeding (**Human-in-the-Loop, HITL**).
-   The Agent Node also maintains an ongoing dialogue with users, providing relevant responses based on the conversation's context and any tool-derived information.

## Inputs

|                               | Required | Description                                                                                                                                     |
| ----------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| System Prompt                 | **Yes**  | A text prompt that defines the **Agent's personality, role, and constraints**. It guides the Agent's decision-making and response generation.   |
| Tools                         | No       | The **Tool Nodes** that the Agent Node can access and execute, each offering specific functionality to retrieve information or perform actions. |
| Chat Model                    | No       | A custom **Chat Model (LLM)** to use instead of the default one defined in the Start Node.                                                      |
| JSON Schema                   | No       | A schema defining the structure of the Agent Node's response when JSON Structured Output is enabled.                                            |
| Enable JSON Structured Output | No       | A configuration to **force the Agent Node's response to follow** a predefined JSON schema.                                                      |
| Update State                  | No       | A JSON object that defines **how the Agent Node should update the custom State** before passing it to the next node.                            |

## Outputs

The Agent Node can connect to the following nodes as outputs:

-   **Agent Node:** Connects to another Agent Node to continue the conversation with a different agent or persona.
-   **LLM Node:** Routes the conversation flow to an LLM Node for processing and response generation.
-   **Condition Agent Node:** Connects to a Condition Agent Node to implement branching logic based on the agent's evaluation of the conversation.
-   **Condition Node:** Connects to a Condition Node to implement branching logic based on predefined conditions.
-   **Loop Node:** Connects to a Loop Node to implement repetitive processes based on specific conditions.
-   **End Node:** Connects to an End Node to conclude the conversational flow.

## Features

### Tool Usage

When connected to Tool Nodes, the Agent Node can **analyze the user's request and determine if tools are necessary** to fulfill it. If multiple Tool Nodes are available, the Agent Node selects the most appropriate one(s) for the task at hand.

For example, if a user asks a recipe-related question, the Agent Node might execute a "Recipe Search" Tool Node to retrieve relevant recipes, or if a user queries about weather data, the Agent Node might execute a "Weather API" Tool Node.

### Human-in-the-Loop (HITL)

The Agent Node supports **Human-in-the-Loop (HITL)** functionality, allowing human intervention in the agent's decision-making process, particularly for sensitive or critical operations. When HITL is enabled, the Agent Node might request human approval before executing a tool, making it a critical feature for workflows where accuracy, security, or alignment are paramount.

### State Management

In addition to responding to the user and executing tools, the Agent Node can **modify the custom State** to reflect progress or changes in the conversation. This enables the Agent Node to share information with subsequent nodes in the workflow.

For example, an Agent Node might update the custom State to mark a client's ID verification as complete, enabling other nodes in the workflow to proceed with sensitive operations.

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Clear system prompt**

    Craft a concise and unambiguous System Prompt that accurately reflects the agent's role and capabilities. This guides the agent's decision-making and ensures it acts within its defined scope.

    **Strategic tool selection**

    Choose and configure a set of tools that align with the agent's purpose. Avoid providing too many tools, which can lead to confusion and suboptimal selections. Instead, select specific, purpose-driven tools that integrate seamlessly with the agent's role.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Unclear or incomplete system prompt**

    * **Problem:** The System Prompt provided to the Agent Node lacks the necessary specificity and context to guide the agent effectively in carrying out its intended tasks. A vague or overly general prompt can lead to irrelevant responses, difficulty in understanding user intent, and an inability to leverage tools or data appropriately.
    * **Example:** You're building a travel booking agent, and your System Prompt simply states "_You are a helpful AI assistant._" This lacks the specific instructions and context needed for the agent to effectively guide users through flight searches, hotel bookings, and itinerary planning.
    * **Solution:** Craft a detailed and context-aware System Prompt:

    ```
    You are a travel booking agent. Your primary goal is to assist users in planning and booking their trips.
    - Guide them through searching for flights, finding accommodations, and exploring destinations.
    - Be polite, patient, and offer travel recommendations based on their preferences.
    - Utilize available tools to access flight data, hotel availability, and destination information.
    ```

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/condition-agent-node.md">
---
description: Understanding the Condition Agent Node in Sequential Agents
---

# Condition Agent Node

The Condition Agent Node is a specialized node that utilizes an LLM Agent to make nuanced, context-aware decisions about the conversation flow. Unlike the Condition Node, which uses static JavaScript expressions, the Condition Agent Node leverages the reasoning capabilities of a language model to dynamically analyze the conversation, extract insights, and determine the appropriate branch for the workflow.

<!-- ![](../../assets/seq-16.png) -->

## Understanding the Condition Agent Node

The Condition Agent Node combines aspects of the Agent Node and the Condition Node, offering a higher level of cognitive processing for branching decisions. It's particularly valuable when:

-   The decision-making criteria are complex or subjective, requiring nuanced understanding of language and context
-   The rules for branching cannot be easily expressed as simple conditional statements
-   The decision should take into account the full conversation history and subtle user cues
-   Human-like reasoning is needed to determine the best path forward

For example, a Condition Agent Node might assess whether a customer's issue requires escalation to a supervisor, determine which product category a user is most interested in, or decide if a user's query is related to sales or technical support.

## Inputs

|                               | Required | Description                                                                                                                                                                          |
| ----------------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| System Prompt                 | **Yes**  | A text prompt that defines the Agent's role in making the decision. It should clearly outline the decision-making criteria and the types of outputs expected.                        |
| Tools                         | No       | The Tool Nodes that the Condition Agent Node can access and execute, each offering specific functionality to retrieve information or perform actions that might aid in the decision. |
| Chat Model                    | No       | A custom Chat Model (LLM) to use instead of the default one defined in the Start Node.                                                                                               |
| JSON Schema                   | No       | A schema defining the structure of the Condition Agent Node's response when JSON Structured Output is enabled.                                                                       |
| Enable JSON Structured Output | No       | A configuration to force the Condition Agent Node's response to follow a predefined JSON schema.                                                                                     |
| Update State                  | No       | A JSON object that defines how the Condition Agent Node should update the custom State before passing it to the next node.                                                           |

## Outputs

The Condition Agent Node has two possible output connections:

-   **True:** This path is followed when the agent determines the condition is true.
-   **False:** This path is followed when the agent determines the condition is false.

## Features

### LLM-Powered Decision Making

The Condition Agent Node uses a language model to make decisions based on:

-   Natural language understanding of the conversation context
-   Reasoning and inference about user intentions, needs, or preferences
-   Pattern recognition in user behavior or conversation flow
-   Decision criteria outlined in the System Prompt

This allows for much more sophisticated branching logic than would be possible with simple conditional expressions.

### JSON Structured Output for Clear Decisions

To ensure reliable and consistent decision-making, the Condition Agent Node typically uses JSON Structured Output with a schema that explicitly captures the decision outcome. A simple schema might look like:

```json
{
    "type": "object",
    "properties": {
        "decision": {
            "type": "boolean",
            "description": "The decision outcome (true or false)"
        },
        "reasoning": {
            "type": "string",
            "description": "Explanation for why this decision was made"
        }
    },
    "required": ["decision", "reasoning"]
}
```

### State Updates for Contextual Information

In addition to making a decision, the Condition Agent Node can update the custom State to include its reasoning, extracted information, or other contextual details that might be useful for downstream nodes. This allows the decision-making process to contribute to the overall workflow, not just determine its path.

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Craft a clear and focused system prompt**

    Provide a well-defined persona and clear instructions to the agent in the System Prompt. This will guide its reasoning and help it generate relevant output for the conditional logic.

    **Structure output for reliable conditions**

    Use the JSON Structured Output feature to define a schema for the Condition Agent's output. This will ensure that the output is consistent and easily parsable, making it more reliable for use in conditional evaluations.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Unreliable routing due to unstructured output**

    * **Problem:** The Condition Agent's output is not properly structured or doesn't consistently provide a clear decision value, leading to unreliable or unpredictable routing in the workflow.
    * **Example:** The System Prompt asks the agent to "determine if the user is interested in premium products" without specifying how to format the output, resulting in verbose text responses that don't clearly indicate true or false.
    * **Solution:** Enable JSON Structured Output and define a clear schema with a boolean decision field. In the System Prompt, emphasize the importance of providing a definitive true/false decision based on specific criteria.

    **Ambiguous decision criteria**

    * **Problem:** The System Prompt doesn't provide clear criteria for the agent to make decisions, leading to inconsistent or subjective routing.
    * **Example:** The prompt simply states "route the conversation based on user needs" without defining how to identify different types of needs or which ones should route to which path.
    * **Solution:** Provide explicit decision criteria in the System Prompt, outlining the factors the agent should consider and how they should be weighted. For example: "Determine if the user needs technical support (route to TRUE) or has a billing inquiry (route to FALSE). Technical support queries typically mention product functionality, errors, or how-to questions, while billing inquiries involve payments, subscriptions, or pricing."

    **Overcomplex reasoning**

    * **Problem:** The agent engages in unnecessary reasoning or analysis, making the decision process slower and more prone to errors or inconsistencies.
    * **Example:** For a simple routing decision (e.g., "Is this a support request?"), the agent performs an exhaustive analysis of the conversation, considering multiple factors that aren't relevant to the decision.
    * **Solution:** Focus the System Prompt on the specific decision the agent needs to make, emphasizing efficiency and directness. Limit the scope of analysis to only the relevant factors. Consider using a simpler Condition Node if the decision criteria can be expressed as a straightforward conditional expression.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/condition-node.md">
---
description: Understanding the Condition Node in Sequential Agents
---

# Condition Node

The Condition Node is a **specialized node that enables branching logic in Sequential Agent workflows**. It evaluates a condition based on the current State (conversation history and custom State) and directs the flow to one of two possible output paths depending on whether the condition is true or false.

<!-- ![](../../assets/seq-15.png) -->

## Understanding the Condition Node

The Condition Node acts as a decision point in your workflow, allowing different execution paths based on:

-   The **content of the conversation** (from `state.messages`)
-   **Custom State variables** updated by previous nodes
-   **User inputs** or preferences
-   **Results of previous operations** or tool executions

For example, a Condition Node might check if the user has provided their contact information, if a certain product is in stock, or if a user's sentiment is positive or negative, and then direct the conversation flow accordingly.

## Inputs

|                 | Required | Description                                                                                                                                          |
| --------------- | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| Condition Value | **Yes**  | A **JavaScript expression** that will be evaluated to determine which branch to follow. This expression should evaluate to either `true` or `false`. |

## Outputs

The Condition Node has two possible output connections:

-   **True:** This path is followed when the condition evaluates to `true`.
-   **False:** This path is followed when the condition evaluates to `false`.

## Features

### JavaScript Expression Evaluation

The Condition Node uses JavaScript expression evaluation to determine which path to take. This provides tremendous flexibility, allowing you to:

-   **Access any part of the State** using dot notation (e.g., `state.userDetails.age > 18`)
-   **Perform comparisons** using operators (`===`, `>`, `<`, etc.)
-   **Use logical operators** for complex conditions (`&&`, `||`, `!`)
-   **Call JavaScript methods** on strings, arrays, or objects (e.g., `state.products.some(p => p.category === 'Electronics')`)

### Dynamic Workflow Routing

The Condition Node enables dynamic routing of the conversation flow, creating personalized user experiences based on various factors:

-   **User preferences or characteristics** stored in the custom State
-   **Conversation context** derived from the messages array
-   **External data** retrieved by Tool Nodes
-   **Progress markers** indicating completion of specific steps

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Clear condition naming**

    Use descriptive names for your conditions (e.g., "If user is under 18, then Policy Advisor Agent", "If order is confirmed, then End Node") to make your workflow easier to understand and debug.

    **Prioritize simple conditions**

    Start with simple conditions and gradually add complexity as needed. This makes your workflow more manageable and reduces the risk of errors.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Mismatched condition logic and workflow design**

    * **Problem:** The conditions you define in the Condition Node don't align with the actual structure or purpose of your workflow, leading to unexpected or illogical conversation paths.
    * **Example:** You create a condition that checks if the user's age is over 18, but the workflow doesn't collect age information in any preceding nodes, resulting in the condition always evaluating to `false`.
    * **Solution:** Ensure that the information needed for condition evaluation is collected or created in preceding nodes. Map out the data flow of your workflow to identify any missing links or incorrect assumptions.

    **Overly complex condition expressions**

    * **Problem:** The JavaScript expression used for the condition is too complex, making it difficult to understand, debug, or maintain.
    * **Example:** You write a multi-line nested condition with complex object traversals, array filters, and multiple logical operators, making it almost impossible to predict its behavior in all scenarios.
    * **Solution:** Break down complex conditions into simpler parts, potentially using multiple Condition Nodes in sequence. Consider moving complex logic to an LLM Node or Agent Node that can simplify the evaluation criteria and update the State accordingly.

    **Undefined or null value handling**

    * **Problem:** The condition doesn't account for undefined or null values, leading to unexpected or default behaviors.
    * **Example:** Your condition checks `state.userPreference.color === 'blue'` without first verifying if `state.userPreference` exists, potentially causing an error.
    * **Solution:** Include checks for undefined or null values in your conditions, or ensure that all necessary State properties are initialized with default values before they're accessed. For example: `state.userPreference && state.userPreference.color === 'blue'`.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/end-node.md">
---
description: Understanding the End Node in Sequential Agents
---

# End Node

The End Node is the **final node in the Sequential Agent workflow**, marking the conclusion of a conversation turn or the entire conversational flow. It receives all the accumulated context from previous nodes, including the conversation history and custom State, and provides the ultimate response back to the user.

<!-- ![](../../assets/seq-19.png) -->

## Understanding the End Node

The End Node serves as the termination point for the Sequential Agent workflow, signaling that all necessary processing for the current conversation turn has been completed. It consolidates the outputs from all preceding nodes and delivers the final response to the user.

Key aspects of the End Node include:

-   **Final response delivery:** The End Node sends the accumulated response back to the user, representing the culmination of all processing in the workflow.
-   **Conversation turn completion:** It marks the end of a conversation turn, allowing the workflow to reset and prepare for the next user input.
-   **State preservation:** While the conversation turn ends, the State (including conversation history and custom State) persists if an Agent Memory Node is connected to the Start Node, maintaining context across turns.

## Inputs

The End Node has no configurable inputs. It simply receives the complete State object, including conversation history and any custom State variables, from the preceding node in the workflow.

## Features

### Final Response Selection

The response delivered to the user by the End Node is determined by the last assistant message in the conversation history (`state.messages`). This message is typically generated by the last Agent Node or LLM Node that was executed before reaching the End Node.

### Multiple End Points

A Sequential Agent workflow can have multiple End Nodes, allowing different conversation paths to terminate independently. This enables:

-   **Different conclusion points** based on conversation flow
-   **Specialized response handling** for different branches of the workflow
-   **Varied termination conditions** depending on the user's needs or the workflow's purpose

### State Persistence

If the workflow includes an Agent Memory Node connected to the Start Node, the State (including conversation history and custom State) persists even after the End Node is reached. This allows the workflow to maintain context across multiple conversation turns, enabling:

-   **Continuous conversations** with full context preservation
-   **Progressive information gathering** across multiple interactions
-   **Personalization improvements** as the workflow learns more about the user

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Multiple End Nodes for clarity**

    Don't hesitate to use multiple End Nodes in complex workflows. Having dedicated End Nodes for different conversation paths can make your workflow more readable and easier to maintain.

    **Final State review**

    Before implementing an End Node, review the conversation history and State structure to ensure you're delivering the most appropriate final response to the user. Sometimes, an additional Agent Node or LLM Node before the End Node can provide a more cohesive summary or conclusion.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Missing End Node**

    * **Problem:** A Sequential Agent workflow is designed without an End Node, leaving the conversation without a proper conclusion point.
    * **Example:** You create a workflow that processes user requests but doesn't include an End Node, causing the system to continue waiting for additional node executions that will never occur.
    * **Solution:** Ensure every possible path through your workflow leads to an End Node. Use Condition Nodes or Condition Agent Nodes to route all possible conversation branches to appropriate conclusions.

    **Inappropriate final response**

    * **Problem:** The final response delivered to the user doesn't appropriately address their query or provide a satisfactory conclusion to the conversation.
    * **Example:** A multi-step process collects all necessary information from the user but then terminates with a generic "Thank you" message rather than confirming the completion of the requested action.
    * **Solution:** Review the complete conversation flow and ensure that the last Agent Node or LLM Node before each End Node generates an appropriate concluding message. Consider adding a final "summarizer" node before important End Nodes to ensure coherent closure.

    **Premature workflow termination**

    * **Problem:** The workflow reaches an End Node before completing all necessary processing or interactions.
    * **Example:** A condition incorrectly routes the conversation to an End Node before all required user information has been collected.
    * **Solution:** Review all conditions and branching logic to ensure End Nodes are only reached when appropriate. Consider adding state checks before critical End Nodes to verify that all required information has been gathered or processed.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/llm-node.md">
---
description: Understanding the LLM Node in Sequential Agents
---

# LLM Node

The LLM Node is a **specialized node that leverages language models to generate responses** based on conversation context. Unlike the Agent Node, the LLM Node does not make autonomous decisions about when to use tools or execute external actions. Instead, it focuses on generating high-quality text responses with flexible output formatting options.

<!-- ![](../../assets/seq-06.png) -->

## Understanding the LLM Node

The LLM Node processes input from previous nodes, including the conversation history `state.messages` and any custom State, and uses a language model to generate a response based on a System Prompt and optional configuration settings. It is particularly useful for:

-   **Generating responses when no external tools are required**
-   **Processing, analyzing, or transforming data** in the State
-   **Creating structured outputs** in JSON format for consumption by other nodes

## Inputs

|                               | Required | Description                                                                                                        |
| ----------------------------- | -------- | ------------------------------------------------------------------------------------------------------------------ |
| System Prompt                 | **Yes**  | Text prompt that **defines the LLM's role and response parameters**.                                               |
| Chat Model                    | No       | A custom **Chat Model (LLM)** to use instead of the default one defined in the Start Node.                         |
| JSON Schema                   | No       | A schema defining the structure of the LLM Node's response when JSON Structured Output is enabled.                 |
| Enable JSON Structured Output | No       | A configuration to **force the LLM Node's response to follow** a predefined JSON schema.                           |
| Update State                  | No       | A JSON object that defines **how the LLM Node should update the custom State** before passing it to the next node. |

## Outputs

The LLM Node can connect to the following nodes as outputs:

-   **Agent Node:** Connects to an Agent Node to continue the conversation with an agent that can access tools.
-   **LLM Node:** Routes the conversation flow to another LLM Node for additional processing or response generation.
-   **Condition Agent Node:** Connects to a Condition Agent Node to implement branching logic based on the agent's evaluation of the conversation.
-   **Condition Node:** Connects to a Condition Node to implement branching logic based on predefined conditions.
-   **Loop Node:** Connects to a Loop Node to implement repetitive processes based on specific conditions.
-   **End Node:** Connects to an End Node to conclude the conversational flow.

## Features

### JSON Structured Output

When the "Enable JSON Structured Output" option is enabled, the LLM Node can generate responses that conform to a predefined JSON schema. This is particularly useful when:

-   You need to extract **specific data points** from the LLM's analysis
-   Downstream nodes require **structured input** for processing
-   You want to ensure **consistent response formats** for various user queries

For example, if you're building a customer inquiry workflow, you might define a JSON schema that extracts the customer's issue type, urgency level, and relevant product information from their message.

### State Management

The LLM Node can update the custom State based on the insights or analysis it generates. This allows the LLM Node to pass valuable information to subsequent nodes in the workflow without exposing that information directly to the user.

For example, an LLM Node might analyze a customer's sentiment and add a "sentimentScore" to the custom State, which a Condition Node could then use to route the conversation to different support agents based on the detected sentiment.

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Clear system prompt**

    Craft a concise and unambiguous System Prompt that accurately reflects the LLM Node's role and capabilities. This guides the LLM Node's decision-making and ensures it acts within its defined scope.

    **Optimize for structured output**

    Keep your JSON schemas as straightforward as possible, focusing on the essential data elements. Only enable JSON Structured Output when you need to extract specific data points from the LLM's response or when downstream nodes require JSON input.

    **Strategic tool selection**

    Choose and configure the tools available to the LLM Node (via the Tool Node), based on the specific tasks it needs to perform. Avoid providing too many tools, which can lead to confusion and inefficiency.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Overly complex JSON schemas**

    * **Problem:** The JSON schema defined for the LLM Node's output is unnecessarily complex, making it difficult for the LLM to generate responses that match the schema or for downstream nodes to process the output efficiently.
    * **Example:** You create a schema with many nested objects, optional fields, and arrays of complex objects, leading to inconsistent or incorrect outputs and difficulties in processing the data.
    * **Solution:** Simplify your JSON schemas to focus on the essential data points. Define clear, flat structures whenever possible, and only use nested objects or arrays when necessary.

    **Inadequate system prompting**

    * **Problem:** The System Prompt doesn't provide enough guidance for the LLM to generate the desired output, especially when trying to produce structured data or provide a specific type of analysis.
    * **Example:** Your prompt simply says "Analyze the user's message," which doesn't specify what kind of analysis to perform or what output format to use.
    * **Solution:** Be specific about what you want the LLM to analyze, how it should present its findings, and any constraints it should consider. For example: "Analyze the user's message for signs of customer dissatisfaction. Identify the specific product or service mentioned, the nature of the issue, and the customer's emotional state, then provide a concise summary."

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/loop-node.md">
---
description: Understanding the Loop Node in Sequential Agents
---

# Loop Node

The Loop Node is a **specialized node that enables repetitive processes** within the Sequential Agent workflow, allowing a sequence of nodes to be executed multiple times until a specific condition is met. This enables iterative refinement, information gathering, or step-by-step task completion.

<!-- ![](../../assets/seq-11.png) -->

## Understanding the Loop Node

The Loop Node acts as a control mechanism, creating a cyclic path within the graph that can be traversed multiple times. It evaluates a condition each time and determines whether to:

-   **Continue the loop:** Proceed to the nodes within the loop for another iteration
-   **Exit the loop:** Move on to the node connected to the "Exit" output when a termination condition is met

This functionality is particularly valuable for workflows that require:

-   **Incremental problem-solving:** Breaking down complex tasks into smaller, repeated steps
-   **Information gathering:** Collecting multiple pieces of information through a series of related questions
-   **Refinement processes:** Iteratively improving a response or output until it meets certain criteria
-   **Hierarchical conversations:** Exploring topics in depth before returning to higher-level discussion

## Inputs

|                | Required | Description                                                                                                                                                                                                                                  |
| -------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Loop Condition | **Yes**  | A **JavaScript expression** that will be evaluated at the beginning of each loop iteration to determine whether to continue looping or exit. This expression should evaluate to either `true` (continue looping) or `false` (exit the loop). |
| Max Iterations | No       | An optional **maximum number of iterations** to prevent infinite loops. If not specified, a default limit is applied as a safety measure.                                                                                                    |

## Outputs

The Loop Node has two possible output connections:

-   **Loop:** This path is followed when the loop condition evaluates to `true`, directing the flow to the nodes within the loop.
-   **Exit:** This path is followed when the loop condition evaluates to `false` or the maximum number of iterations is reached, directing the flow to continue beyond the loop.

## Features

### Conditional Looping with JavaScript Expressions

Like the Condition Node, the Loop Node uses JavaScript expression evaluation to determine whether to continue looping. This provides tremendous flexibility, allowing you to:

-   **Access any part of the State** using dot notation (e.g., `state.itemsToProcess.length > 0`)
-   **Track iteration counts** using custom State variables (e.g., `state.iterationCount < 5`)
-   **Check for completion criteria** in the custom State (e.g., `!state.allRequiredInfoCollected`)
-   **Examine conversation history** to make loop decisions (e.g., `state.messages.find(m => m.includes("stop"))`)

### Iteration Safeguards

To prevent infinite loops, the Loop Node includes safeguards:

-   **Explicit maximum iterations:** Allows you to set a specific limit based on your workflow's needs
-   **Default maximum:** Applies a reasonable limit if none is specified
-   **Early exit:** Allows breaking out of the loop based on the condition, even before reaching the maximum iterations

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Clear loop purpose**

    Define a clear purpose for each loop in your workflow. If possible, document with a sticky note what you're trying to achieve with the loop.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Confusing workflow structure**

    * **Problem:** Excessive or poorly designed loops make the workflow difficult to understand and maintain.
    * **Example:** You use multiple nested loops without clear purpose or labels, making it hard to follow the flow of the conversation.
    * **Solution:** Use loops sparingly and only when necessary. Clearly document each loop's purpose and termination condition. Consider using descriptive names or annotations to indicate the loop's function in the workflow.

    **Infinite or excessive looping**

    * **Problem:** The loop condition never evaluates to `false`, or the condition isn't correctly set up to terminate the loop appropriately, leading to repetitive or stuck conversations.
    * **Example:** Your loop condition checks for a specific value in the custom State, but that value is never updated within the loop, causing the workflow to reach the maximum iteration limit repeatedly.
    * **Solution:** Ensure that the loop condition is based on values that are updated within the loop's execution path. Set appropriate maximum iteration limits based on the expected loop behavior. Include debugging state variables to track the loop's progress and help diagnose issues.

    **State management gaps**

    * **Problem:** The custom State isn't properly updated within the loop, leading to incorrect loop behavior or loss of information.
    * **Example:** You collect user preferences in each loop iteration, but overwrite previous preferences instead of appending to a list, resulting in only the last preference being saved.
    * **Solution:** Carefully plan how the State will be modified in each iteration. Use appropriate data structures (arrays, objects) to accumulate information across iterations. Consider adding timestamp or sequence information to track the order of updates.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/README.md">
---
description: Learn the Fundamentals of Sequential Agents in Flowise
sidebar_position: 1
---

# Sequential Agents

This guide offers a complete overview of the Sequential Agent AI system architecture within Flowise, exploring its core components and workflow design principles.

:::warning
**Disclaimer**: This documentation is intended to help Flowise users understand and build conversational workflows using the Sequential Agent system architecture. It is not intended to be a comprehensive technical reference for the LangGraph framework and should not be interpreted as defining industry standards or core LangGraph concepts.
:::

## Concept

Built on top of [LangGraph](https://www.langchain.com/langgraph), Flowise's Sequential Agents architecture facilitates the **development of conversational agentic systems by structuring the workflow as a directed cyclic graph (DCG)**, allowing controlled loops and iterative processes.

This graph, composed of interconnected nodes, defines the sequential flow of information and actions, enabling the agents to process inputs, execute tasks, and generate responses in a structured manner.

### Understanding Sequential Agents' DCG Architecture

This architecture simplifies the management of complex conversational workflows by defining a clear and understandable sequence of operations through its DCG structure.

Let's explore some key elements of this approach:

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="core" label="Core Principles">
    * **Node-based processing:** Each node in the graph represents a discrete processing unit, encapsulating its own functionality like language processing, tool execution, or conditional logic.
    * **Data flow as connections:** Edges in the graph represent the flow of data between nodes, where the output of one node becomes the input for the subsequent node, enabling a chain of processing steps.
    * **State management:** State is managed as a shared object, persisting throughout the conversation. This allows nodes to access relevant information as the workflow progresses.
  </TabItem>
  <TabItem value="terminology" label="Terminology">
    * **Flow:** The movement or direction of data within the workflow. It describes how information passes between nodes during a conversation.
    * **Workflow:** The overall design and structure of the system. It's the blueprint that defines the sequence of nodes, their connections, and the logic that orchestrates the conversation flow.
    * **State:** A shared data structure that represents the current snapshot of the conversation. It includes the conversation history `state.messages` and any custom State variables defined by the user.
    * **Custom State:** User-defined key-value pairs added to the state object to store additional information relevant to the workflow.
    * **Tool:** An external system, API, or service that can be accessed and executed by the workflow to perform specific tasks, such as retrieving information, processing data, or interacting with other applications.
    * **Human-in-the-Loop (HITL):** A feature that allows human intervention in the workflow, primarily during tool execution. It enables a human reviewer to approve or reject a tool call before it's executed.
    * **Parallel node execution:** It refers to the ability to execute multiple nodes concurrently within a workflow by using a branching mechanism. This means that different branches of the workflow can process information or interact with tools simultaneously, even though the overall flow of execution remains sequential.
  </TabItem>
</Tabs>

## Key Components of Sequential Agents

Sequential Agents bring a whole new dimension to Flowise, **introducing 10 specialized nodes**, each serving a specific purpose, offering more control over how our conversational agents interact with users, process information, make decisions, and execute actions.

-   **Start Node:** The entry point for all Sequential Agent workflows
-   **Agent Memory Node:** Enables persistence of conversation state across multiple interactions
-   **State Node:** Creates and manages custom state variables
-   **Agent Node:** A core processing unit that can interact with tools and generate responses
-   **LLM Node:** Provides structured outputs and specialized language processing
-   **Tool Node:** Connects to external systems and APIs
-   **Condition Node:** Implements branching logic using simple conditions
-   **Condition Agent Node:** Uses AI reasoning for advanced branching decisions
-   **Loop Node:** Creates iterative processes within the workflow
-   **End Node:** Marks the termination of a conversation flow

## Advantages of Sequential Agents

Sequential Agent systems offer several distinct advantages for complex workflows:

-   **Fine-grained control:** Direct access to every step of the conversation flow
-   **Explicit state management:** Custom state variables for tracking conversation context
-   **Parallel execution:** Multiple branches of processing can run simultaneously
-   **Human-in-the-Loop:** Built-in support for human review and approval of critical actions
-   **Dynamic branching:** Sophisticated condition-based routing of conversation flows
-   **Iterative processing:** Loop-based refinement of outputs and multi-step processes

## Getting Started

To begin working with Sequential Agents, you'll need to understand the function and configuration of each node type. The following sections provide detailed documentation for each component of the Sequential Agent architecture.
</file>

<file path="sidekick-studio/agentflows/sequential-agents/start-node.md">
---
description: Understanding the Start Node in Sequential Agents
---

# Start Node

As its name implies, the Start Node is the **entry point for all workflows in the Sequential Agent architecture**. It receives the initial user query, initializes the conversation State, and sets the flow in motion.

<!-- ![](../../assets/seq-02.png) -->

## Understanding the Start Node

The Start Node ensures that our conversational workflows have the necessary setup and context to function correctly. **It's responsible for setting up key functionalities** that will be used throughout the rest of the workflow:

-   **Defining the default LLM:** The Start Node requires us to specify a Chat Model (LLM) compatible with function calling, enabling agents in the workflow to interact with tools and external systems. It will be the default LLM used under the hood in the workflow.
-   **Initializing Memory:** We can optionally connect an Agent Memory Node to store and retrieve conversation history, enabling more context-aware responses.
-   **Setting a custom State:** By default, the State contains an immutable `state.messages` array, which acts as the transcript or history of the conversation between the user and the agents. The Start Node allows you to connect a custom State to the workflow adding a State Node, enabling the storage of additional information relevant to your workflow
-   **Enabling moderation:** Optionally, we can connect Input Moderation to analyze the user's input and prevent potentially harmful content from being sent to the LLM.

## Inputs

|                   | Required | Description                                                                                                                                     |
| ----------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| Chat Model        | **Yes**  | The default LLM that will power the conversation. Only compatible with **models that are capable of function calling**.                         |
| Agent Memory Node | No       | Connect an Agent Memory Node to **enable persistence and context preservation**.                                                                |
| State Node        | No       | Connect a State Node to **set a custom State**, a shared context that can be accessed and modified by other nodes in the workflow.              |
| Input Moderation  | No       | Connect a Moderation Node to **filter content** by detecting text that could generate harmful output, preventing it from being sent to the LLM. |

## Outputs

The Start Node can connect to the following nodes as outputs:

-   **Agent Node:** Routes the conversation flow to an Agent Node, which can then execute actions or access tools based on the conversation's context.
-   **LLM Node:** Routes the conversation flow to an LLM Node for processing and response generation.
-   **Condition Agent Node:** Connects to a Condition Agent Node to implement branching logic based on the agent's evaluation of the conversation.
-   **Condition Node:** Connects to a Condition Node to implement branching logic based on predefined conditions.

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Choose the right Chat Model**

    Ensure your selected LLM supports function calling, a key feature for enabling agent-tool interactions. Additionally, choose an LLM that aligns with the complexity and requirements of your application. You can override the default LLM by setting it at the Agent/LLM/Condition Agent node level when necessary.

    **Consider context and persistence**

    If your use case demands it, utilize Agent Memory Node to maintain context and personalize interactions.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Incorrect Chat Model (LLM) selection**

    * **Problem:** The Chat Model selected in the Start Node is not suitable for the intended tasks or capabilities of the workflow, resulting in poor performance or inaccurate responses.
    * **Example:** A workflow requires a Chat Model with strong summarization capabilities, but the Start Node selects a model optimized for code generation, leading to inadequate summaries.
    * **Solution:** Choose a Chat Model that aligns with the specific requirements of your workflow. Consider the model's strengths, weaknesses, and the types of tasks it excels at. Refer to the documentation and experiment with different models to find the best fit.

    **Overlooking Agent Memory Node configuration**

    * **Problem:** The Agent Memory Node is not properly connected or configured, resulting in the loss of conversation history data between sessions.
    * **Example:** You intend to use persistent memory to store user preferences, but the Agent Memory Node is not connected to the Start Node, causing preferences to be reset on each new conversation.
    * **Solution:** Ensure that the Agent Memory Node is connected to the Start Node and configured with the appropriate database (SQLite). For most use cases, the default SQLite database will be sufficient.

    **Inadequate Input Moderation**

    * **Problem:** The "Input Moderation" is not enabled or configured correctly, allowing potentially harmful or inappropriate user input to reach the LLM and generate undesirable responses.
    * **Example:** A user submits offensive language, but the input moderation fails to detect it or is not set up at all, allowing the query to reach the LLM.
    * **Solution:** Add and configure an input moderation node in the Start Node to filter out potentially harmful or inappropriate language. Customize the moderation settings to align with your specific requirements and use cases.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/state-node.md">
---
description: Understanding the State Node in Sequential Agents
---

# State Node

The State Node, which can only be connected to the Start Node, **provides a mechanism to set a user-defined or custom State** into our workflow from the start of the conversation. This custom State is a JSON object that is shared and can be updated by nodes in the graph, passing from one node to another as the flow progresses.

<!-- ![](../../assets/seq-04.png) -->

## Understanding the State Node

By default, the State includes a `state.messages` array, which acts as our conversation history. This array stores all messages exchanged between the user and the agents, or any other actors in the workflow, preserving it throughout the workflow execution.

Since by definition this `state.messages` cannot be directly manipulated or overwritten, the **State Node allows us to** augment this default State **by adding our own custom attributes** to the top level of the State object (alongside `state.messages`), enabling us to track and manage workflow-specific information.

**For example**, let's consider a workflow where we want to track the user's language preferences, usage statistics, and product inquiries:

```
{
  "messages": [...],  // Conversation history - implicit
  "userSettings": {   // Custom State - defined by the State Node
    "preferredLanguage": "English",
    "productInquiries": ["Laptop", "Smartphone"],
    "usageMetrics": {
      "totalInteractions": 5,
      "lastInteractionDate": "2023-11-20"
    }
  }
}
```

In addition to the conversation history in the `messages` array, our workflow now has access to the custom State properties encapsulated under the `userSettings` object.

## Inputs

|               | Required | Description                                                                                   |
| ------------- | -------- | --------------------------------------------------------------------------------------------- |
| Initial State | **Yes**  | A JSON object that defines the custom State that will be **added to the conversation state**. |

## Outputs

The State Node has no direct outputs. Instead, it connects to the Start Node as its input, making the custom State available to all subsequent nodes in the workflow.

## Dynamics of the State

Think of the State as a package of information that **flows through the nodes in our Sequential Agent workflow**. Each node can inspect the package and either:

1. **Use the information** contained in the State to make decisions or generate responses, or
2. **Update the information** by modifying existing properties or adding new ones, all while preserving the `messages` array.

As the State passes from one node to the next, these modifications are carried forward, enabling cumulative and progressive data management throughout the workflow. For example, an Agent Node might add interview responses to the State, an LLM Node can analyze that information, and a Condition Node can use it to determine the next steps.

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Use structured data**

    Design your custom State with well-organized, nested objects rather than flat key-value pairs. This approach promotes clarity and scalability, making it easier to manage and evolve your State as your workflow grows more complex.

    **Consistent naming conventions**

    Adopt clear, descriptive naming conventions for your State properties. This enhances readability and helps maintain consistency throughout your workflow, especially when multiple team members are involved.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Setting unnecessarily complex initial state**

    * **Problem:** The initial State defined in the State Node is overly complex or contains redundant information, leading to confusion, processing overhead, and difficulty in maintaining the workflow.
    * **Example:** You create a State with deeply nested objects and arrays that are not actually needed by any nodes in the workflow, complicating debugging and increasing the risk of errors.
    * **Solution:** Start with a minimalist approach to your State design. Define only the essential properties that your workflow needs and gradually expand as required. Regularly review your State structure to identify and remove unused or redundant properties.

    **Not planning for state updates**

    * **Problem:** Initial State is defined without considering how and where it will be updated, leading to inconsistent or unexpected behaviors in the workflow.
    * **Example:** You define a `userPreferences` object in the initial State but don't properly plan which nodes will update this object or how updates will be made, resulting in overwritten preferences or conflicts.
    * **Solution:** Map out the complete lifecycle of each State property, including where it's initialized, updated, and consumed. Document which nodes are responsible for updating specific parts of the State to avoid conflicts and ensure consistency. Consider using more granular objects or adding version tracking to manage updates more effectively.

    **Overwriting or overriding critical properties**

    * **Problem:** Custom State properties unintentionally override or conflict with system-defined properties or functions, causing the workflow to malfunction.
    * **Example:** You define a `messages` property in your custom State, which conflicts with the system's `state.messages` array, leading to loss of conversation history or errors in message handling.
    * **Solution:** Avoid using property names that might conflict with system-defined properties (like `messages`). If your custom State needs to track message-related data, use a different name or nest it under a unique property. Review the system documentation to understand what property names are reserved or have special meanings.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/sequential-agents/tool-node.md">
---
description: Understanding the Tool Node in Sequential Agents
---

# Tool Node

The Tool Node is a **specialized node that enables workflow to interact with external systems**, retrieve information, or perform actions. Tool Nodes do not act independently. Instead, they serve as a bridge between your conversational workflow and external resources, allowing Agent Nodes to access and utilize them as needed.

<!-- ![](../../assets/sa-tool.png) -->

## Understanding the Tool Node

The Tool Node encapsulates external functionality or services that Agent Nodes can invoke to complete specific tasks. When an Agent Node determines that external assistance is needed to fulfill a user's request, it selects and invokes the appropriate Tool Node.

Tool Nodes are particularly useful for **enhancing the capabilities of Agent Nodes**, allowing them to:

-   **Retrieve information** from external sources (e.g., weather data, stock prices, product information)
-   **Perform actions** in external systems (e.g., booking a reservation, creating a ticket, sending an email)
-   **Process data** in specialized ways (e.g., analyzing sentiment, generating images, translating text)

## Inputs

|                  | Required | Description                                                                                                   |
| ---------------- | -------- | ------------------------------------------------------------------------------------------------------------- |
| Tool             | **Yes**  | The **specific tool or external service** functionality the node will provide.                                |
| JSON Schema      | No       | A schema defining the structure of the Tool Node's output.                                                    |
| Require Approval | No       | A configuration to **enable Human-in-the-Loop (HITL)**, requiring human approval before the tool is executed. |

## Features

### Human-in-the-Loop (HITL)

One of the most powerful features of the Tool Node is its support for **Human-in-the-Loop (HITL)** functionality. When the "Require Approval" option is enabled, every time the tool is invoked by an Agent Node, the execution pauses and waits for human approval. This feature is particularly valuable for:

-   **Critical operations** where errors could have significant consequences
-   **Security or compliance requirements** that necessitate human oversight
-   **Costly operations** where unauthorized use could lead to financial impact
-   **Learning scenarios** where human feedback improves the system's performance over time

### Tool Integration

Tool Nodes can integrate with a wide variety of external systems and services through various mechanisms:

-   **API integration:** Connecting to web APIs to access data or functionality
-   **Database connections:** Retrieving or storing data in databases
-   **Webhooks:** Triggering actions in external systems
-   **Custom code execution:** Running custom code to perform specialized operations

## Available Tools

Flowise provides a range of built-in tools that can be used with Tool Nodes, including:

-   **Search Tools:** Web search, Wikipedia lookup, etc.
-   **Data Retrieval Tools:** Database queries, file system access, etc.
-   **Communication Tools:** Email sending, SMS notifications, etc.
-   **AI Service Tools:** Image generation, audio processing, etc.
-   **Custom Integration Tools:** Connects to custom APIs and services

## Best Practices

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="pro-tips" label="Pro Tips">
    **Clear tool naming**

    Name your tools in a way that clearly communicates their functionality to the Agent Node. For instance, if you create a weather information tool, instead of naming it "Lookup" or "API," use a descriptive name like "Get Current Weather" to help the Agent make better decisions about tool selection.

    **Precise input specifications**

    When configuring tools, especially those with API connections, define clear and precise input specifications. This helps the Agent understand what information it needs to provide for the tool to execute successfully.

  </TabItem>
  <TabItem value="pitfalls" label="Potential Pitfalls">
    **Inadequate tool descriptions**

    * **Problem:** The tool's description doesn't provide enough information for the Agent to understand when or how to use it, leading to incorrect or inefficient tool selection.
    * **Example:** A tool is simply described as "Get Data" without specifying what kind of data it retrieves or what parameters it requires.
    * **Solution:** Provide clear, detailed descriptions for each tool, specifying what it does, when it should be used, what inputs it requires, and what outputs it provides.

    **Over-reliance on HITL**

    * **Problem:** Enabling "Require Approval" for all Tool Nodes creates excessive interruptions in the workflow, leading to a poor user experience and inefficient operations.
    * **Example:** Even simple, low-risk operations like retrieving public weather data require human approval, slowing down the conversation unnecessarily.
    * **Solution:** Reserve HITL for critical or sensitive operations where human oversight provides meaningful value. For routine, low-risk operations, consider allowing automated execution.

    **Insufficient error handling**

    * **Problem:** The Tool Node doesn't properly handle errors or exceptions from the external system, leading to cryptic error messages or workflow failures.
    * **Example:** When an API returns an error, the raw error message is displayed to the user instead of a friendly explanation of what went wrong.
    * **Solution:** Implement robust error handling in your Tool Nodes, with clear, user-friendly messages for common error scenarios. Consider fallback options for critical operations.

  </TabItem>
</Tabs>
</file>

<file path="sidekick-studio/agentflows/multi-agents.md">
---
description: Understanding Multi-Agent Systems in Flowise
sidebar_position: 2
---

# Multi-Agent Systems

Multi-Agent systems in Flowise provide a high-level, collaborative approach to solving complex tasks by dividing them among specialized agents working under the supervision of a coordinator.

## Understanding Multi-Agent Systems

Multi-Agent systems employ a hierarchical structure where a supervisor agent manages and delegates tasks to specialized worker agents. This architecture excels at breaking down complex problems into manageable subtasks that can be addressed sequentially.

## Key Components

### Supervisor Agent

The Supervisor Agent acts as the central coordinator of the Multi-Agent system. It:

-   Analyzes user requests to determine what needs to be accomplished
-   Breaks complex tasks into smaller, manageable subtasks
-   Delegates these subtasks to the most appropriate Worker Agents
-   Synthesizes the results from Worker Agents into coherent responses
-   Maintains the overall context and flow of the conversation

### Worker Agents

Worker Agents are specialized agents designed to handle specific types of tasks or domains. Each Worker Agent:

-   Focuses on a particular skillset or knowledge area
-   Receives tasks from the Supervisor Agent
-   May access specialized tools relevant to their domain
-   Completes their assigned subtask and returns results to the Supervisor
-   Operates independently without awareness of other Workers

## Benefits of Multi-Agent Systems

Multi-Agent systems offer several advantages for certain types of workflows:

-   **Simplicity:** Provides an intuitive, task-based approach to workflow design
-   **Specialization:** Allows for creating highly specialized agents optimized for particular tasks
-   **Isolation:** Each Worker Agent operates independently, reducing cognitive load and complexity
-   **Linear Execution:** Tasks are completed in sequence, ensuring orderly processing
-   **Abstraction:** Handles many complex details automatically, allowing focus on high-level workflow design

## Limitations

While powerful, Multi-Agent systems do have some limitations:

-   Limited support for parallel execution of tasks
-   No built-in support for Human-in-the-Loop (HITL) review
-   Less granular control over conversation flow compared to Sequential Agents
-   Implicit rather than explicit State management

## Ideal Use Cases

Multi-Agent systems are particularly well-suited for:

-   Customer service workflows that follow predictable patterns
-   Information retrieval and summarization tasks
-   Linear processes that can be broken down into sequential steps
-   Applications where simplicity of design is a priority
-   Workflows that don't require parallel execution or complex branching

## Getting Started

To create a Multi-Agent system in Flowise, you'll work with the Supervisor Agent node and one or more Worker Agent nodes. Each Worker can be customized with specific tools, knowledge, and instructions appropriate to their assigned tasks.
</file>

<file path="sidekick-studio/agentflows/README.md">
---
description: Learn about the different Agent Flows available in Flowise
---

# Agent Flows (Depricated)

Flowise provides multiple approaches to building agent systems, each designed for specific use cases and offering different levels of control and abstraction. This guide explores the two main agent architectures: **Multi-Agents** and **Sequential Agents**.

## Understanding Agent Flows

Agent Flows in Flowise represent different patterns and architectures for constructing conversational AI systems. Each pattern offers a unique approach to managing conversation flow, tool usage, and decision-making processes.

## Multi-Agents vs Sequential Agents

While both Multi-Agent and Sequential Agent systems in Flowise are built upon the LangGraph framework and share the same fundamental principles, the Sequential Agent architecture provides a lower level of abstraction, offering more granular control over every step of the workflow.

|                          | Multi-Agent                                                                                                                                        | Sequential Agent                                                                                                                                                                                                             |
| ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Structure                | **Hierarchical**; Supervisor delegates to specialized Workers.                                                                                     | **Linear, cyclic and/or branching**; nodes connect in a sequence, with conditional logic for branching.                                                                                                                      |
| Workflow                 | Flexible; designed for breaking down a complex task into a **sequence of sub-tasks**, completed one after another.                                 | Highly flexible; **supports parallel node execution**, complex dialogue flows, branching logic, and loops within a single conversation turn.                                                                                 |
| Parallel Node Execution  | **No**; Supervisor handles one task at a time.                                                                                                     | **Yes**; can trigger multiple actions in parallel within a single run.                                                                                                                                                       |
| State Management         | **Implicit**; State is in place, but is not explicitly managed by the developer.                                                                   | **Explicit**; State is in place, and developers can define and manage an initial or custom State using the State Node and the "Update State" field in various nodes.                                                         |
| Tool Usage               | **Workers** can access and use tools as needed.                                                                                                    | Tools are accessed and executed through **Agent Nodes** and **Tool Nodes**.                                                                                                                                                  |
| Human-in-the-Loop (HITL) | HITL is **not supported.**                                                                                                                         | **Supported** through the Agent Node and Tool Node's "Require Approval" feature, allowing human review and approval or rejection of tool execution.                                                                          |
| Complexity               | Higher level of abstraction; **simplifies workflow design.**                                                                                       | Lower level of abstraction; **more complex workflow design**, requiring careful planning of node interactions, custom State management, and conditional logic.                                                               |
| Ideal Use Cases          |  Automating linear processes (e.g., data extraction, lead generation).<br/> Situations where sub-tasks need to be completed one after the other. |  Building conversational systems with dynamic flows.<br/> Complex workflows requiring parallel node execution or branching logic.<br/> Situations where decision-making is needed at multiple points in the conversation. |

## Choosing the Right System

Selecting the ideal system for your application depends on understanding your specific workflow needs. Factors like task complexity, the need for parallel processing, and your desired level of control over data flow are all key considerations.

-   **For simplicity:** If your workflow is relatively straightforward, where tasks can be completed one after the other and does not require parallel node execution or Human-in-the-Loop (HITL), the Multi-Agent approach offers ease of use and quick setup.
-   **For flexibility:** If your workflow needs parallel execution, dynamic conversations, custom State management, and the ability to incorporate HITL, the **Sequential Agent** approach provides the necessary flexibility and control.

:::info
**Note**: Even though Multi-Agent systems are technically a higher-level layer built upon the Sequential Agent architecture, they offer a distinct user experience and approach to workflow design. The comparison above treats them as separate systems to help you select the best option for your specific needs.
:::
</file>

<file path="sidekick-studio/agentflowsv2/README.md">
---
description: Learn how to build multi-agents system using Agentflow V2 for more powerful agents that can handle complex tasks over long periods of time
---

# Agent Flows V2

This guide explores the AgentFlow V2 architecture, detailing its core concepts, use cases, Flow State, and comprehensive node references.

:::warning
**Disclaimer:** This documentation describes AgentFlow V2 as of its current official release. Features, functionalities, and node parameters are subject to change in future updates and versions of AnswerAgent/Flowise. Please refer to the latest official release notes or in-app information for the most up-to-date details.
:::

## Core Concept

AgentFlow V2 represents a significant architectural evolution, introducing a new paradigm in AnswerAgent/Flowise that focuses on explicit workflow orchestration and enhanced flexibility. Unlike V1's primary reliance on external frameworks for its core agent graph logic, V2 shifts the focus towards designing the entire workflow using a granular set of specialized, standalone nodes developed natively as core AnswerAgent/Flowise components.

In this V2 architecture, each node functions as an independent unit, executing a discrete operation based on its specific design and configuration. The visual connections between nodes on the canvas explicitly define the workflow's path and control sequence, data can be passed between nodes by referencing the outputs of any previously executed node in the current flow, and the Flow State provides an explicit mechanism for managing and sharing data throughout the workflow.

V2 architecture implements a comprehensive node-dependency and execution queue system that precisely respects these defined pathways while maintaining clear separation between components, allowing workflows to become both more sophisticated and easier to design. This allow complex patterns like loops, conditional branching, human-in-the-loop interactions and others to be achievable. This makes it more adaptable to diverse use cases while remaining more maintainable and extensible.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-1-flow-types.png" alt="AgentFlow V2 Flow Types" /><figcaption></figcaption></figure>

## Difference between Agentflow and Automation Platform

One of the most asked question: What is the difference between Agentflow and automation platforms like n8n, Make, or Zapier?

###  **Agent-to-agent Communication**

Multimodal communication between agents is supported. A Supervisor agent can formulate and delegate tasks to multiple Worker agents, with outputs from the Worker agents subsequently returned to the Supervisor.

At each step, agents have access to the complete conversation history, enabling the Supervisor to determine the next task and the Worker agents to interpret the task, select appropriate tools, and execute actions accordingly.

This architecture enables **collaboration, delegation, and shared task management** across multiple agents, such capabilities are not typically offered by traditional automation tools.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-2-a2a.png" alt="Agent-to-Agent Communication" /><figcaption></figcaption></figure>

###  Human-in-the-loop

Execution is paused while awaiting human input, without blocking the running thread. Each checkpoint is saved, allowing the workflow to resume from the same point even after an application restart.

The use of checkpoints enables **long-running, stateful agents**.

Agents can also be configured to **request permission before executing tools**, similar to how Claude asks for user approval before using MCP tools. This helps prevent the autonomous execution of sensitive actions without explicit user approval.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-3-feedback.png" alt="Human-in-the-Loop Feedback" /><figcaption></figcaption></figure>

###  Shared State

Shared state enables data exchange between agents, especially useful for passing data across branches or non-adjacent steps in a flow. Refer to [Understanding Flow State](#understanding-flow-state)

###  Streaming

Supports Server-Sent Events (SSE) for real-time streaming of LLM or agent responses. Streaming also enables subscription to execution updates as the workflow progresses.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-4-streaming.png" alt="Streaming Support" /><figcaption></figcaption></figure>

###  MCP Tools

While traditional automation platforms often feature extensive libraries of pre-built integrations, Agentflow allows MCP ([Model Context Protocol](https://github.com/modelcontextprotocol)) tools to be connected as part of the workflow, rather than functioning solely as agent tools.

Custom MCPs can also be created independently, without depending on platform-provided integrations. MCP is widely considered an industry standard and is typically supported and maintained by the official providers. For example, the GitHub MCP is developed and maintained by the GitHub team, with similar support provided for Atlassian Jira, Brave Search, and others.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-5-mcp-tools.png" alt="MCP Tools Integration" /><figcaption></figcaption></figure>

## Agentflow V2 Node Reference

This section provides a detailed reference for each available node, outlining its specific purpose, key configuration parameters, expected inputs, generated outputs, and its role within the AgentFlow V2 architecture.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-6-node-ref.png" alt="AgentFlow V2 Node Reference" /><figcaption></figcaption></figure>

---

### **1. Start Node**

The designated entry point for initiating any AgentFlow V2 workflow execution. Every flow must begin with this node.

-   **Functionality:** Defines how the workflow is triggered and sets up the initial conditions. It can accept input either directly from the chat interface or through a customizable form presented to the user. It also allows for the initialization of `Flow State` variables at the beginning of the execution and can manage how conversation memory is handled for the run.
-   **Configuration Parameters**
    -   **Input Type**: Determines how the workflow execution is initiated, either by `Chat Input` from the user or via a submitted `Form Input`.
        -   **Form Title, Form Description, Form Input Types**: If `Form Input` is selected, these fields configure the appearance of the form presented to the user, allowing for various input field types with defined labels and variable names.
    -   **Ephemeral Memory**: If enabled, instructs the workflow to begin the execution without considering any past messages from the conversation thread, effectively starting with a clean memory slate.
    -   **Flow State**: Defines the complete set of initial key-value pairs for the workflow's runtime state `$flow.state`. All state keys that will be used or updated by subsequent nodes must be declared and initialized here.
-   **Inputs:** Receives the initial data that triggers the workflow, which will be either a chat message or the data submitted through a form.
-   **Outputs:** Provides a single output anchor to connect to the first operational node, passing along the initial input data and the initialized Flow State.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-7-start.png" alt="Start Node" width="343" /><figcaption></figcaption></figure>

---

### **2. LLM Node**

Provides direct access to a configured Large Language Model (LLM) for executing AI tasks, enabling the workflow to perform structured data extraction if needed.

-   **Functionality:** This node sends requests to an LLM based on provided instructions (Messages) and context. It can be used for text generation, summarization, translation, analysis, answering questions, and generating structured JSON output according to a defined schema. It has access to memory for the conversation thread and can read/write to the `Flow State`.
-   **Configuration Parameters**
    -   **Model**: Specifies the AI model from a chosen service  e.g., OpenAI's GPT-4o or Google Gemini.
    -   **Messages**: Define the conversational input for the LLM, structuring it as a sequence of roles  System, User, Assistant, Developer  to guide the AI's response. Dynamic data can be inserted using `{{ variable }}`.
    -   **Memory**: If enabled, determines if the LLM should consider the history of the current conversation thread when generating its response.
        -   **Memory Type, Window Size, Max Token Limit**: If memory is used, these settings refine how the conversation history is managed and presented to the LLM  for example, whether to include all messages, only a recent window of turns, or a summarized version.
        -   **Input Message**: Specifies the variable or text that will be appended as the most recent user message at the end of the existing conversation context  including initial context and memory  before being processed by the LLM/Agent.
    -   **Return Response As**: Configures how the LLM's output is categorized  as a `User Message` or `Assistant Message`  which can influence how it's handled by subsequent memory systems or logging.
    -   **JSON Structured Output**: Instructs the LLM to format its output according to a specific JSON schema  including keys, data types, and descriptions  ensuring predictable, machine-readable data.
    -   **Update Flow State**: Allows the node to modify the workflow's runtime state `$flow.state` during execution by updating pre-defined keys. This makes it possible, for example, to store this LLM node's output under such a key, making it accessible to subsequent nodes.
-   **Inputs:** This node utilizes data from the workflow's initial trigger or from the outputs of preceding nodes, incorporating this data into the `Messages` or `Input Message` fields. It can also retrieve values from `$flow.state` when input variables reference it.
-   **Outputs:** Produces the LLM's response, which will be either plain text or a structured JSON object. The categorization of this output  as User or Assistant  is determined by the `Return Response` setting.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-8-llm.png" alt="LLM Node" width="375" /><figcaption></figcaption></figure>

---

### **3. Agent Node**

Represents an autonomous AI entity capable of reasoning, planning, and interacting with tools or knowledge sources to accomplish a given objective.

-   **Functionality:** This node uses an LLM to dynamically decide a sequence of actions. Based on the user's goal  provided via messages/input  it can choose to use available Tools or query Document Stores to gather information or perform actions. It manages its own reasoning cycle and can utilize memory for the conversation thread and `Flow State`. Suitable for tasks requiring multi-step reasoning or interacting dynamically with external systems or tools.
-   **Configuration Parameters**
    -   **Model**: Specifies the AI model from a chosen service  e.g., OpenAI's GPT-4o or Google Gemini  that will drive the agent's reasoning and decision-making processes.
    -   **Messages**: Define the initial conversational input, objective, or context for the agent, structuring it as a sequence of roles  System, User, Assistant, Developer  to guide the agent's understanding and subsequent actions. Dynamic data can be inserted using `{{ variable }}`.
    -   **Tools**: Specify which pre-defined AnswerAgent/Flowise Tools the agent is authorized to use to achieve its goals.
        -   For each selected tool, an optional **Require Human Input flag** indicates if the tool's operation might itself pause to ask for human intervention.
    -   **Knowledge / Document Stores**: Configure access to information within AnswerAgent/Flowise-managed Document Stores.
        -   **Document Store**: Choose a pre-configured Document Store from which the agent can retrieve information. These stores must be set up and populated in advance.
        -   **Describe Knowledge**: Provide a natural language description of the content and purpose of this Document Store. This description guides the agent in understanding what kind of information the store contains and when it would be appropriate to query it.
    -   **Knowledge / Vector Embeddings**: Configure access to external, pre-existing vector stores as additional knowledge sources for the agent.
        -   **Vector Store**: Selects the specific, pre-configured vector database the agent can query.
        -   **Embedding Model**: Specifies the embedding model associated with the selected vector store, ensuring compatibility for queries.
        -   **Knowledge Name**: Assigns a short, descriptive name to this vector-based knowledge source, which the agent can use for reference.
        -   **Describe Knowledge**: Provide a natural language description of the content and purpose of this vector store, guiding the agent on when and how to utilize this specific knowledge source.
        -   **Return Source Documents**: If enabled, instructs the agent to include source document information with the data retrieved from the vector store.
    -   **Memory**: If enabled, determines if the agent should consider the history of the current conversation thread when making decisions and generating responses.
        -   **Memory Type, Window Size, Max Token Limit**: If memory is used, these settings refine how the conversation history is managed and presented to the agent  for example, whether to include all messages, only a recent window of turns, or a summarized version.
        -   **Input Message**: Specifies the variable or text that will be appended as the most recent user message at the end of the existing conversation context  including initial context and memory  before being processed by the LLM/Agent.
    -   **Return Response**: Configures how the agent's final output or message is categorized  as a User Message or Assistant Message  which can influence how it's handled by subsequent memory systems or logging.
    -   **Update Flow State**: Allows the node to modify the workflow's runtime state `$flow.state` during execution by updating pre-defined keys. This makes it possible, for example, to store this Agent node's output under such a key, making it accessible to subsequent nodes.
-   **Inputs:** This node utilizes data from the workflow's initial trigger or from the outputs of preceding nodes, often incorporated into the `Messages` or `Input Message` fields. It accesses the configured tools and knowledge sources as needed.
-   **Outputs:** Produces the final result or response generated by the agent after it has completed its reasoning, planning, and any interactions with tools or knowledge sources.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-9-agent.png" alt="Agent Node" width="375" /><figcaption></figcaption></figure>

---

### **4. Tool Node**

Provides a mechanism for directly and deterministically executing a specific, pre-defined AnswerAgent/Flowise Tool within the workflow sequence. Unlike the Agent node, where the LLM dynamically chooses a tool based on reasoning, the Tool node executes exactly the tool selected by the workflow designer during configuration.

-   **Functionality:** This node is used when the workflow requires the execution of a known, specific capability at a defined point, with readily available inputs. It ensures deterministic action without involving LLM reasoning for tool selection.
-   **How it Works**
    1. **Triggering:** When the workflow execution reaches a Tool node, it activates.
    2. **Tool Identification:** It identifies the specific AnswerAgent/Flowise Tool selected in its configuration.
    3. **Input Argument Resolution:** It looks at the Tool Input Arguments configuration. For each required input parameter of the selected tool.
    4. **Execution:** It invokes the underlying code or API call associated with the selected AnswerAgent/Flowise Tool, passing the resolved input arguments.
    5. **Output Generation:** It receives the result returned by the tool's execution.
    6. **Output Propagation:** It makes this result available via its output anchor for subsequent nodes to use.
-   **Configuration Parameters**
    -   **Tool Selection**: Choose the specific, registered AnswerAgent/Flowise Tool that this node will execute from a dropdown list.
    -   **Input Arguments**: Define how data from your workflow is supplied to the selected tool. This section dynamically adapts based on the chosen tool, presenting its specific required input parameters:
        -   **Map Argument Name**: For each input the selected tool requires (e.g., `input` for a Calculator), this field will show the expected parameter name as defined by the tool itself.
        -   **Provide Argument Value**: Set the value for that corresponding parameter, using a dynamic variable like `{{ previousNode.output }}`, `{{ $flow.state.someKey }}`, or by entering static text.
    -   **Update Flow State**: Allows the node to modify the workflow's runtime state `$flow.state` during execution by updating pre-defined keys. This makes it possible, for example, to store this Tool node's output under such a key, making it accessible to subsequent nodes.
-   **Inputs:** Receives necessary data for the tool's arguments via the `Input Arguments` mapping, sourcing values from previous node outputs, `$flow.state`, or static configurations.
-   **Outputs:** Produces the raw output generated by the executed tool  e.g., a JSON string from an API, a text result, or a numerical value.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-10-tool.png" alt="Tool Node" width="375" /><figcaption></figcaption></figure>

---

### **5. Retriever Node**

Performs targeted information retrieval from configured Document Stores.

-   **Functionality:** This node queries one or more specified Document Stores, fetching relevant document chunks based on semantic similarity. It's a focused alternative to using an Agent node when the only required action is retrieval and dynamic tool selection by an LLM is not necessary.
-   **Configuration Parameters**
    -   **Knowledge / Document Stores**: Specify which pre-configured and populated Document Store(s) this node should query to find relevant information.
    -   **Retriever Query**: Define the text query that will be used to search the selected Document Stores. Dynamic data can be inserted using `{{ variables }}`.
    -   **Output Format**: Choose how the retrieved information should be presented  either as plain `Text` or as `Text with Metadata`, which might include details like source document names or locations.
    -   **Update Flow State**: Allows the node to modify the workflow's runtime state `$flow.state` during execution by updating pre-defined keys. This makes it possible, for example, to store this Retriever node's output under such a key, making it accessible to subsequent nodes.
-   **Inputs:** Requires a query string  often supplied as a variable from a previous step or user input  and accesses the selected Document Stores for information.
-   **Outputs:** Produces the document chunks retrieved from the knowledge base, formatted according to the chosen `Output Format`.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-11-retriever.png" alt="Retriever Node" width="375" /><figcaption></figcaption></figure>

---

### 6. HTTP Node

Facilitates direct communication with external web services and APIs via the Hypertext Transfer Protocol (HTTP).

-   **Functionality:** This node enables the workflow to interact with any external system accessible via HTTP. It can send various types of requests (GET, POST, PUT, DELETE, PATCH) to a specified URL, allowing for integration with third-party APIs, fetching data from web resources, or triggering external webhooks. The node supports configuration of authentication methods, custom headers, query parameters, and different request body types to accommodate diverse API requirements.
-   **Configuration Parameters**
    -   **HTTP Credential**: Optionally select pre-configured credentials  such as Basic Auth, Bearer Token, or API Key  to authenticate requests to the target service.
    -   **Request Method**: Specify the HTTP method to be used for the request  e.g., `GET`, `POST`, `PUT`, `DELETE`, `PATCH`.
    -   **Target URL**: Define the complete URL of the external endpoint to which the request will be sent.
    -   **Request Headers**: Set any necessary HTTP headers as key-value pairs to be included in the request.
    -   **URL Query Parameters**: Define key-value pairs that will be appended to the URL as query parameters.
    -   **Request Body Type**: Choose the format of the request payload if sending data  options include `JSON`, `Raw text`, `Form Data`, or `x-www-form-urlencoded`.
    -   **Request Body**: Provide the actual data payload for methods like POST or PUT. The format should match the selected `Body Type`, and dynamic data can be inserted using `{{ variables }}`.
    -   **Response Type**: Specify how the workflow should interpret the response received from the server  options include `JSON`, `Text`, `Array Buffer`, or `Base64` for binary data.
-   **Inputs:** Receives configuration data such as the URL, method, headers, and body, often incorporating dynamic values from previous workflow steps or `$flow.state`.
-   **Outputs:** Produces the response received from the external server, parsed according to the selected `Response Type`.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-12-http.png" alt="HTTP Node" width="375" /><figcaption></figcaption></figure>

---

### **7. Condition Node**

Implements deterministic branching logic within the workflow based on defined rules.

-   **Functionality:** This node acts as a decision point, evaluating one or more specified conditions to direct the workflow down different paths. It compares input values  which can be strings, numbers, or booleans  using a variety of logical operators, such as equals, contains, greater than, or is empty. Based on whether these conditions evaluate to true or false, the workflow execution proceeds along one of the distinct output branches connected to this node.
-   **Configuration Parameters**
    -   **Conditions**: Configure the set of logical rules the node will evaluate.
        -   **Type**: Specify the type of data being compared for this rule  `String`, `Number`, or `Boolean`.
        -   **Value 1**: Define the first value for the comparison. Dynamic data can be inserted using `{{ variables }}`.
        -   **Operation**: Select the logical operator to apply between Value 1 and Value 2  e.g., `equal`, `notEqual`, `contains`, `larger`, `isEmpty`.
        -   **Value 2**: Define the second value for the comparison, if required by the chosen operation. Dynamic data can also be inserted here using `{{ variables }}`.
-   **Inputs:** Requires the data for `Value 1` and `Value 2` for each condition being evaluated. These values are supplied from previous node outputs or retrieved from `$flow.state`.
-   **Outputs:** Provides multiple output anchors, corresponding to the boolean outcome (true/false) of the evaluated conditions. The workflow continues along the specific path connected to the output anchor that matches the result.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-13-condition.png" alt="Condition Node" width="375" /><figcaption></figcaption></figure>

---

### **8. Condition Agent Node**

Provides AI-driven dynamic branching based on natural language instructions and context.

-   **Functionality:** This node uses a Large Language Model (LLM) to route the workflow. It analyzes provided input data against a set of user-defined "Scenarios"  potential outcomes or categories  guided by high-level natural language "Instructions" that define the decision-making task. The LLM then determines which scenario best fits the current input context. Based on this AI-driven classification, the workflow execution proceeds down the specific output path corresponding to the chosen scenario. This node is particularly useful for tasks like user intent recognition, complex conditional routing, or nuanced situational decision-making where simple, predefined rules  as in the Condition Node  are insufficient.
-   **Configuration Parameters**
    -   **Model**: Specifies the AI model from a chosen service that will perform the analysis and scenario classification.
    -   **Instructions**: Define the overall goal or task for the LLM in natural language  e.g., "Determine if the user's request is about sales, support, or general inquiry."
    -   **Input**: Specify the data, often text from a previous step or user input, using `{{ variables }}`, that the LLM will analyze to make its routing decision.
    -   **Scenarios**: Configure an array defining the possible outcomes or distinct paths the workflow can take. Each scenario is described in natural language  e.g., "Sales Inquiry," "Support Request," "General Question"  and each corresponds to a unique output anchor on the node.
-   **Inputs:** Requires the `Input` data for analysis and the `Instructions` to guide the LLM.
-   **Outputs:** Provides multiple output anchors, one for each defined `Scenario`. The workflow continues along the specific path connected to the output anchor that the LLM determines best matches the input.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-14-condition-agent.png" alt="Condition Agent Node" width="375" /><figcaption></figcaption></figure>

---

### **9. Iteration Node**

Executes a defined "sub-flow"  a sequence of nodes nested within it  for each item in an input array, implementing a "for-each" loop."

-   **Functionality:** This node is designed for processing collections of data. It takes an array, either provided directly or referenced via a variable, as its input. For every individual element within that array, the Iteration Node sequentially executes the sequence of other nodes that are visually placed inside its boundaries on the canvas.
-   **Configuration Parameters**
    -   **Array Input**: Specifies the input array that the node will iterate over. This is provided by referencing a variable that holds an array from a previous node's output or from the `$flow.state`  e.g., `{{ $flow.state.itemList }}`.
-   **Inputs:** Requires an array to be supplied to its `Array Input` parameter.
-   **Outputs:** Provides a single output anchor that becomes active only after the nested sub-flow has completed execution for all items in the input array. The data passed through this output can include aggregated results or the final state of variables modified within the loop, depending on the design of the sub-flow. Nodes placed inside the iteration block have their own distinct input and output connections that define the sequence of operations for each item.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-15-iteration.png" alt="Iteration Node" width="563" /><figcaption></figcaption></figure>

---

### **10. Loop Node**

Explicitly redirects the workflow execution back to a previously executed node.

-   **Functionality:** This node enables the creation of cycles or iterative retries within a workflow. When the execution flow reaches the Loop Node, it does not proceed forward to a new node; instead, it "jumps" back to a specified target node that has already been executed earlier in the current workflow run. This action causes the re-execution of that target node and any subsequent nodes in that part of the flow.
-   **Configuration Parameters**
    -   **Loop Back To**: Selects the unique ID of a previously executed node within the current workflow to which the execution should return.
    -   **Max Loop Count**: Defines the maximum number of times this loop operation can be performed within a single workflow execution, safeguarding against infinite cycles. The default value is 5.
-   **Inputs:** Receives the execution signal to activate. It internally tracks the number of times the loop has occurred for the current execution.
-   **Outputs:** This node does not have a standard forward-pointing output anchor, as its primary function is to redirect the execution flow backward to the `Loop Back To` target node, from where the workflow then continues.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-16-loop.png" alt="Loop Node" width="375" /><figcaption></figcaption></figure>

---

### **11. Human Input Node**

Pauses the workflow execution to request explicit input, approval, or feedback from a human user  a key component for Human-in-the-Loop (HITL) processes.

-   **Functionality:** This node halts the automated progression of the workflow and presents information or a question to a human user, via the chat interface. The content displayed to the user can either be a predefined, static text or dynamically generated by a LLM based on the current workflow context. The user is provided with distinct action choices  e.g., "Proceed," "Reject"  and, if enabled, a field to provide textual feedback. Once the user makes a selection and submits their response, the workflow resumes execution along the specific output path corresponding to their chosen action.
-   **Configuration Parameters**
    -   **Description Type**: Determines how the message or question presented to the user is generated  either `Fixed` (static text) or `Dynamic` (generated by an LLM).
        -   **If Description Type is `Fixed`**
            -   **Description**: This field contains the exact text to be displayed to the user. It supports the insertion of dynamic data using `{{ variables }}`
        -   **If `Description Type` is `Dynamic`**
            -   **Model**: Selects the AI model from a chosen service that will generate the user-facing message.
            -   **Prompt**: Provides the instructions or prompt for the selected LLM to generate the message shown to the user.
    -   **Feedback:** If enabled, the user will be prompted with a feedback window to leave their feedback, and this feedback will be appended to the node's output.
-   **Inputs:** Receives the execution signal to pause the workflow. It can utilize data from previous steps or `$flow.state` through variables in the `Description` or `Prompt` fields if configured for dynamic content.
-   **Outputs:** Provides two output anchors, each corresponding to a distinct user action  an anchor for "proceed" and another for "reject". The workflow continues along the path connected to the anchor matching the user's selection.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-17-human-input.png" alt="Human Input Node" width="375" /><figcaption></figcaption></figure>

---

### **12. Direct Reply Node**

Sends a final message to the user and terminates the current execution path.

-   **Functionality:** This node serves as an endpoint for a specific branch or the entirety of a workflow. It takes a configured message  which can be static text or dynamic content from a variable  and delivers it directly to the end-user through the chat interface. Upon sending this message, the execution along this particular path of the workflow concludes; no further nodes connected from this point will be processed.
-   **Configuration Parameters**
    -   **Message**: Define the text or variable `{{ variable }}` that holds the content to be sent as the final reply to the user.
-   **Inputs:** Receives the message content, which is sourced from a previous node's output or a value stored in `$flow.state`.
-   **Outputs:** This node has no output anchors, as its function is to terminate the execution path after sending the reply.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-18-direct-reply.png" alt="Direct Reply Node" width="375" /><figcaption></figcaption></figure>

---

### **13. Custom Function Node**

Provides a mechanism for executing custom server-side Javascript code within the workflow.

-   **Functionality:** This node allows to write and run arbitrary Javascript snippets, offering a efective way to implement complex data transformations, bespoke business logic, or interactions with resources not directly supported by other standard nodes. The executed code operates within a Node.js environment and has specific ways to access data:
    -   **Input Variables:** Values passed via the `Input Variables` configuration are accessible within the function, typically prefixed with `$`  e.g., if an input variable `userid` is defined, it can be accessed as `$userid`.
    -   **Flow Context:** Default flow configuration variables are available, such as `$flow.sessionId`, `$flow.chatId`, `$flow.chatflowId`, `$flow.input`  the initial input that started the workflow  and the entire `$flow.state` object.
    -   **Custom Variables:** Any custom variables set up in AnswerAgent/Flowise  e.g., `$vars.<variable-name>`.
    -   **Libraries:** The function can utilize any libraries that have been imported and made available within the AnswerAgent/Flowise backend environment.**The function must return a string value at the end of its execution**.
-   **Configuration Parameters**
    -   **Input Variables**: Configure an array of input definitions that will be passed as variables into the scope of your Javascript function. For each variable you wish to define, you will specify:
        -   **Variable Name**: The name you will use to refer to this variable within your Javascript code, typically prefixed with a `$`  e.g., if you enter `myValue` here, you might access it as `$myValue` in the script, corresponding to how input schema properties are mapped.
        -   **Variable Value**: The actual data to be assigned to this variable, which can be static text or, more commonly, a dynamic value sourced from the workflow  e.g., `{{ previousNode.output }}` or `{{ $flow.state.someKey }}`.
    -   **Javascript Function**: The code editor field where the server-side Javascript function is written. This function must ultimately return a string value.
    -   **Update Flow State**: Allows the node to modify the workflow's runtime state `$flow.state` during execution by updating pre-defined keys. This makes it possible, for example, to store this Custom Function node's string output under such a key, making it accessible to subsequent nodes.
-   **Inputs:** Receives data through the variables configured in `Input Variables`. Can also implicitly access elements of the `$flow` context and `$vars`.
-   **Outputs:** Produces the string value returned by the executed Javascript function.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-19-custom-function.png" alt="Custom Function Node" width="375" /><figcaption></figcaption></figure>

---

### **14. Execute Flow Node**

Enables the invocation and execution of another complete AnswerAgent/Flowise Chatflow or AgentFlow from within the current workflow.

-   **Functionality:** This node functions as a sub-workflow caller, promoting modular design and reusability of logic. It allows the current workflow to trigger a separate, pre-existing workflow  identified by its name or ID within the AnswerAgent/Flowise instance  pass an initial input to it, optionally override specific configurations of the target flow for that particular run, and then receive its final output back into the calling workflow to continue processing.
-   **Configuration Parameters**
    -   **Connect Credential**: Optionally provide Chatflow API credentials if the target flow being called requires specific authentication or permissions for execution.
    -   **Select Flow**: Specify the particular Chatflow or AgentFlow that this node will execute from the list of available flows in your AnswerAgent/Flowise instance.
    -   **Input**: Define the data  static text or `{{ variable }}`  that will be passed as the primary input to the target workflow when it is invoked.
    -   **Override Config**: Optionally provide a JSON object containing parameters that will override the default configuration of the target workflow specifically for this execution instance  e.g., temporarily changing a model or prompt used in the sub-flow.
    -   **Base URL**: Optionally specify an alternative base URL for the AnswerAgent/Flowise instance that hosts the target flow. This is useful in distributed setups or when flows are accessed via different routes, defaulting to the current instance's URL if not set.
    -   **Return Response As**: Determine how the final output from the executed sub-flow should be categorized when it's returned to the current workflow  as a `User Message` or `Assistant Message`.
    -   **Update Flow State**: Allows the node to modify the workflow's runtime state `$flow.state` during execution by updating pre-defined keys. This makes it possible, for example, to store this Execute Flow node's output under such a key, making it accessible to subsequent nodes.
-   **Inputs:** Requires the selection of a target flow and the `Input` data for it.
-   **Outputs:** Produces the final output returned by the executed target workflow, formatted according to the `Return Response As` setting.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-20-execute-flow.png" alt="Execute Flow Node" width="375" /><figcaption></figcaption></figure>

## Understanding Flow State

A key architectural feature enabling the flexibility and data management capabilities of AgentFlow V2 is the **Flow State**. This mechanism provides a way to manage and share data dynamically throughout the execution of a single workflow instance.

### **What is Flow State?**

-   Flow State (`$flow.state`) is a **runtime, key-value store** that is shared among the nodes in a single execution.
-   It functions as temporary memory or a shared context that exists only for the duration of that particular run/execution.

### **Purpose of Flow State**

The primary purpose of `$flow.state` is to enable **explicit data sharing and communication between nodes, especially those that may not be directly connected** in the workflow graph, or when data needs to be intentionally persisted and modified across multiple steps. It addresses several common orchestration challenges:

1. **Passing Data Across Branches:** If a workflow splits into conditional paths, data generated or updated in one branch can be stored in `$flow.state` to be accessed later if the paths merge or if other branches need that information.
2. **Accessing Data Across Non-Adjacent Steps:** Information initialized or updated by an early node can be retrieved by a much later node without needing to pass it explicitly through every intermediate node's inputs and outputs.

### **How Flow State Works**

1. **Initialization / Declaration of Keys**
    - All state keys that will be used throughout the workflow **must be initialized** with their default (even if empty) values using the `Flow State` parameter within the **Start node**. This step effectively declares the schema or structure of your `$flow.state` for that workflow. You define the initial key-value pairs here.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-21-flowstate.png" alt="Flow State Initialization" /><figcaption></figcaption></figure>

2. **Updating State / Modifying Existing Keys**

-   Many operational nodes  e.g., `LLM`, `Agent`, `Tool`, `HTTP`, `Retriever`, `Custom Function`  include an `Update Flow State` parameter in their configuration.
-   This parameter allows the node to **modify the values of pre-existing keys** within `$flow.state`.
-   The value can be static text, the direct output of the current node, output from previous node, and many other variables. Type `{{` will show all the available variables.
-   When the node executes successfully, it **updates** the specified key(s) in `$flow.state` with the new value(s). **New keys cannot be created by operational nodes; only pre-defined keys can be updated.**

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-22-update-flowstate.png" alt="Update Flow State" /><figcaption></figcaption></figure>

3. **Reading from State**

-   Any node input parameter that accepts variables can read values from the Flow State.
-   Use the specific syntax: `{{ $flow.state.yourKey }}`  replace `yourKey` with the actual key name that was initialized in the Start Node.
-   For example, an LLM node's prompt might include `"...based on the user status: {{ $flow.state.customerStatus }}"`.

<figure><img src="/.gitbook/assets/agentflowsv2/agentflowsv2-23-use-flowstate.png" alt="Using Flow State" /><figcaption></figcaption></figure>

### **Scope and Persistence:**

-   It is created and initialized when a workflow execution begins and is destroyed when that specific execution ends.
-   It does **not** persist across different user sessions or separate runs of the same workflow.
-   Each concurrent execution of the workflow maintains its own independent `$flow.state`.
</file>

<file path="sidekick-studio/chatflows/agents/openai-assistant/README.md">
---
description: An agent that uses OpenAI Assistant API to pick the tool and args to call.
---

# OpenAI Assistant

## Overview

The OpenAI Assistant feature in AnswerAgentAI allows users to import and utilize their OpenAI assistants within the AnswerAgentAI platform. This integration leverages the powerful Assistants API from OpenAI, enabling you to build sophisticated AI assistants tailored to your specific needs.

## Key Benefits

-   Seamless Integration: Import your existing OpenAI assistants directly into AnswerAgentAI.
-   Powerful Capabilities: Access models, tools, and files to respond to user queries effectively.
-   Versatile Tool Support: Utilize Code Interpreter, File Search, and Function calling tools.
-   Persistent Conversations: Benefit from OpenAI's Thread system for maintaining context in long interactions.

## How to Use

1. Create an OpenAI Assistant:

    - If you haven't already, create an assistant using the OpenAI platform or the Assistants API.
    - Configure your assistant with the desired instructions, models, and tools.

2. Import Your Assistant:

    - In the AnswerAgentAI Sidekick studio, locate the OpenAI Assistant node.
    - Use the import feature to bring your OpenAI assistant into AnswerAgentAI.

3. Configure the OpenAI Assistant Node:

    - Connect the necessary inputs and outputs within your AnswerAgentAI workflow.
    - Adjust any additional settings specific to your use case.

4. Run Your Workflow:
    - Start your AnswerAgentAI workflow to begin interacting with your imported OpenAI assistant.

<figure><img src="/.gitbook/assets/screenshots/openaiassistant.png" alt="" /><figcaption><p>OpenAI Assistant Tool &#x26; Drop UI</p></figcaption></figure><!-- TODO: Add a screenshot of the OpenAI Assistant import process in AnswerAgentAI -->

## Tips and Best Practices

-   Review the latest OpenAI documentation for up-to-date information on assistant capabilities and best practices.
-   Experiment with different tool combinations to enhance your assistant's problem-solving abilities.
-   Use the Assistants playground on the OpenAI platform to test and refine your assistant before importing it into AnswerAgentAI.
-   Take advantage of the persistent Threads feature for maintaining context in long-running conversations.

## Troubleshooting

-   If your assistant isn't performing as expected, double-check its configuration on the OpenAI platform.
-   Ensure that your OpenAI API credentials are correctly set up in AnswerAgentAI.
-   For issues with specific tools, refer to the OpenAI documentation for tool-specific troubleshooting advice.

## Technical Details

The OpenAI Assistant in AnswerAgentAI utilizes the following key components from the Assistants API:

1. **Assistant**: The AI entity with specific instructions, model preferences, and tool access.
2. **Thread**: A conversation session that stores messages and handles context management.
3. **Message**: Individual interactions within a Thread, which can include text, images, and files.
4. **Run**: An invocation of the Assistant on a Thread, performing tasks and generating responses.
5. **Run Step**: Detailed actions taken by the Assistant during a Run, useful for understanding its decision-making process.

<!-- TODO: Add a diagram illustrating the relationship between Assistant, Thread, Message, Run, and Run Step -->

Remember to review the [latest OpenAI documentation](https://platform.openai.com/docs/assistants/overview) for the most up-to-date information on the Assistants API and its capabilities.

By importing your OpenAI assistants into AnswerAgentAI, you can leverage their advanced capabilities within your custom workflows, creating powerful and flexible AI-driven applications.
</file>

<file path="sidekick-studio/chatflows/agents/openai-assistant/threads.md">
# Threads

[Threads](https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages) is only used when an OpenAI Assistant is being used. It is a conversation session between an Assistant and a user. Threads store messages and automatically handle truncation to fit content into a models context.

<figure><img src="../..//.gitbook/assets/screely-1699896158130.png" alt="" /><figcaption></figcaption></figure>

## Separate conversations for multiple users

### UI & Embedded Chat

By default, UI and Embedded Chat will automatically separate threads for multiple users conversations. This is done by generating a unique **`chatId`** for each new interaction. That logic is handled under the hood by Flowise.

### Prediction API

POST /`api/v1/prediction/{your-chatflowid}`, specify the **`chatId`** . Same thread will be used for the same chatId.

```json
{
    "question": "hello!",
    "chatId": "user1"
}
```

### Message API

-   GET `/api/v1/chatmessage/{your-chatflowid}`
-   DELETE `/api/v1/chatmessage/{your-chatflowid}`

You can also filter via **`chatId` -** `/api/v1/chatmessage/{your-chatflowid}?chatId={your-chatid}`

All conversations can be visualized and managed from UI as well:

<figure><img src="../..//.gitbook/assets/image (77).png" alt="" /><figcaption></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/agents/README.md">
---
description: LangChain Agent Nodes
---

# Agents

---

By themselves, language models can't take actions - they just output text.

Agents are systems that use an LLM as a reasoning enginer to determine which actions to take and what the inputs to those actions should be. The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.

### Agent Nodes

-   [OpenAI Assistant](openai-assistant/)
-   [Tool Agent](tool-agent.md)
</file>

<file path="sidekick-studio/chatflows/agents/tool-agent.md">
---
description: Learn how to use the Tool Agent in AnswerAgentAI
---

# Tool Agent

## Overview

The Tool Agent is an advanced AI assistant that uses Function Calling to select and execute appropriate tools based on user input. It's designed to handle complex tasks by breaking them down into steps and utilizing various tools to accomplish the goal.

## Key Benefits

-   Flexible problem-solving: Can tackle a wide range of tasks by combining different tools.
-   Intelligent decision-making: Chooses the most appropriate tools for each step of a task.
-   Memory integration: Maintains context across multiple interactions for more coherent conversations.

## How to Use

1. Configure the Tool Agent with the following required inputs:

    - Tools: Select the tools you want the agent to have access to.
    - Memory: Choose a memory component to maintain conversation history.
    - Tool Calling Chat Model: Select a compatible language model (e.g., ChatOpenAI, ChatMistral, ChatAnthropic).

2. (Optional) Configure additional parameters:

    - System Message: Customize the agent's behavior with specific instructions.
    - Input Moderation: Add moderation checks to prevent harmful content.
    - Max Iterations: Set a limit on the number of tool-using steps the agent can take.

3. Connect the Tool Agent node to your AnswerAgentAI workflow.

4. Run your workflow and interact with the Tool Agent by providing input queries or tasks.

<figure><img src="/.gitbook/assets/screenshots/toolagent.png" alt="" /><figcaption><p>Tool Agent &#x26; Drop UI</p></figcaption></figure><!-- TODO: Add a screenshot of the Tool Agent configuration interface -->

## Tips and Best Practices

-   Choose a diverse set of tools to give the agent more problem-solving capabilities.
-   Use a memory component to maintain context in long conversations.
-   Customize the system message to fine-tune the agent's behavior for specific use cases.
-   Monitor the agent's performance and adjust the max iterations if needed to balance thoroughness with efficiency.

## Troubleshooting

-   If the agent is not using tools as expected, check that the selected language model supports function calling.
-   For vision-related tasks, ensure you're using a compatible model and have properly configured image inputs.
-   If the agent seems to be stuck in a loop, try adjusting the max iterations parameter.

## Technical Details

The Tool Agent uses a sophisticated process to handle user inputs:

1. It first applies any configured input moderation to filter out potentially harmful content.
2. The agent then uses a chat prompt template that incorporates system instructions, conversation history, and user input.
3. For models supporting vision capabilities, it can process and incorporate image data into the conversation.
4. The agent uses the selected language model to determine which tools to use and how to use them.
5. It executes the chosen tools and can iterate multiple times to solve complex problems.
6. Finally, it formats the response and can include details about used tools and source documents.

<!-- TODO: Add a diagram showing the Tool Agent's decision-making process -->

Remember that the Tool Agent is a powerful feature that can handle complex tasks, but it may require some experimentation and fine-tuning to achieve optimal results for your specific use case.
</file>

<file path="sidekick-studio/chatflows/cache/in-memory-cache.md">
---
description: Cache LLM responses in local memory for improved performance
---

# InMemory Cache

## Overview

The InMemory Cache feature in AnswerAgentAI allows you to store Language Model (LLM) responses in local memory. This caching mechanism improves performance by reducing the need for repeated API calls for identical prompts.

## Key Benefits

-   Faster response times for repeated queries
-   Reduced API usage, potentially lowering costs
-   Improved overall system efficiency

## How to Use

1. Add the InMemory Cache node to your AnswerAgentAI workflow:
       <!-- TODO: Screenshot of adding InMemory Cache node to the workflow -->
    <figure><img src="/.gitbook/assets/screenshots/inmemorycache.png" alt="" /><figcaption><p>In Memory Cache &#x26; Drop UI</p></figcaption></figure>

2. Connect the InMemory Cache node to your LLM node:
       <!-- TODO: Screenshot showing the connection between InMemory Cache and LLM nodes -->
    <figure><img src="/.gitbook/assets/screenshots/in memory cache in a workflow.png" alt="" /><figcaption><p>Tool Agent &#x26; Drop UI</p></figcaption></figure>
3. Configure your workflow to use the cache:

    - No additional configuration is required. The cache will automatically store and retrieve responses.

4. Run your workflow:
    - The first time a unique prompt is processed, the response will be cached.
    - Subsequent identical prompts will retrieve the cached response, improving performance.

## Tips and Best Practices

1. Use caching for stable, non-dynamic content:

    - Ideal for frequently asked questions or standard procedures.
    - Avoid caching for time-sensitive or rapidly changing information.

2. Monitor cache usage:

    - Regularly review which prompts are being cached to ensure relevance.
    - Consider clearing the cache periodically if information becomes outdated.

3. Combine with other caching strategies:

    - For more persistent caching, consider using the InMemory Cache alongside database caching solutions.

4. Test thoroughly:
    - Ensure that caching doesn't negatively impact the accuracy or relevance of your AI responses.

## Troubleshooting

1. Cached responses seem outdated:

    - Remember that the InMemory Cache is cleared when the AnswerAgentAI app is restarted.
    - If you need to update cached responses, restart the application or implement a cache clearing mechanism.

2. No performance improvement observed:

    - Verify that the InMemory Cache node is correctly connected in your workflow.
    - Ensure that you're testing with repeated, identical prompts to see the caching effect.

3. Unexpected responses:
    - Check if your prompt includes dynamic elements (e.g., timestamps, user-specific data) that might prevent proper caching.
    - Review your workflow to ensure the cache is being used as intended.

Remember that the InMemory Cache is cleared when the AnswerAgentAI app is restarted. For long-term persistence, consider using a database caching solution in addition to or instead of the InMemory Cache.

By leveraging the InMemory Cache feature, you can significantly enhance the performance and efficiency of your AnswerAgentAI workflows, especially for frequently repeated queries or stable information retrieval tasks.
</file>

<file path="sidekick-studio/chatflows/cache/inmemory-embedding-cache.md">
---
description: Cache generated Embeddings in memory for improved performance and efficiency
---

# InMemory Embedding Cache

## Overview

The InMemory Embedding Cache feature in AnswerAgentAI allows you to store generated embeddings in local memory. This caching mechanism improves performance by avoiding the need to recompute embeddings for previously processed text, leading to faster response times and reduced computational overhead.

## Key Benefits

-   Faster processing of repeated or similar text inputs
-   Reduced computational resource usage
-   Improved overall system efficiency and response time

## How to Use

1. Add the InMemory Embedding Cache node to your AnswerAgentAI workflow:
 <figure><img src="/.gitbook/assets/screenshots/inmemory embedding cache.png" alt="" /><figcaption><p>In Memory Embedding Cache &#x26; Drop UI</p></figcaption></figure><!-- TODO: Screenshot of adding InMemory Embedding Cache node to the workflow -->

2. Configure the InMemory Embedding Cache node:

    - Connect an Embeddings node to the "Embeddings" input.
    - (Optional) Specify a "Namespace" for the cache.
      <!-- TODO: Screenshot showing the configuration of the InMemory Embedding Cache node -->
      <figure><img src="/.gitbook/assets/screenshots/inmemory embedding cache configuration.png" alt="" /><figcaption><p>In Memory Embedding Cache Configuration &#x26; Drop UI</p></figcaption></figure>

3. Connect the InMemory Embedding Cache node to other nodes in your workflow that require embeddings:
       <!-- TODO: Screenshot showing the connection between InMemory Embedding Cache and other relevant nodes -->
    <figure><img src="/.gitbook/assets/screenshots/inmemoery embedding cache in a workflow.png" alt="" /><figcaption><p>In Memory Embedding Cache In A Workflow &#x26; Drop UI</p></figcaption></figure>
4. Run your workflow:
    - The first time a unique text is processed, its embedding will be computed and cached.
    - Subsequent identical or similar texts will retrieve the cached embedding, improving performance.

## Tips and Best Practices

1. Use caching for repetitive or similar text inputs:

    - Ideal for processing documents with recurring phrases or concepts.
    - Effective for chatbots or Q&A systems with frequently asked questions.

2. Consider namespace usage:

    - Use namespaces to organize embeddings for different purposes or datasets.
    - This can help manage cache entries and prevent conflicts between different parts of your application.

3. Monitor memory usage:

    - Keep in mind that the cache is stored in memory, which may impact your application's overall memory footprint.
    - Consider implementing a cache size limit or periodic clearing mechanism for long-running applications.

4. Combine with persistent storage:

    - For long-term caching across application restarts, consider implementing a database-backed embedding cache alongside the in-memory cache.

5. Evaluate cache hit rate:
    - Monitor how often cached embeddings are being used versus new computations.
    - This can help you optimize your workflow and identify areas where caching is most beneficial.

## Troubleshooting

1. Unexpected or inconsistent results:

    - Ensure that the Embeddings node connected to the InMemory Embedding Cache is correctly configured.
    - Verify that the cache namespace (if used) is consistent across relevant parts of your workflow.

2. High memory usage:

    - If your application processes a large volume of unique text, consider implementing a cache eviction strategy or size limit.
    - Regularly monitor memory usage and clear the cache if necessary.

3. No performance improvement observed:

    - Confirm that your workflow is processing repeated or similar text inputs to benefit from caching.
    - Check that the InMemory Embedding Cache node is correctly connected in your workflow.

4. Cache not persisting between sessions:
    - Remember that the InMemory Embedding Cache is cleared when the AnswerAgentAI app is restarted.
    - For persistent caching across restarts, consider implementing a database-backed caching solution.

Remember that the InMemory Embedding Cache is cleared when the AnswerAgentAI app is restarted. For long-term persistence, consider implementing a database-backed caching solution in addition to or instead of the InMemory Embedding Cache.

By leveraging the InMemory Embedding Cache feature, you can significantly enhance the performance and efficiency of your AnswerAgentAI workflows, especially for tasks involving repetitive text processing or embedding-based operations.
</file>

<file path="sidekick-studio/chatflows/cache/momento-cache.md">
---
description: Cache LLM responses using Momento, a distributed, serverless cache
---

# Momento Cache

## Overview

The Momento Cache feature in AnswerAgentAI allows you to store Language Model (LLM) responses using Momento, a distributed, serverless cache. This caching mechanism improves performance and reduces costs by storing and retrieving responses for repeated queries, eliminating the need for redundant API calls.

## Key Benefits

-   Faster response times for repeated queries
-   Reduced API usage, potentially lowering costs
-   Scalable, serverless caching solution
-   Distributed cache for improved reliability and performance

## How to Use

1. Set up a Momento account and obtain your API key:

    - Sign up for a Momento account at [https://gomomento.com/](https://gomomento.com/)
    - Create a new cache and note down the cache name
    - Generate an API key for authentication

2. Configure the Momento Cache credential in AnswerAgentAI:

    - Navigate to the credentials section in AnswerAgentAI
    - Create a new credential of type 'momentoCacheApi'
    - Enter your Momento API key and cache name
          <figure><img src="/.gitbook/assets/screenshots/momento cache api credentials.png" alt="" /><figcaption><p>Momento Cache Node &#x26; Drop UI</p></figcaption></figure><!-- TODO: Screenshot of creating Momento Cache credential -->

3. Add the Momento Cache node to your AnswerAgentAI workflow:
    <!-- TODO: Screenshot of adding Momento Cache node to the workflow -->
    <figure><img src="/.gitbook/assets/screenshots/momento cache configuration.png" alt="" /><figcaption><p>Momento  Cache Node Configuration &#x26; Drop UI</p></figcaption></figure>

4. Configure the Momento Cache node:

    - Connect the previously created credential to the node
          <!-- TODO: Screenshot showing the configuration of the Momento Cache node -->

5. Connect the Momento Cache node to your LLM node:
     <!-- TODO: Screenshot showing the connection between Momento Cache and LLM nodes -->
    <figure><img src="/.gitbook/assets/screenshots/momento cache in  a workflow.png" alt="" /><figcaption><p>Momento  Cache Node In A Workflow &#x26; Drop UI</p></figcaption></figure>

6. Run your workflow:
    - The first time a unique prompt is processed, the response will be cached in Momento
    - Subsequent identical prompts will retrieve the cached response, improving performance

## Tips and Best Practices

1. Optimize cache usage:

    - Use caching for stable, non-dynamic content
    - Ideal for frequently asked questions or standard procedures
    - Avoid caching for time-sensitive or rapidly changing information

2. Monitor cache performance:

    - Regularly review cache hit rates and response times
    - Adjust cache settings (e.g., TTL) based on your specific use case

3. Secure your API key:

    - Keep your Momento API key confidential
    - Use environment variables or secure credential management systems

4. Implement error handling:

    - Add appropriate error handling in your workflow to manage potential cache connection issues

5. Consider data privacy:
    - Ensure that caching sensitive information complies with your data privacy policies and regulations

## Troubleshooting

1. Cache misses or unexpected responses:

    - Verify that the Momento Cache node is correctly connected in your workflow
    - Check if your prompt includes dynamic elements that might prevent proper caching
    - Ensure the cache name in your credential matches the one in your Momento account

2. Authentication errors:

    - Double-check that your Momento API key is correct and active
    - Verify that the credential is properly linked to the Momento Cache node

3. Performance issues:

    - If you're not seeing expected performance improvements, ensure you're testing with repeated, identical prompts
    - Check your Momento account dashboard for any service issues or limits

4. Cached data persistence:
    - Remember that Momento Cache has a default TTL (Time To Live) of 24 hours
    - Adjust the TTL in your workflow if you need longer or shorter cache persistence

If you encounter any issues not covered here, refer to the Momento documentation or contact AnswerAgentAI support for assistance.

By leveraging the Momento Cache feature, you can significantly enhance the performance, efficiency, and scalability of your AnswerAgentAI workflows, especially for frequently repeated queries or stable information retrieval tasks.
</file>

<file path="sidekick-studio/chatflows/cache/README.md">
---
description: LangChain Cache Nodes
---

# Cache

---

Caching can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider.
[LangChain Cache Nodes](https://js.langchain.com/v0.2/docs/how_to/llm_caching/)

### Cache Nodes

-   [InMemory Cache](in-memory-cache.md)
-   [InMemory Embedding Cache](inmemory-embedding-cache.md)
-   [Momento Cache](momento-cache.md)
-   [Redis Cache](redis-cache.md)
-   [Redis Embeddings Cache](redis-embeddings-cache.md)
-   [Upstash Redis Cache](upstash-redis-cache.md)
</file>

<file path="sidekick-studio/chatflows/cache/redis-cache.md">
---
description: Cache LLM response in Redis, useful for sharing cache across multiple processes or servers.
---

# Redis Cache

## Overview

The Redis Cache feature in AnswerAgentAI allows you to store Language Model (LLM) responses using Redis, a high-performance, in-memory data store. This caching mechanism is particularly useful for sharing cache across multiple processes or servers, improving response times, and reducing the load on your LLM service.

## Key Benefits

-   Faster response times for repeated queries
-   Reduced API usage, potentially lowering costs
-   Shared cache across multiple processes or servers
-   Scalable and high-performance caching solution
-   Configurable Time to Live (TTL) for cache entries

## How to Use

1.  Set up a Redis server:

    -   Install Redis on your server or use a managed Redis service
    -   Note down the connection details (host, port, username, password)

2.  Configure the Redis Cache credential in AnswerAgentAI:

    -   Navigate to the credentials section in AnswerAgentAI
    -   Create a new credential of type 'redisCacheApi' or 'redisCacheUrlApi'
    -   For 'redisCacheApi', enter the Redis host, port, username, and password
    -   For 'redisCacheUrlApi', enter the Redis connection URL
    -   If using SSL, enable the SSL option
        <!-- TODO: Screenshot of creating Redis Cache credential -->
            <figure><img src="/.gitbook/assets/screenshots/redis cache api credentials.png" alt="" /><figcaption><p>Redis Cache Credentials &#x26; Drop UI</p></figcaption></figure>

3.  Add the Redis Cache node to your AnswerAgentAI workflow:
    <!-- TODO: Screenshot of adding Redis Cache node to the workflow -->
    <figure><img src="/.gitbook/assets/screenshots/redis cache configuration.png" alt="" /><figcaption><p>Redis Cache Configuration &#x26; Drop UI</p></figcaption></figure>

4.  Configure the Redis Cache node:

    -   Connect the previously created credential to the node
    -   (Optional) Set the Time to Live (TTL) in milliseconds
        <!-- TODO: Screenshot showing the configuration of the Redis Cache node -->
        <figure><img src="/.gitbook/assets/screenshots/redis cache api credentials.png" alt="" /><figcaption><p>Redis API Cache Configuration &#x26; Drop UI</p></figcaption></figure>

5.  Connect the Redis Cache node to your LLM node:
      <!-- TODO: Screenshot showing the connection between Redis Cache and LLM nodes -->
    <figure><img src="/.gitbook/assets/screenshots/redis cache in a workflow.png" alt="" /><figcaption><p>Redis API Cache In A workflow &#x26; Drop UI</p></figcaption></figure>

6.  Run your workflow:
    -   The first time a unique prompt is processed, the response will be cached in Redis
    -   Subsequent identical prompts will retrieve the cached response, improving performance

## Tips and Best Practices

1. Optimize cache usage:

    - Use caching for stable, non-dynamic content
    - Ideal for frequently asked questions or standard procedures
    - Avoid caching for time-sensitive or rapidly changing information

2. Configure Time to Live (TTL):

    - Set an appropriate TTL based on how frequently your data changes
    - Shorter TTL for more dynamic content, longer TTL for stable information
    - If TTL is not set, cached entries will persist until manually removed

3. Monitor Redis performance:

    - Regularly check Redis memory usage and performance metrics
    - Implement alerting for Redis server health

4. Secure your Redis instance:

    - Use strong passwords and consider using SSL for encrypted connections
    - Implement proper network security measures to protect your Redis server

5. Scale your Redis setup:

    - For high-traffic applications, consider using Redis clusters or replication
    - Implement proper backup and recovery procedures for your Redis data

6. Handle cache misses gracefully:
    - Implement fallback mechanisms in case of cache misses or Redis connection issues

## Troubleshooting

1. Connection issues:

    - Verify that the Redis server is running and accessible
    - Check if the credential details (host, port, username, password) are correct
    - Ensure that firewalls or network policies allow connections to the Redis server

2. Cache misses or unexpected responses:

    - Verify that the Redis Cache node is correctly connected in your workflow
    - Check if your prompt includes dynamic elements that might prevent proper caching
    - Ensure the TTL is set appropriately for your use case

3. Performance issues:

    - Monitor Redis server load and memory usage
    - Consider upgrading your Redis server or implementing clustering for better performance
    - Optimize your cache key generation if necessary

4. Data consistency issues:

    - If you're using multiple Redis instances, ensure they are properly synchronized
    - Implement proper error handling for scenarios where cached data might be inconsistent

5. SSL connection problems:
    - Verify that SSL is properly configured on both the Redis server and in the AnswerAgentAI credential
    - Check SSL certificate validity and expiration

If you encounter any issues not covered here, refer to the Redis documentation or contact AnswerAgentAI support for assistance.

By leveraging the Redis Cache feature, you can significantly enhance the performance, scalability, and efficiency of your AnswerAgentAI workflows, especially for high-traffic applications or scenarios requiring shared caching across multiple processes or servers.
</file>

<file path="sidekick-studio/chatflows/cache/redis-embeddings-cache.md">
---
description: Cache generated Embeddings in Redis to improve performance and efficiency
---

# Redis Embeddings Cache

## Overview

The Redis Embeddings Cache feature in AnswerAgentAI allows you to store generated embeddings in Redis, a high-performance, in-memory data store. This caching mechanism improves performance by avoiding the need to recompute embeddings for previously processed text, leading to faster response times and reduced computational overhead.

## Key Benefits

-   Faster processing of repeated or similar text inputs
-   Reduced computational resource usage
-   Improved overall system efficiency and response time
-   Shared cache across multiple processes or servers
-   Configurable Time to Live (TTL) for cache entries

## How to Use

1. Set up a Redis server:

    - Install Redis on your server or use a managed Redis service
    - Note down the connection details (host, port, username, password)

2. Configure the Redis Cache credential in AnswerAgentAI:

    - Navigate to the credentials section in AnswerAgentAI
    - Create a new credential of type 'redisCacheApi' or 'redisCacheUrlApi'
    - For 'redisCacheApi', enter the Redis host, port, username, and password
    - For 'redisCacheUrlApi', enter the Redis connection URL
    - If using SSL, enable the SSL option
          <figure><img src="/.gitbook/assets/screenshots/redis cache api credentials.png" alt="" /><figcaption><p>Redis Cache API Configuration &#x26; Drop UI</p></figcaption></figure> <!-- TODO: Screenshot of creating Redis Cache credential -->

3. Add the Redis Embeddings Cache node to your AnswerAgentAI workflow:
    <!-- TODO: Screenshot of adding Redis Embeddings Cache node to the workflow -->
    <figure><img src="/.gitbook/assets/screenshots/redis embedding cache configuration.png" alt="" /><figcaption><p>Redis Embedding Cache Node &#x26; Drop UI</p></figcaption></figure>  <!-- TODO: Screenshot of creating Redis Cache credential -->

4. Configure the Redis Embeddings Cache node:

    - Connect an Embeddings node to the "Embeddings" input
    - Connect the previously created Redis credential to the node
    - (Optional) Set the Time to Live (TTL) in seconds (default is 3600 seconds or 1 hour)
    - (Optional) Specify a namespace for the cache
          <!-- TODO: Screenshot showing the configuration of the Redis Embeddings Cache node -->

5. Connect the Redis Embeddings Cache node to other nodes in your workflow that require embeddings:
      <!-- TODO: Screenshot showing the connection between Redis Embeddings Cache and other relevant nodes -->
    <figure><img src="/.gitbook/assets/screenshots/redis embedding cache in a workflow.png" alt="" /><figcaption><p>Redis Embedding Cache Node In A Workflow &#x26; Drop UI</p></figcaption></figure>

6. Run your workflow:
    - The first time a unique text is processed, its embedding will be computed and cached in Redis
    - Subsequent identical or similar texts will retrieve the cached embedding, improving performance

## Tips and Best Practices

1. Optimize cache usage:

    - Use caching for repetitive or similar text inputs
    - Ideal for processing documents with recurring phrases or concepts
    - Effective for chatbots or Q&A systems with frequently asked questions

2. Configure Time to Live (TTL):

    - Set an appropriate TTL based on how frequently your data changes
    - Shorter TTL for more dynamic content, longer TTL for stable information
    - The default TTL is 1 hour (3600 seconds)

3. Use namespaces:

    - Implement namespaces to organize embeddings for different purposes or datasets
    - This can help manage cache entries and prevent conflicts between different parts of your application

4. Monitor Redis performance:

    - Regularly check Redis memory usage and performance metrics
    - Implement alerting for Redis server health

5. Secure your Redis instance:

    - Use strong passwords and consider using SSL for encrypted connections
    - Implement proper network security measures to protect your Redis server

6. Scale your Redis setup:

    - For high-traffic applications, consider using Redis clusters or replication
    - Implement proper backup and recovery procedures for your Redis data

7. Combine with other caching strategies:
    - Use Redis Embeddings Cache alongside other caching mechanisms for optimal performance

## Troubleshooting

1. Connection issues:

    - Verify that the Redis server is running and accessible
    - Check if the credential details (host, port, username, password) are correct
    - Ensure that firewalls or network policies allow connections to the Redis server

2. Cache misses or unexpected results:

    - Verify that the Redis Embeddings Cache node is correctly connected in your workflow
    - Check if your input text includes dynamic elements that might prevent proper caching
    - Ensure the TTL is set appropriately for your use case

3. Performance issues:

    - Monitor Redis server load and memory usage
    - Consider upgrading your Redis server or implementing clustering for better performance
    - Optimize your cache key generation if necessary

4. Namespace conflicts:

    - If you're using multiple workflows or applications with the same Redis instance, ensure you're using unique namespaces to avoid conflicts

5. SSL connection problems:

    - Verify that SSL is properly configured on both the Redis server and in the AnswerAgentAI credential
    - Check SSL certificate validity and expiration

6. Unexpected cache evictions:
    - If embeddings are being evicted from the cache too quickly, consider increasing the TTL or Redis memory allocation

Remember that the Redis Embeddings Cache is cleared based on the TTL you set (default 1 hour). For long-term persistence, consider implementing a database-backed caching solution in addition to or instead of the Redis Embeddings Cache.

By leveraging the Redis Embeddings Cache feature, you can significantly enhance the performance and efficiency of your AnswerAgentAI workflows, especially for tasks involving repetitive text processing or embedding-based operations across multiple processes or servers.
</file>

<file path="sidekick-studio/chatflows/cache/upstash-redis-cache.md">
---
description: Cache LLM responses in Upstash Redis for improved performance and serverless scalability
---

# Upstash Redis Cache

## Overview

The Upstash Redis Cache feature in AnswerAgentAI allows you to store Language Model (LLM) responses using Upstash Redis, a serverless data solution for Redis. This caching mechanism improves performance by storing and retrieving responses for repeated queries, eliminating the need for redundant API calls while leveraging the scalability and ease of use of a serverless architecture.

## Key Benefits

-   Faster response times for repeated queries
-   Reduced API usage, potentially lowering costs
-   Serverless architecture for easy scalability
-   No infrastructure management required
-   Global distribution for low-latency access

## How to Use

1. Set up an Upstash Redis account:

    - Sign up for an Upstash account at [https://upstash.com/](https://upstash.com/)
    - Create a new Redis database
    - Note down the Redis connection URL and token

2. Configure the Upstash Redis Cache credential in AnswerAgentAI:

    - Navigate to the credentials section in AnswerAgentAI
    - Create a new credential of type 'upstashRedisApi'
    - Enter your Upstash Redis connection URL and token
          <figure><img src="/.gitbook/assets/screenshots/upstash redis cache credentials.png" alt="" /><figcaption><p>Upstash Redis Cache API Credential &#x26; Drop UI</p></figcaption></figure>
          <!-- TODO: Screenshot of creating Upstash Redis Cache credential -->

3. Add the Upstash Redis Cache node to your AnswerAgentAI workflow:
       <!-- TODO: Screenshot of adding Upstash Redis Cache node to the workflow -->
    <figure><img src="/.gitbook/assets/screenshots/upstash redis cache configuration.png" alt="" /><figcaption><p>Upstash Redis Cache Node &#x26; Drop UI</p></figcaption></figure>

4. Configure the Upstash Redis Cache node:

    - Connect the previously created credential to the node
          <!-- TODO: Screenshot showing the configuration of the Upstash Redis Cache node -->

5. Connect the Upstash Redis Cache node to your LLM node:
    <!-- TODO: Screenshot showing the connection between Upstash Redis Cache and LLM nodes -->
    <figure><img src="/.gitbook/assets/screenshots/upstash redis cache in a workflow.png" alt="" /><figcaption><p>Upstash Redis Cache In a Workflow &#x26; Drop UI</p></figcaption></figure>

6. Run your workflow:
    - The first time a unique prompt is processed, the response will be cached in Upstash Redis
    - Subsequent identical prompts will retrieve the cached response, improving performance

## Tips and Best Practices

1. Optimize cache usage:

    - Use caching for stable, non-dynamic content
    - Ideal for frequently asked questions or standard procedures
    - Avoid caching for time-sensitive or rapidly changing information

2. Monitor cache performance:

    - Use Upstash's dashboard to monitor cache usage, hit rates, and performance metrics
    - Adjust your caching strategy based on observed patterns

3. Secure your Upstash credentials:

    - Keep your Upstash connection URL and token confidential
    - Use environment variables or secure credential management systems

4. Implement error handling:

    - Add appropriate error handling in your workflow to manage potential cache connection issues

5. Consider data privacy:

    - Ensure that caching sensitive information complies with your data privacy policies and regulations

6. Leverage Upstash features:
    - Explore Upstash's global replication for multi-region low-latency access
    - Utilize Upstash's automatic scaling capabilities for high-traffic scenarios

## Troubleshooting

1. Connection issues:

    - Verify that your Upstash Redis database is active
    - Double-check that your connection URL and token are correct in the AnswerAgentAI credential
    - Ensure your network allows outbound connections to Upstash servers

2. Cache misses or unexpected responses:

    - Verify that the Upstash Redis Cache node is correctly connected in your workflow
    - Check if your prompt includes dynamic elements that might prevent proper caching
    - Ensure you're not exceeding Upstash's rate limits or storage quotas

3. Performance issues:

    - If you're not seeing expected performance improvements, ensure you're testing with repeated, identical prompts
    - Check your Upstash dashboard for any service issues or limits
    - Consider upgrading your Upstash plan if you're hitting performance bottlenecks

4. Data persistence concerns:

    - Be aware that Upstash may have default TTL (Time To Live) settings
    - Configure appropriate TTL values in your Upstash database settings if needed

5. Cost management:
    - Monitor your Upstash usage to ensure it aligns with your budget
    - Implement cache eviction strategies if needed to manage storage costs

If you encounter any issues not covered here, refer to the Upstash documentation or contact AnswerAgentAI support for assistance.

By leveraging the Upstash Redis Cache feature, you can significantly enhance the performance, efficiency, and scalability of your AnswerAgentAI workflows, especially for serverless architectures or globally distributed applications requiring low-latency caching solutions.
</file>

<file path="sidekick-studio/chatflows/chains/conversation-chain.md">
---
description: Learn about the Conversation Chain, a fundamental building block for chatbots in AnswerAgentAI
---

# Conversation Chain

## Overview

The Conversation Chain is the most basic and versatile chain type for building chatbots in AnswerAgentAI. It provides a simple yet powerful framework for creating interactive, conversational AI experiences. This chain is designed to maintain context throughout a conversation, making it suitable for a wide range of applications.

## Key Benefits

-   Simplicity: Easy to set up and understand, making it ideal for beginners and straightforward use cases.
-   Versatility: Adaptable to various chatbot scenarios and requirements.
-   Memory Integration: Maintains conversation history for context-aware responses.
-   Customizable: Can be tailored with different language models, prompts, and memory types.

## When to Use Conversation Chain

The Conversation Chain is an excellent choice for many chatbot applications, including:

1. Customer Support Bots: Handle basic inquiries and provide information.
2. Personal Assistants: Perform tasks and answer questions based on user input.
3. Educational Chatbots: Engage in learning conversations and answer student questions.
4. Entertainment Bots: Create interactive storytelling or role-playing experiences.
5. Information Retrieval: Provide answers to user queries from a defined knowledge base.

## How It Works

1. **Input Processing**: The chain receives user input and processes it along with the conversation history.
2. **Context Maintenance**: It uses a memory component to keep track of the conversation flow.
3. **Prompt Generation**: The chain constructs a prompt using a template, which includes the system message, conversation history, and user input.
4. **Language Model Interaction**: The constructed prompt is sent to the specified language model for processing.
5. **Response Generation**: The model generates a response based on the input and context.
6. **Memory Update**: The new interaction is added to the conversation history for future context.

## Key Components

### 1. Chat Model

The underlying language model that powers the conversation. You can choose from various models like GPT-3.5, GPT-4, or other compatible chat models.

### 2. Memory

Stores and retrieves conversation history, allowing the chatbot to maintain context across multiple interactions.

### 3. Chat Prompt Template

Defines the structure of the prompt sent to the language model, including system messages and placeholders for user input and conversation history.

### 4. Input Moderation (Optional)

Helps filter and moderate user inputs to ensure safe and appropriate interactions.

## Tips for Effective Use

1. **Craft Clear System Messages**: Define the chatbot's persona and behavior through well-written system prompts.
2. **Choose Appropriate Memory**: Select a memory type that suits your use case (e.g., buffer memory for recent context, vector memory for long-term information).
3. **Optimize Prompts**: Refine your prompt templates to guide the model towards desired outputs.
4. **Monitor and Iterate**: Regularly review chatbot performance and user interactions to improve the conversation flow.

## Limitations and Considerations

-   **Lack of Specialized Knowledge**: For domain-specific applications, you may need to augment the Conversation Chain with additional data sources or specialized components.
-   **Context Window Limitations**: Be mindful of the chosen model's context window size, as very long conversations may exceed these limits.
-   **Potential for Inconsistency**: Without careful prompt engineering, the chatbot may sometimes provide inconsistent responses across long conversations.

By leveraging the Conversation Chain, you can quickly develop functional and engaging chatbots for a wide array of applications. Its simplicity and flexibility make it an excellent starting point for many conversational AI projects in AnswerAgentAI.
</file>

<file path="sidekick-studio/chatflows/chains/conversational-retrieval-qa-chain.md">
---
description: Learn about the Conversational Retrieval QA Chain, a powerful tool for document-based Q&A with conversation history in AnswerAgentAI
---

# Conversational Retrieval QA Chain

## Overview

The Conversational Retrieval QA Chain is an advanced chain in AnswerAgentAI that combines document retrieval capabilities with conversation history management. This chain is designed to provide context-aware answers to user queries by referencing a large corpus of documents while maintaining the flow of a conversation.

## Key Benefits

-   Context-Aware Responses: Leverages both document content and conversation history for more accurate answers.
-   Efficient Document Retrieval: Utilizes vector store retrieval for quick and relevant document access.
-   Flexible Memory Management: Can use custom or default memory to maintain conversation context.
-   Customizable Prompts: Allows fine-tuning of question rephrasing and response generation.
-   Source Attribution: Option to return source documents for transparency and further exploration.

## When to Use Conversational Retrieval QA Chain

This chain is ideal for applications that require:

1. Question-Answering Systems: Build chatbots that can answer questions based on a large knowledge base.
2. Customer Support: Create AI assistants that can reference product documentation while maintaining conversation context.
3. Research Assistants: Develop tools that can answer questions based on academic papers or reports.
4. Educational Platforms: Design interactive learning systems that can answer student queries using course materials.
5. Legal or Compliance Chatbots: Create systems that can answer questions based on legal documents or company policies.

## How It Works

1. **Question Processing**: The chain receives a user question and the current conversation history.
2. **Question Rephrasing**: If there's conversation history, the question is rephrased to be standalone, incorporating context from previous interactions.
3. **Document Retrieval**: The (possibly rephrased) question is used to retrieve relevant documents from the vector store.
4. **Context Formation**: Retrieved documents are formatted and combined with the original question and conversation history.
5. **Response Generation**: The language model generates a response based on the retrieved context and conversation history.
6. **Memory Update**: The new interaction is added to the conversation history for future context.

## Key Components

### 1. Chat Model

The underlying language model that powers the conversation and generates responses.

### 2. Vector Store Retriever

Efficiently retrieves relevant documents based on the user's query.

### 3. Memory

Stores and retrieves conversation history. You can use a custom memory or the default BufferMemory.

### 4. Rephrase Prompt

Defines how to rephrase the user's question in the context of the conversation history.

### 5. Response Prompt

Guides the model in generating a response based on the retrieved documents and conversation context.

## Tips for Effective Use

1. **Optimize Your Vector Store**: Ensure your document chunks are appropriately sized and indexed for efficient retrieval.
2. **Refine Prompts**: Customize the rephrase and response prompts to suit your specific use case and desired AI behavior.
3. **Balance Context**: Adjust the amount of conversation history and retrieved documents to provide sufficient context without overwhelming the model.
4. **Monitor Performance**: Regularly review the chain's responses and retrieved documents to identify areas for improvement.
5. **Consider Source Attribution**: Use the option to return source documents when transparency is important for your application.

## Limitations and Considerations

-   **Retrieval Quality**: The chain's effectiveness depends on the quality and relevance of the retrieved documents.
-   **Context Window Limitations**: Be mindful of the total token count from conversation history and retrieved documents to avoid exceeding model limits.
-   **Potential for Hallucination**: While the chain aims to ground responses in retrieved documents, there's still a possibility of the model generating inaccurate information.
-   **Computation Overhead**: The document retrieval and rephrasing steps may increase response time compared to simpler chains.

By leveraging the Conversational Retrieval QA Chain, you can create sophisticated, document-grounded chatbots that maintain conversational context. This chain is particularly powerful for applications requiring both broad knowledge access and nuanced understanding of ongoing conversations.
</file>

<file path="sidekick-studio/chatflows/chains/get-api-chain.md">
---
description: Learn how to use the GET API Chain feature in AnswerAgentAI
---

# GET API Chain

## Overview

The GET API Chain is a powerful feature in AnswerAgentAI that allows you to run queries against GET APIs. This feature enables you to integrate external API data into your workflows, enhancing the capabilities of your AI-powered applications.

## Key Benefits

-   Seamless integration with external data sources
-   Customizable API interactions
-   Enhanced AI responses with real-time data

## How to Use

### 1. Set up the GET API Chain

<!-- TODO: Screenshot of the GET API Chain node in the AnswerAgentAI interface -->
 <figure><img src="/.gitbook/assets/screenshots/get api chain node.png" alt="" /><figcaption><p>Get API Chain &#x26; Drop UI</p></figcaption></figure>

1. In your AnswerAgentAI workflow, locate and add the "GET API Chain" node.
2. Connect the node to your desired input and output nodes.

 <figure><img src="/.gitbook/assets/screenshots/get api configuration.png" alt="" /><figcaption><p>Get API Chain Configuration &#x26; Drop UI</p></figcaption></figure>

### 2. Configure the Language Model

1. In the "Language Model" field, select the appropriate language model for your task.
2. Ensure the chosen model is compatible with your API and query requirements.

### 3. Provide API Documentation

1. In the "API Documentation" field, enter a detailed description of how the API works.
2. Include information such as endpoint structure, available parameters, and response format. Many times you can just copy and paste the docs into the field.

<figure><img src="/.gitbook/assets/screenshots/get api chain in a workflow.png" alt="" /><figcaption><p>Get API Chain In a Workflow &#x26; Drop UI</p></figcaption></figure>
3. For reference, you can use this example:

```plaintext
BASE URL: https://api.open-meteo.com/

API Documentation
The API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:

Parameter Format Required Default Description
latitude, longitude Floating point Yes  Geographical WGS84 coordinate of the location
hourly String array No  A list of weather variables which should be returned. Values can be comma separated, or multiple &hourly= parameter in the URL can be used.
daily String array No  A list of daily weather variable aggregations which should be returned. Values can be comma separated, or multiple &daily= parameter in the URL can be used. If daily weather variables are specified, parameter timezone is required.
current_weather Bool No false Include current weather conditions in the JSON output.
temperature_unit String No celsius If fahrenheit is set, all temperature values are converted to Fahrenheit.
windspeed_unit String No kmh Other wind speed speed units: ms, mph and kn
precipitation_unit String No mm Other precipitation amount units: inch
timeformat String No iso8601 If format unixtime is selected, all time values are returned in UNIX epoch time in seconds. Please note that all timestamp are in GMT+0! For daily values with unix timestamps, please apply utc_offset_seconds again to get the correct date.
timezone String No GMT If timezone is set, all timestamps are returned as local-time and data is returned starting at 00:00 local-time. Any time zone name from the time zone database is supported. If auto is set as a time zone, the coordinates will be automatically resolved to the local time zone.
past_days Integer (0-2) No 0 If past_days is set, yesterday or the day before yesterday data are also returned.
start_date
end_date String (yyyy-mm-dd) No  The time interval to get weather data. A day must be specified as an ISO8601 date (e.g. 2022-06-30).
models String array No auto Manually select one or more weather models. Per default, the best suitable weather models will be combined.

Hourly Parameter Definition
The parameter &hourly= accepts the following values. Most weather variables are given as an instantaneous value for the indicated hour. Some variables like precipitation are calculated from the preceding hour as an average or sum.

Variable Valid time Unit Description
temperature_2m Instant C (F) Air temperature at 2 meters above ground
snowfall Preceding hour sum cm (inch) Snowfall amount of the preceding hour in centimeters. For the water equivalent in millimeter, divide by 7. E.g. 7 cm snow = 10 mm precipitation water equivalent
rain Preceding hour sum mm (inch) Rain from large scale weather systems of the preceding hour in millimeter
showers Preceding hour sum mm (inch) Showers from convective precipitation in millimeters from the preceding hour
weathercode Instant WMO code Weather condition as a numeric code. Follow WMO weather interpretation codes. See table below for details.
snow_depth Instant meters Snow depth on the ground
freezinglevel_height Instant meters Altitude above sea level of the 0C level
visibility Instant meters Viewing distance in meters. Influenced by low clouds, humidity and aerosols. Maximum visibility is approximately 24 km.
```

### 4. Set Headers (Optional)

1. If your API requires specific headers, click on "Add Additional Parameter."
2. Select "Headers" and enter the required headers in JSON format.

### 5. Customize Prompts (Optional)

1. To fine-tune how AnswerAgentAI interacts with the API, you can customize two prompts:

    - URL Prompt: Determines how AnswerAgentAI constructs the API URL
    - Answer Prompt: Guides AnswerAgentAI on how to process and return the API response

2. To customize, click on "Add Additional Parameter" and select the prompt you want to modify.
3. Ensure that you include the required placeholders (`{api_docs}, {question}, {api_response}, {api_url}`) in your custom prompts.

### 6. Run Your Workflow

1. Once configured, run your workflow with an input query.
2. The GET API Chain will process the query, construct the appropriate API call, and return the results.

## Tips and Best Practices

-   Keep your API documentation clear and concise for optimal results.
-   Use specific and relevant examples in your API documentation to guide the AI.
-   Regularly update your API documentation to reflect any changes in the external API.
-   Test your GET API Chain with various queries to ensure it handles different scenarios correctly.
-   Monitor API usage to stay within rate limits and optimize performance.

## Troubleshooting

### Issue: Incorrect API URL Construction

-   **Solution**: Review and refine your API documentation. Ensure all necessary endpoints and parameters are clearly described.

### Issue: Unexpected API Responses

-   **Solution**: Check that your Answer Prompt accurately guides the AI in interpreting the API response. Adjust as needed for better results.

### Issue: Authentication Errors

-   **Solution**: Verify that you've correctly included any required authentication headers in the "Headers" section.

### Issue: Rate Limiting Issues

-   **Solution**: Implement appropriate rate limiting in your workflow or consider using caching mechanisms to reduce API calls.

If you encounter persistent issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/chains/llm-chain.md">
---
description: Learn about the LLM Chain, a fundamental building block for AI-powered applications in AnswerAgentAI
---

# LLM Chain

## Overview

The LLM Chain is the most basic and versatile chain type in AnswerAgentAI. It provides a straightforward way to interact with Large Language Models (LLMs), making it an essential tool for a wide range of AI-powered applications. This chain is designed for simplicity and flexibility, allowing you to quickly implement AI capabilities in various scenarios.

## Key Benefits

-   Simplicity: Easy to set up and use, making it ideal for beginners and straightforward use cases.
-   Versatility: Adaptable to a wide range of applications and integration scenarios.
-   Quick Responses: Perfect for generating one-off responses or handling simple queries.
-   Integration-Friendly: Easily connects with external services and workflows.
-   Customizable: Can be tailored with different prompts and configurations.

## When to Use LLM Chain

The LLM Chain is an excellent choice for many applications, including:

1. One-Off Responses: Generate quick, context-independent responses to user queries.
2. API Endpoints: Create AI-powered endpoints for your applications.
3. Multistep Workflows: Use as a component in more complex chains or workflows.
4. Integration with External Services: Connect to platforms like Make or Zapier for automated tasks.
5. User Interface Interactions: Respond to user actions, such as button clicks, with AI-generated content.

## How It Works

1. **Input Processing**: The chain receives input through the AnswerAgentAI API.
2. **Prompt Preparation**: The input is combined with a predefined prompt template.
3. **LLM Interaction**: The prepared prompt is sent to the specified language model for processing.
4. **Response Generation**: The model generates a response based on the input and prompt.
5. **Output Delivery**: The response is returned via the API.

## Key Components

### 1. AnswerAgentAI API Endpoint

The entry point for interacting with the LLM Chain.

### 2. Prompt

Defines the structure of the query sent to the language model. This is crucial for guiding the model's response.

### 3. Configuration Options

Allow customization of the LLM Chain behavior, such as temperature and max tokens.

## Tips for Effective Use

1. **Craft Clear Prompts**: The quality of your prompt significantly affects the output. Be specific and provide context.
2. **Use Appropriate Configuration**: Adjust settings like temperature to control the creativity and focus of responses.
3. **Implement Error Handling**: Account for potential issues like network errors or unexpected model outputs.
4. **Consider Rate Limiting**: If using the LLM Chain in high-traffic scenarios, implement rate limiting to manage API usage.
5. **Secure Your API Key**: Always keep your API key confidential and use secure methods to include it in your requests.

## Use Case Examples

### 1. AI-Powered API Endpoint

```python
from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

@app.route('/generate-description', methods=['POST'])
def generate_description():
    product_name = request.json['productName']

    response = requests.post(
        API_URL,
        headers={"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"},
        json={
            "question": f"Write a compelling product description for {product_name}.",
            "overrideConfig": {
                "temperature": 0.7,
                "maxTokens": 150
            }
        }
    )

    return jsonify({"description": response.json()['text']})

if __name__ == '__main__':
    app.run(debug=True)
```

### 2. Automated Customer Response

```javascript
// In a Make.com or Zapier workflow
const API_URL = 'http://localhost:3000/api/v1/prediction/<your-chatflowid>'
const API_KEY = 'your-api-key'

async function generateResponse(customerQuery) {
    const response = await fetch(API_URL, {
        method: 'POST',
        headers: {
            Authorization: `Bearer ${API_KEY}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            question: `Customer Query: ${customerQuery}\nGenerate a helpful response:`,
            overrideConfig: {
                temperature: 0.5
            }
        })
    })
    const result = await response.json()
    return result.text
}

// Use this function in your Make.com or Zapier workflow
```

### 3. Interactive UI Element

```html
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>AnswerAgentAI Story Generator</title>
    </head>
    <body>
        <input type="text" id="theme" placeholder="Enter a story theme" />
        <button onclick="generateStory()">Generate Story</button>
        <div id="story"></div>

        <script>
            const API_URL = 'http://localhost:3000/api/v1/prediction/<your-chatflowid>'
            const API_KEY = 'your-api-key'

            async function generateStory() {
                const theme = document.getElementById('theme').value
                const response = await fetch(API_URL, {
                    method: 'POST',
                    headers: {
                        Authorization: `Bearer ${API_KEY}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        question: `Generate a creative story idea based on the theme: ${theme}`,
                        overrideConfig: {
                            temperature: 0.8,
                            maxTokens: 200
                        }
                    })
                })
                const result = await response.json()
                document.getElementById('story').innerText = result.text
            }
        </script>
    </body>
</html>
```

## Limitations and Considerations

-   **Context Limitation**: LLM Chain doesn't maintain conversation history by default, making it less suitable for multi-turn interactions without additional configuration.
-   **Prompt Dependency**: The quality of output heavily depends on the design of your prompt.
-   **Cost Consideration**: Frequent use, especially with more advanced models, can incur significant API costs.
-   **Potential for Inconsistency**: Without careful prompt engineering, responses may vary in style or content.

By leveraging the LLM Chain through the AnswerAgentAI API, you can quickly implement AI capabilities in a wide range of applications. Its simplicity and flexibility make it an excellent starting point for many AI projects, particularly for standalone queries or as components in larger, more complex systems.
</file>

<file path="sidekick-studio/chatflows/chains/multi-prompt-chain.md">
---
description: Learn about the Multi Prompt Chain, a powerful tool for automatic prompt selection in AnswerAgentAI
---

# Multi Prompt Chain

## Overview

The Multi Prompt Chain in AnswerAgentAI is an advanced chain type that automatically selects the most appropriate prompt from multiple prompt templates based on the input. This chain is particularly useful when dealing with diverse types of queries that require different specialized prompts.

## Key Benefits

-   Automatic Prompt Selection: Chooses the best prompt for each input, improving response relevance.
-   Versatility: Handles a wide range of query types with a single chain.
-   Efficiency: Reduces the need for manual prompt switching or complex if-else logic.
-   Scalability: Easily expandable by adding new prompt templates.

## When to Use Multi Prompt Chain

The Multi Prompt Chain is ideal for:

1. General-Purpose Chatbots: Handle a variety of user queries efficiently.
2. Customer Support Systems: Address different types of customer inquiries.
3. Educational Platforms: Provide responses tailored to various subjects or question types.
4. Content Generation: Create different types of content based on input specifications.
5. Complex Decision Trees: Simplify logic by letting the chain choose the appropriate path.

## How It Works

1. Input Processing: The chain receives a user query via the AnswerAgentAI API.
2. Prompt Analysis: The system evaluates the input against available prompt descriptions.
3. Prompt Selection: The most suitable prompt is automatically chosen.
4. LLM Interaction: The selected prompt is used with the input to query the language model.
5. Response Generation: The model generates a response based on the chosen prompt and input.

## Key Components

1. Language Model: The underlying AI model used for generating responses.
2. Prompt Retrievers: A collection of prompts, each with a name, description, and template.
3. Input Moderation (Optional): Filters to ensure safe and appropriate inputs.

## Using the Multi Prompt Chain with AnswerAgentAI API

### Basic Usage

Here's how to use the Multi Prompt Chain via the AnswerAgentAI API:

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "question": "What's the capital of France?",
    "overrideConfig": {
        "promptRetriever": [
            {
                "name": "Geography",
                "description": "Answer questions about world geography",
                "systemMessage": "You are a geography expert. Provide accurate information about countries, cities, and landmarks."
            },
            {
                "name": "History",
                "description": "Answer questions about historical events and figures",
                "systemMessage": "You are a history expert. Provide detailed information about historical events, figures, and time periods."
            }
        ]
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

### Adding Input Moderation

To include input moderation:

```python
data = {
    "question": "What's the capital of France?",
    "overrideConfig": {
        "promptRetriever": [
            # ... prompt retrievers as before ...
        ],
        "inputModeration": [
            {
                "type": "toxicity",
                "threshold": 0.8
            }
        ]
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

## Use Case Examples

### 1. Multi-Purpose Customer Support Bot

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

def customer_support_bot(query):
    data = {
        "question": query,
        "overrideConfig": {
            "promptRetriever": [
                {
                    "name": "TechnicalSupport",
                    "description": "Handle technical issues and troubleshooting",
                    "systemMessage": "You are a technical support expert. Provide step-by-step solutions for technical problems."
                },
                {
                    "name": "BillingInquiries",
                    "description": "Address questions about billing and payments",
                    "systemMessage": "You are a billing specialist. Provide accurate information about invoices, payments, and subscription details."
                },
                {
                    "name": "ProductInformation",
                    "description": "Provide details about products and services",
                    "systemMessage": "You are a product expert. Offer detailed information about our products, their features, and use cases."
                }
            ]
        }
    }

    response = requests.post(API_URL, json=data, headers={"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"})
    return response.json()['text']

# Usage
print(customer_support_bot("How do I reset my password?"))
print(customer_support_bot("What are the features of your premium plan?"))
print(customer_support_bot("I haven't received my latest invoice"))
```

### 2. Educational Content Generator

```javascript
const API_URL = 'http://localhost:3000/api/v1/prediction/<your-chatflowid>'
const API_KEY = 'your-api-key'

async function generateEducationalContent(topic, grade) {
    const response = await fetch(API_URL, {
        method: 'POST',
        headers: {
            Authorization: `Bearer ${API_KEY}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            question: `Create educational content about ${topic} for grade ${grade}`,
            overrideConfig: {
                promptRetriever: [
                    {
                        name: 'ElementaryContent',
                        description: 'Generate content for elementary school students',
                        systemMessage: 'You are an elementary school teacher. Create simple, engaging content suitable for young learners.'
                    },
                    {
                        name: 'MiddleSchoolContent',
                        description: 'Generate content for middle school students',
                        systemMessage:
                            'You are a middle school teacher. Create content that balances depth and accessibility for adolescent learners.'
                    },
                    {
                        name: 'HighSchoolContent',
                        description: 'Generate content for high school students',
                        systemMessage:
                            'You are a high school teacher. Create detailed, challenging content that prepares students for advanced studies.'
                    }
                ]
            }
        })
    })

    const result = await response.json()
    return result.text
}

// Usage
generateEducationalContent('Photosynthesis', '8')
    .then((content) => console.log(content))
    .catch((error) => console.error(error))
```

## Best Practices

1. Diverse Prompts: Create a varied set of prompt templates to cover different types of queries.
2. Clear Descriptions: Write clear, distinctive descriptions for each prompt to aid in selection.
3. Regular Updates: Continuously refine and expand your prompt set based on user interactions.
4. Fallback Option: Include a general-purpose prompt as a fallback for unexpected query types.
5. Monitor Performance: Regularly review which prompts are being selected to optimize your system.

By leveraging the Multi Prompt Chain in AnswerAgentAI, you can create more versatile and intelligent AI applications that adapt to various user inputs and contexts, providing more accurate and relevant responses across a wide range of scenarios.
</file>

<file path="sidekick-studio/chatflows/chains/multi-retrieval-qa-chain.md">
---
description: Learn about the Multi Retrieval QA Chain, a powerful tool for automatic vector store selection in AnswerAgentAI
---

# Multi Retrieval QA Chain

## Overview

The Multi Retrieval QA Chain in AnswerAgentAI is an advanced chain type that automatically selects the most appropriate vector store from multiple retrievers based on the input query. This chain is particularly useful when dealing with diverse types of questions that require information from different specialized knowledge bases.

## Key Benefits

-   Automatic Retriever Selection: Chooses the best vector store for each query, improving response relevance.
-   Versatility: Handles a wide range of question types with a single chain.
-   Efficiency: Reduces the need for manual retriever switching or complex routing logic.
-   Scalability: Easily expandable by adding new vector stores and retrievers.

## When to Use Multi Retrieval QA Chain

The Multi Retrieval QA Chain is ideal for:

1. Comprehensive Knowledge Bases: Handle queries across multiple domains or subjects.
2. Enterprise Search Systems: Retrieve information from various departmental databases.
3. Research Platforms: Access and combine data from multiple specialized collections.
4. E-commerce Product Search: Query across different product categories or attributes.
5. Multi-lingual or Multi-regional Systems: Select appropriate language or region-specific retrievers.

## How It Works

1. Input Processing: The chain receives a user query via the AnswerAgentAI API.
2. Retriever Analysis: The system evaluates the input against available retriever descriptions.
3. Retriever Selection: The most suitable vector store retriever is automatically chosen.
4. Information Retrieval: The selected retriever fetches relevant documents from its vector store.
5. LLM Interaction: The retrieved information and the original query are used to generate a response.
6. Response Generation: The model provides an answer based on the retrieved information and the query.

## Key Components

1. Language Model: The underlying AI model used for generating responses.
2. Vector Store Retrievers: A collection of retrievers, each with a name, description, and associated vector store.
3. Return Source Documents Option: Allows including source documents in the response.
4. Input Moderation (Optional): Filters to ensure safe and appropriate inputs.

## Using the Multi Retrieval QA Chain with AnswerAgentAI API

### Basic Usage

Here's how to use the Multi Retrieval QA Chain via the AnswerAgentAI API:

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "question": "What are the key features of quantum computing?",
    "overrideConfig": {
        "vectorStoreRetriever": [
            {
                "name": "ComputerScience",
                "description": "Information about computer science topics",
                "vectorStore": {
                    "k": 4  # Number of documents to retrieve
                }
            },
            {
                "name": "Physics",
                "description": "Information about physics concepts",
                "vectorStore": {
                    "k": 4
                }
            }
        ],
        "returnSourceDocuments": True
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

### Adding Input Moderation

To include input moderation:

```python
data = {
    "question": "What are the key features of quantum computing?",
    "overrideConfig": {
        "vectorStoreRetriever": [
            # ... vector store retrievers as before ...
        ],
        "returnSourceDocuments": True,
        "inputModeration": [
            {
                "type": "toxicity",
                "threshold": 0.8
            }
        ]
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

## Use Case Examples

### 1. Multi-Domain Research Assistant

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

def research_assistant(query):
    data = {
        "question": query,
        "overrideConfig": {
            "vectorStoreRetriever": [
                {
                    "name": "ScientificPapers",
                    "description": "Database of scientific research papers across various disciplines",
                    "vectorStore": {"k": 5}
                },
                {
                    "name": "NewsArticles",
                    "description": "Recent news articles on various topics",
                    "vectorStore": {"k": 3}
                },
                {
                    "name": "HistoricalRecords",
                    "description": "Historical documents and records",
                    "vectorStore": {"k": 4}
                }
            ],
            "returnSourceDocuments": True
        }
    }

    response = requests.post(API_URL, json=data, headers={"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"})
    return response.json()

# Usage
result = research_assistant("What are the latest developments in renewable energy?")
print(result['text'])  # The answer
print(result['sourceDocuments'])  # The source documents used
```

### 2. E-commerce Product Recommendation System

```javascript
const API_URL = 'http://localhost:3000/api/v1/prediction/<your-chatflowid>'
const API_KEY = 'your-api-key'

async function productRecommendation(query, userPreferences) {
    const response = await fetch(API_URL, {
        method: 'POST',
        headers: {
            Authorization: `Bearer ${API_KEY}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            question: query,
            overrideConfig: {
                vectorStoreRetriever: [
                    {
                        name: 'Electronics',
                        description: 'Database of electronic products',
                        vectorStore: { k: 5 }
                    },
                    {
                        name: 'Clothing',
                        description: 'Database of clothing and fashion items',
                        vectorStore: { k: 5 }
                    },
                    {
                        name: 'HomeGoods',
                        description: 'Database of home and kitchen products',
                        vectorStore: { k: 5 }
                    }
                ],
                returnSourceDocuments: true
            },
            userPreferences: userPreferences
        })
    })

    const result = await response.json()
    return {
        recommendations: result.text,
        products: result.sourceDocuments
    }
}

// Usage
productRecommendation('I need a new laptop for graphic design', { budget: 'high', brand: 'Apple' })
    .then((result) => {
        console.log('Recommendations:', result.recommendations)
        console.log('Product Details:', result.products)
    })
    .catch((error) => console.error(error))
```

## Best Practices

1. Diverse Retrievers: Create a varied set of vector store retrievers to cover different domains or data types.
2. Clear Descriptions: Write clear, distinctive descriptions for each retriever to aid in selection.
3. Optimize Retrieval: Adjust the number of documents retrieved (k) based on the needs of your application.
4. Regular Updates: Keep your vector stores updated with the latest information.
5. Use Source Documents: Enable returnSourceDocuments for transparency and to provide additional context.
6. Monitor Performance: Regularly review which retrievers are being selected to optimize your system.

By leveraging the Multi Retrieval QA Chain in AnswerAgentAI, you can create more sophisticated and intelligent QA systems that adapt to various query types and information sources, providing more accurate and comprehensive responses across a wide range of domains.
</file>

<file path="sidekick-studio/chatflows/chains/openapi-chain.md">
---
description: Learn about the OpenAPI Chain, a powerful tool for automatic API selection and calling in AnswerAgentAI
---

# OpenAPI Chain

## Overview

The OpenAPI Chain in AnswerAgentAI is an advanced chain type that automatically selects and calls APIs based solely on an OpenAPI specification. This chain is particularly useful when you need to interact with external APIs dynamically, without hardcoding specific API calls in your application.

## Key Benefits

-   Automatic API Selection: Chooses the appropriate API endpoint based on the user's query.
-   Dynamic Interaction: Allows interaction with APIs without prior knowledge of their specific endpoints.
-   Flexibility: Can work with any API that provides an OpenAPI (formerly Swagger) specification.
-   Simplicity: Reduces the complexity of integrating multiple APIs into your application.

## When to Use OpenAPI Chain

The OpenAPI Chain is ideal for:

1. Multi-API Integration: When your application needs to interact with multiple APIs.
2. Chatbots and Virtual Assistants: To provide responses that require real-time data from external sources.
3. Dynamic Data Retrieval: When the required data source may vary based on user queries.
4. API Testing and Exploration: For quickly testing or exploring new APIs without writing custom integration code.
5. Workflow Automation: In scenarios where different API calls need to be made based on varying conditions.

## How It Works

1. Input Processing: The chain receives a user query via the AnswerAgentAI API.
2. OpenAPI Spec Analysis: The system analyzes the provided OpenAPI specification.
3. API Selection: Based on the user's query, the appropriate API endpoint is selected.
4. API Call Execution: The selected API is called with the necessary parameters.
5. Response Processing: The API response is processed and formatted.
6. Answer Generation: A human-readable answer is generated based on the API response.

## Key Components

1. ChatOpenAI Model: The language model used for understanding queries and generating responses.
2. OpenAPI Specification: Provided either as a YAML link or a YAML file.
3. Headers: Optional HTTP headers for API calls.
4. Input Moderation: Optional filters to ensure safe and appropriate inputs.

## Using the OpenAPI Chain with AnswerAgentAI API

### Basic Usage

Here's how to use the OpenAPI Chain via the AnswerAgentAI API:

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "question": "What's the weather like in New York?",
    "overrideConfig": {
        "yamlLink": "https://api.openweathermap.org/data/2.5/weather.yaml",
        "headers": {
            "api-key": "your-openweathermap-api-key"
        }
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

### Using a YAML File Instead of a Link

If you have a local YAML file, you can use it instead of a link:

```python
import base64

# Read and encode the YAML file
with open("openweathermap.yaml", "rb") as file:
    yaml_content = base64.b64encode(file.read()).decode()

data = {
    "question": "What's the weather like in London?",
    "overrideConfig": {
        "yamlFile": f"data:application/x-yaml;base64,{yaml_content}",
        "headers": {
            "api-key": "your-openweathermap-api-key"
        }
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

### Adding Input Moderation

To include input moderation:

```python
data = {
    "question": "What's the weather like in Paris?",
    "overrideConfig": {
        "yamlLink": "https://api.openweathermap.org/data/2.5/weather.yaml",
        "headers": {
            "api-key": "your-openweathermap-api-key"
        },
        "inputModeration": [
            {
                "type": "toxicity",
                "threshold": 0.8
            }
        ]
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

## Use Case Examples

### 1. Multi-Service Travel Assistant

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

def travel_assistant(query):
    data = {
        "question": query,
        "overrideConfig": {
            "yamlLink": "https://api.travelservices.com/openapi.yaml",
            "headers": {
                "Authorization": "Bearer your-travel-api-key"
            }
        }
    }

    response = requests.post(API_URL, json=data, headers={"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"})
    return response.json()['text']

# Usage
print(travel_assistant("Find flights from New York to London next week"))
print(travel_assistant("What are the top-rated hotels in Paris?"))
print(travel_assistant("Is a visa required for a US citizen to visit Japan?"))
```

### 2. Financial Information Aggregator

```javascript
const API_URL = 'http://localhost:3000/api/v1/prediction/<your-chatflowid>'
const API_KEY = 'your-api-key'

async function financialInfo(query) {
    const response = await fetch(API_URL, {
        method: 'POST',
        headers: {
            Authorization: `Bearer ${API_KEY}`,
            'Content-Type': 'application/json'
        },
        body: JSON.stringify({
            question: query,
            overrideConfig: {
                yamlLink: 'https://api.financialdata.com/openapi.yaml',
                headers: {
                    'X-API-Key': 'your-financial-api-key'
                }
            }
        })
    })

    const result = await response.json()
    return result.text
}

// Usage
financialInfo("What's the current stock price of Apple?").then(console.log).catch(console.error)

financialInfo('Give me a summary of the latest market trends').then(console.log).catch(console.error)
```

## Best Practices

1. Secure API Keys: Always keep API keys secure and never expose them in client-side code.
2. Validate OpenAPI Specs: Ensure the OpenAPI specifications you're using are up-to-date and valid.
3. Handle Rate Limits: Be aware of and respect the rate limits of the APIs you're interacting with.
4. Error Handling: Implement robust error handling to manage potential API failures or unexpected responses.
5. Use Input Moderation: Implement input moderation to prevent misuse and ensure appropriate queries.
6. Keep OpenAPI Specs Updated: Regularly update the OpenAPI specifications to ensure you're working with the latest API versions.
7. Monitor Usage: Keep track of API usage to optimize performance and manage costs.

By leveraging the OpenAPI Chain in AnswerAgentAI, you can create flexible and powerful applications that dynamically interact with a wide range of APIs, providing rich and varied functionalities without the need for extensive custom integration code.
</file>

<file path="sidekick-studio/chatflows/chains/post-api-chain.md">
---
description: Learn how to use the POST API Chain feature in AnswerAgentAI
---

# POST API Chain

## Overview

The POST API Chain is a powerful feature in AnswerAgentAI that allows you to run queries against POST APIs. This feature enables you to integrate external API data into your workflows, enhancing the capabilities of your AI-powered applications by sending data to and receiving responses from external services.

## Key Benefits

-   Seamless integration with external services that require POST requests
-   Customizable API interactions for complex data submissions
-   Enhanced AI responses with real-time data exchange

## How to Use

### 1. Set up the POST API Chain

<!-- TODO: Screenshot of the POST API Chain node in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/postapinode.png" alt="" /><figcaption><p>Post API Chain &#x26; Drop UI</p></figcaption></figure>

1. In your AnswerAgentAI workflow, locate and add the "POST API Chain" node.
2. Connect the node to your desired input and output nodes.

### 2. Configure the Language Model

1. In the "Language Model" field, select the appropriate language model for your task.
2. Ensure the chosen model is compatible with your API and query requirements.

### 3. Provide API Documentation

1. In the "API Documentation" field, enter a detailed description of how the API works.
2. Include information such as endpoint structure, required POST parameters, request body format, and expected response format. Many times you can just copy and paste the docs into the field.
3. For reference, you can use this example:

<!-- TODO: Add example of API Post docs -->

### 4. Set Headers (Optional)

1. If your API requires specific headers, click on "Add Additional Parameter."
2. Select "Headers" and enter the required headers in JSON format.

### 5. Customize Prompts (Optional)

1. To fine-tune how AnswerAgentAI interacts with the API, you can customize two prompts:

    - URL Prompt: Determines how AnswerAgentAI constructs the API URL and request body
    - Answer Prompt: Guides AnswerAgentAI on how to process and return the API response

2. To customize, click on "Add Additional Parameter" and select the prompt you want to modify.
3. Ensure that you include the required placeholders in your custom prompts:
    - URL Prompt: `{api_docs}` and `{question}`
    - Answer Prompt: `{api_docs}`, `{question}`, `{api_url_body}`, and `{api_response}`

### 6. Run Your Workflow

1. Once configured, run your workflow with an input query.
2. The POST API Chain will process the query, construct the appropriate API call with the necessary POST data, and return the results.

## Tips and Best Practices

-   Provide clear and detailed API documentation, especially regarding the structure of the POST request body.
-   Use specific and relevant examples in your API documentation to guide the AI in constructing proper POST requests.
-   Regularly update your API documentation to reflect any changes in the external API's request or response format.
-   Test your POST API Chain with various queries to ensure it handles different scenarios and data inputs correctly.
-   Monitor API usage to stay within rate limits and optimize performance.
-   Be cautious with sensitive data in POST requests and ensure proper security measures are in place.

## Troubleshooting

### Issue: Incorrect API Request Construction

-   **Solution**: Review and refine your API documentation. Ensure all necessary endpoints, parameters, and POST body structures are clearly described.

### Issue: Unexpected API Responses

-   **Solution**: Check that your Answer Prompt accurately guides the AI in interpreting the API response. Adjust as needed for better results. Verify that the API is receiving the expected POST data.

### Issue: Authentication Errors

-   **Solution**: Verify that you've correctly included any required authentication headers or tokens in the "Headers" section.

### Issue: Rate Limiting Issues

-   **Solution**: Implement appropriate rate limiting in your workflow or consider using caching mechanisms to reduce API calls.

### Issue: Data Format Errors

-   **Solution**: Ensure that the POST data being sent matches the format expected by the API. Review the API documentation and adjust your prompts if necessary.

If you encounter persistent issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/chains/README.md">
---
description: LangChain Chain Nodes - A Fundamental Building Block in AnswerAgentAI
---

# Chains

Chains are one of the main starting points for every new LLM route, chatbot, and application being developed in AnswerAgentAI, alongside agents and sidekick teams. They form a crucial foundation for creating sophisticated AI-powered solutions.

## Overview

In the context of large language models and AI applications, "chains" refer to sequences of operations or conversation turns. These chains are used to manage the flow of information, maintain context, and orchestrate complex tasks involving language models and other components.

## Key Concepts

1. **Conversation Management**: Chains store and manage conversation history and context for chatbots and language models, enabling coherent and contextually relevant responses.

2. **Task Orchestration**: Chains can be used to break down complex tasks into a series of smaller, manageable steps, allowing for more sophisticated AI behaviors.

3. **Flexible Integration**: Chains can incorporate various components such as language models, vector stores, APIs, and databases, making them versatile for different use cases.

4. **Context Preservation**: By maintaining a history of interactions or operations, chains ensure that the AI system understands the broader context of a conversation or task.

## Importance in AnswerAgentAI

Chains play a crucial role in AnswerAgentAI for several reasons:

1. **Starting Point for Development**: When creating new LLM routes, chatbots, or applications, chains often serve as the initial framework, providing a structured way to design the flow of information and operations.

2. **Customization and Flexibility**: AnswerAgentAI offers various types of chains that can be combined and customized to suit specific use cases, from simple question-answering to complex multi-step reasoning.

3. **Integration with Other Components**: Chains in AnswerAgentAI can easily integrate with other powerful features like vector stores, APIs, and databases, enabling the creation of sophisticated AI solutions.

4. **Scalability**: As your AI applications grow in complexity, chains provide a scalable architecture that can be extended and modified to accommodate new features and capabilities.

## Types of Chains in AnswerAgentAI

AnswerAgentAI offers a variety of chain types to suit different needs:

-   [GET API Chain](get-api-chain.md)
-   [OpenAPI Chain](openapi-chain.md)
-   [POST API Chain](post-api-chain.md)
-   [Conversation Chain](conversation-chain.md)
-   [Conversational Retrieval QA Chain](conversational-retrieval-qa-chain.md)
-   [LLM Chain](llm-chain.md)
-   [Multi Prompt Chain](multi-prompt-chain.md)
-   [Multi Retrieval QA Chain](multi-retrieval-qa-chain.md)
-   [Retrieval QA Chain](retrieval-qa-chain.md)
-   [Sql Database Chain](sql-database-chain.md)
-   [Vectara QA Chain](vectara-chain.md)
-   [VectorDB QA Chain](vectordb-qa-chain.md)

Each of these chains serves specific purposes and can be combined to create powerful AI applications.

## Getting Started

To begin using chains in AnswerAgentAI:

1. Identify the type of task or application you want to build.
2. Choose the appropriate chain type(s) that best suit your needs.
3. Configure the chain with the necessary components (e.g., language models, vector stores, knowledge bases, etc).
4. Test and iterate on your chain to refine its performance.

By leveraging chains effectively, you can create sophisticated, context-aware AI applications that can handle a wide range of tasks and interactions.
</file>

<file path="sidekick-studio/chatflows/chains/retrieval-qa-chain.md">
---
description: Learn about the Retrieval QA Chain in AnswerAgentAI and how it differs from the Conversational Retrival QA Chain
---

# Retrieval QA Chain

## Overview

The Retrieval QA Chain in AnswerAgentAI is a specialized chain designed to answer questions based on retrieved documents. Unlike the [Conversational Retrival QA Chain](./conversational-retrieval-qa-chain.md), this chain focuses on providing accurate answers from a knowledge base without maintaining conversation history.

## Key Differences from Conversational Chain

1. **No Memory**: The Retrieval QA Chain does not maintain memory of previous interactions. Each query is treated independently.

2. **Document Retrieval**: This chain actively retrieves relevant documents for each query, whereas the [Conversational Retrival QA Chain](./conversational-retrieval-qa-chain.md) relies on its conversation history.

3. **Single-Turn Interactions**: Optimized for standalone questions rather than multi-turn conversations.

4. **Knowledge-Base Focused**: Answers are derived from a specific knowledge base rather than general knowledge or conversation context.

## Key Benefits

-   Accuracy: Provides answers based on specific, retrieved information.
-   Scalability: Can handle a large knowledge base efficiently.
-   Consistency: Gives consistent answers to similar questions, regardless of conversation history.
-   Flexibility: Can be easily updated with new information by updating the underlying vector store.

## When to Use Retrieval QA Chain

Use this chain when:

1. You need to answer questions based on a specific knowledge base or dataset.
2. Conversation history is not important or relevant.
3. You want to provide factual, consistent answers across different user sessions.
4. Dealing with domain-specific queries that require accurate, up-to-date information.

## How It Works

1. Query Reception: The chain receives a question via the AnswerAgentAI API.
2. Document Retrieval: Relevant documents are fetched from the vector store.
3. Context Preparation: Retrieved documents are prepared as context for the language model.
4. Answer Generation: The language model generates an answer based on the question and retrieved context.
5. Response Delivery: The answer is returned through the API.

## Key Components

1. Language Model: Processes the query and generates answers.
2. Vector Store Retriever: Fetches relevant documents based on the query.
3. Input Moderation (Optional): Ensures safe and appropriate inputs.

## Using the Retrieval QA Chain with AnswerAgentAI API

### Basic Usage

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

data = {
    "question": "What are the main features of quantum computing?",
    "overrideConfig": {
        "vectorStoreRetriever": {
            "type": "pinecone",
            "indexName": "quantum-computing-index"
        }
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

### Adding Input Moderation

```python
data = {
    "question": "What are the main features of quantum computing?",
    "overrideConfig": {
        "vectorStoreRetriever": {
            "type": "pinecone",
            "indexName": "quantum-computing-index"
        },
        "inputModeration": [
            {
                "type": "toxicity",
                "threshold": 0.8
            }
        ]
    }
}

response = requests.post(API_URL, json=data, headers=headers)
print(response.json())
```

## Use Case Example: Technical Support System

```python
import requests

API_URL = "http://localhost:3000/api/v1/prediction/<your-chatflowid>"
API_KEY = "your-api-key"

def technical_support(query):
    data = {
        "question": query,
        "overrideConfig": {
            "vectorStoreRetriever": {
                "type": "pinecone",
                "indexName": "technical-support-docs"
            }
        }
    }

    response = requests.post(API_URL, json=data, headers={
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    })
    return response.json()['text']

# Usage
print(technical_support("How do I reset my router?"))
print(technical_support("What are the system requirements for the latest software update?"))
```

## Best Practices

1. Quality of Vector Store: Ensure your vector store contains high-quality, relevant documents.
2. Regular Updates: Keep your knowledge base up-to-date for accurate answers.
3. Clear Queries: Encourage users to ask clear, specific questions for better retrieval.
4. Fallback Mechanisms: Implement fallbacks for when the chain can't find relevant information.
5. Monitoring: Regularly review the chain's performance and the relevance of retrieved documents.

## Limitations and Considerations

-   No Conversation Context: Unlike the Conversational Chain, this chain doesn't consider previous interactions.
-   Retrieval Dependence: The quality of answers heavily depends on the quality and relevance of retrieved documents.
-   Single-Query Focus: May not perform as well for complex queries that require multi-step reasoning.

By leveraging the Retrieval QA Chain in AnswerAgentAI, you can create powerful question-answering systems that provide accurate, knowledge-based responses without the need for maintaining conversation history. This makes it ideal for scenarios where factual, consistent information retrieval is more important than contextual conversation.
</file>

<file path="sidekick-studio/chatflows/chains/sql-database-chain.md">
---
description: Learn how to use the SQL Database Chain feature in AnswerAgentAI
---

# SQL Database Chain

## Overview

The SQL Database Chain is a powerful feature in AnswerAgentAI that allows you to query SQL databases using natural language. This feature bridges the gap between human language and SQL, enabling you to interact with your databases more intuitively and efficiently.

## Key Benefits

1. Natural Language Queries: Ask questions about your data in plain English, without writing complex SQL queries.
2. Multiple Database Support: Compatible with SQLite, PostgreSQL, MSSQL, and MySQL databases.
3. Customizable and Flexible: Tailor the chain to your specific needs with various configuration options.

## How to Use

Follow these steps to set up and use the SQL Database Chain:

1.  Select the Language Model:
    Choose a language model that will interpret your natural language queries and generate SQL.

2.  Choose Your Database Type:
    Select the type of database you're working with (SQLite, PostgreSQL, MSSQL, or MySQL).

3.  Provide Connection Details:
    Enter the connection string or file path for your database.

        <!-- TODO: Screenshot of the database connection input fields -->
        <figure><img src="/.gitbook/assets/screenshots/sqldatabasechain.png" alt="" /><figcaption><p>SQL Database Chain &#x26; Drop UI</p></figcaption></figure>

4.  Configure Table Selection (Optional):

    -   To include specific tables: Enter table names separated by commas in the "Include Tables" field.
    -   To exclude specific tables: Enter table names separated by commas in the "Ignore Tables" field.

    Note: You can use either "Include Tables" or "Ignore Tables", but not both simultaneously.

5.  Set Additional Parameters (Optional):

    -   Sample Rows: Specify the number of sample rows to load for each table.
    -   Top K Results: Limit the maximum number of results returned for each query.

6.  Customize the Prompt (Optional):
    If needed, provide a custom prompt to guide the language model's interpretation of your queries.

7.  Enable Input Moderation (Optional):
    Set up input moderation to filter out potentially harmful queries before they reach the language model.

8.  Run Your Query:
    Enter your natural language question about the database, and the SQL Database Chain will process it and return the results.

## Tips and Best Practices

1. Start Simple: Begin with straightforward queries to understand how the system interprets your questions.

2. Be Specific: The more precise your question, the more accurate the results will be.

3. Use Table Names: Including relevant table names in your query can help the system generate more accurate SQL.

4. Leverage Sample Rows: Setting an appropriate number of sample rows can improve the system's understanding of your data structure without overloading it.

5. Optimize with Top K: Use the Top K parameter to limit results for large tables, improving performance and reducing token usage.

6. Customize Prompts Carefully: If you're using a custom prompt, ensure it includes the required variables: `{input}`, `{dialect}`, and `{table_info}`.

## Troubleshooting

1. Connection Issues:

    - Double-check your connection string or file path.
    - Ensure your database is accessible from the environment where AnswerAgentAI is running.

2. Unexpected Results:

    - Verify that you've included all necessary tables if using the "Include Tables" option.
    - Check if any crucial tables are being excluded by the "Ignore Tables" option.

3. Performance Problems:

    - If queries are slow, try reducing the number of sample rows or using the Top K parameter to limit results.

4. Incorrect Interpretations:
    - Rephrase your question to be more specific or include key table names.
    - Consider using a custom prompt to guide the model's interpretation if you're consistently getting incorrect results for certain types of queries.

Remember, the SQL Database Chain is a powerful tool that becomes more effective as you learn to phrase your questions in a way that aligns with your database structure. Don't hesitate to experiment and refine your approach to get the best results.
</file>

<file path="sidekick-studio/chatflows/chains/vectara-chain.md">
---
description: Learn how to use the Vectara QA Chain feature in AnswerAgentAI
---

# Vectara QA Chain

## Overview

The Vectara QA Chain is a powerful feature in AnswerAgentAI that enables you to perform question-answering tasks using Vectara's advanced retrieval and summarization capabilities. This chain allows you to query your knowledge base in natural language and receive concise, relevant answers based on the information stored in your Vectara vector store.

## Key Benefits

.

1. Natural Language Querying: Ask questions in plain language and receive coherent, contextual answers.
2. Customizable Summarization: Choose from different summarizer models to tailor the response generation to your needs.
3. Multilingual Support: Obtain responses in various languages, making it suitable for global applications.

## How to Use

Follow these steps to set up and use the Vectara QA Chain:

1.  Connect Vectara Store:
    Select your pre-configured Vectara vector store as the source of information for the QA chain.

2.  Choose Summarizer Model:
    Select a summarizer prompt name from the available options:

    -   vectara-summary-ext-v1.2.0 (GPT-3.5-turbo, available to all users)
    -   vectara-experimental-summary-ext-2023-10-23-small (GPT-3.5-turbo, beta for Growth and Scale users)
    -   vectara-summary-ext-v1.3.0 (GPT-4.0, available to Scale users)
    -   vectara-experimental-summary-ext-2023-10-23-med (GPT-4.0, beta for Scale users)

             <!-- TODO: Screenshot of summarizer model selection dropdown -->

          <figure><img src="/.gitbook/assets/screenshots/vectorqachainsummarizer.png" alt="" /><figcaption><p>Vectera QA Chain Summarizer Model &#x26; Drop UI</p></figcaption></figure>

3.  Set Response Language (Optional):
    Choose the desired language for the response from the provided list. If not selected, Vectara will automatically detect the language.

4.  Configure Max Summarized Results:
    Set the maximum number of top results to use in generating the summarized response. The default is 7.

5.  Enable Input Moderation (Optional):
    Set up input moderation to filter out potentially harmful queries before they reach the language model.

6.  Run Your Query:
    Enter your question, and the Vectara QA Chain will process it, retrieve relevant information, and generate a summarized answer.

## Tips and Best Practices

1. Refine Your Questions: Be specific in your queries to get more accurate and relevant answers.

2. Experiment with Summarizers: Try different summarizer models to find the one that best suits your use case and content type.

3. Adjust Max Summarized Results: Increase this number for more comprehensive answers or decrease it for quicker, more concise responses.

4. Leverage Multilingual Capabilities: Use the response language feature to cater to diverse audience needs.

5. Monitor Performance: Pay attention to response times and quality to optimize your chain configuration.

## Troubleshooting

1. Unexpected or Irrelevant Answers:

    - Review and refine the content in your Vectara store.
    - Try rephrasing your question or using more specific terms.

2. Slow Response Times:

    - Reduce the number of max summarized results.
    - Check your Vectara store's performance and indexing.

3. Language Issues:

    - Ensure you've selected the correct response language.
    - Verify that your Vectara store contains content in the desired language.

4. Summarizer Errors:
    - If you encounter a "BAD REQUEST" error related to summarization, try reducing the number of search results or adjusting the context parameters.
    - Ensure you're using a summarizer that's available for your Vectara account tier.

Remember, the effectiveness of the Vectara QA Chain depends on the quality and relevance of the data in your Vectara store. Regularly update and maintain your knowledge base to ensure the best possible answers to your queries.
</file>

<file path="sidekick-studio/chatflows/chains/vectordb-qa-chain.md">
---
description: Learn how to use the VectorDB QA Chain feature in AnswerAgentAI
---

# VectorDB QA Chain

## Overview

The VectorDB QA Chain is a powerful feature in AnswerAgentAI that enables you to perform question-answering tasks using vector databases. This chain combines the capabilities of a language model with a vector store to provide accurate and contextual answers to your queries.

## Key Benefits

1. Efficient Information Retrieval: Quickly find relevant information from large datasets stored in vector databases.
2. Contextual Understanding: Leverage the power of language models to interpret questions and generate coherent answers.
3. Flexible Integration: Compatible with various vector stores and language models, allowing for customized setups.

## How to Use

Follow these steps to set up and use the VectorDB QA Chain:

1.  Select a Language Model:
    Choose a language model that will interpret your questions and generate answers.

2.  Connect a Vector Store:
    Select a pre-configured vector store that contains your indexed data.

        <!-- TODO: Screenshot of language model and vector store selection interface -->

     <figure><img src="/.gitbook/assets/screenshots/vectordbqachain.png" alt="" /><figcaption><p>VectorDB QA Chain &#x26; Drop UI</p></figcaption></figure>

3.  Configure Input Moderation (Optional):
    Set up input moderation to filter out potentially harmful queries before they reach the language model.

4.  Run Your Query:
    Enter your question, and the VectorDB QA Chain will process it, retrieve relevant information from the vector store, and generate an answer using the language model.

## Tips and Best Practices

1. Optimize Vector Store:
   Ensure your vector store is well-indexed and contains relevant, high-quality data for the best results.

2. Phrase Questions Clearly:
   Frame your questions in a clear and specific manner to get more accurate answers.

3. Experiment with Different Models:
   Try different language models to find the one that works best for your specific use case and data.

4. Monitor Performance:
   Pay attention to response times and answer quality to optimize your chain configuration.

5. Use Input Moderation:
   Implement input moderation to ensure safe and appropriate use of the QA chain, especially in public-facing applications.

## Troubleshooting

1. Irrelevant or Inaccurate Answers:

    - Review the content in your vector store and ensure it's up-to-date and relevant.
    - Try rephrasing your question or using more specific terms.
    - Consider adjusting the `k` value (number of retrieved documents) if available in your vector store configuration.

2. Slow Response Times:

    - Check the performance of your vector store and consider optimizing its configuration.
    - If using a complex language model, consider switching to a lighter version for faster processing.

3. Error Messages:

    - If you encounter moderation-related errors, review your input moderation settings and ensure they're not overly restrictive.
    - For other errors, check your language model and vector store connections.

4. Inconsistent Results:
    - Ensure your vector store is not being updated while you're querying it, which could lead to inconsistent results.
    - If using a random-based language model, consider setting a seed for reproducibility if needed.

Remember, the effectiveness of the VectorDB QA Chain depends on the quality of your vector store data and the capabilities of your chosen language model. Regularly update your vector store and stay informed about the latest advancements in language models to get the best results from your QA chain.
</file>

<file path="sidekick-studio/chatflows/chat-models/aws-chatbedrock.md">
---
description: AWS ChatBedrock - Chat Models for AnswerAgentAI
---

# AWS ChatBedrock

## Overview

AWS ChatBedrock is a powerful integration in AnswerAgentAI that allows you to use Amazon Web Services (AWS) Bedrock large language models for chat-based applications. This feature provides access to advanced AI models like Claude 3, enabling you to create sophisticated conversational experiences.

## Key Benefits

-   Access to state-of-the-art AWS Bedrock language models
-   Flexible configuration options for optimal performance
-   Support for multi-modal interactions, including image uploads (with compatible models)

## How to Use

Follow these steps to set up and use AWS ChatBedrock in AnswerAgentAI:

1. Navigate to the Chat Models section in AnswerAgentAI.
2. Locate and select the "AWS ChatBedrock" node.

3. Configure the following settings:

    - AWS Credential: (Optional) Select your AWS API credential if you have one set up.
    - Cache: (Optional) Choose a cache option if desired.
    - Region: Select the AWS region you want to use (e.g., "us-east-1").
    - Model Name: Choose from available AWS Bedrock models (e.g., "anthropic.claude-3-haiku").
    - Custom Model Name: (Optional) Enter a custom model name to override the selected Model Name.

4. Adjust additional parameters as needed:

    - Temperature: Set the creativity level of the model's responses (default: 0.7).
    - Max Tokens to Sample: Specify the maximum number of tokens for the model to generate (default: 200).
    - Allow Image Uploads: Enable this option for multi-modal interactions with compatible Claude 3 models.

5. Connect the AWS ChatBedrock node to other components in your AnswerAgentAI workflow.

6. Run your workflow to start using AWS ChatBedrock for generating responses.

<!-- TODO: Screenshot of the AWS ChatBedrock node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/aws chatbedrock node.png" alt="" /><figcaption><p>AWS Chatbedrock Configuration &#x26; Drop UI</p></figcaption></figure>
<figure><img src="/.gitbook/assets/screenshots/aws chatbedrock in a workflow.png" alt="" /><figcaption><p>AWS Chatbedrock Node in a workflow &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose the appropriate region based on your location or compliance requirements.
2. Experiment with different temperature values to find the right balance between creativity and coherence in responses.
3. Adjust the Max Tokens to Sample based on the desired length of the model's output.
4. When using Claude 3 models, enable the "Allow Image Uploads" option for multi-modal capabilities in compatible chains and agents.
5. If you have specific AWS credentials, make sure to set them up in the AnswerAgentAI credential manager for secure access.

## Troubleshooting

1. Model not available:

    - Ensure you have the necessary permissions in your AWS account to access the selected model.
    - Verify that the chosen model is available in the selected region.

2. Credential issues:

    - Double-check that your AWS API credentials are correctly set up in AnswerAgentAI.
    - If using environment-based credentials, ensure they are properly configured on your system.

3. Unexpected responses:

    - Adjust the temperature and max tokens parameters to fine-tune the model's output.
    - Verify that your input prompts are clear and well-structured for optimal results.

4. Image upload not working:
    - Confirm that you're using a Claude 3 model and have enabled the "Allow Image Uploads" option.
    - Check that you're using a compatible chain or agent (e.g., LLMChain, Conversation Chain, ReAct Agent, or Conversational Agent).

By following this guide, you'll be able to harness the power of AWS ChatBedrock models in your AnswerAgentAI workflows, creating sophisticated chat-based applications with ease.
</file>

<file path="sidekick-studio/chatflows/chat-models/azure-chatopenai.md">
---
description: Azure ChatOpenAI - Chat Models for AnswerAgentAI
---

# Azure ChatOpenAI

## Overview

Azure ChatOpenAI is a powerful feature in AnswerAgentAI that allows you to interact with Azure OpenAI's large language models using the Chat endpoint. This integration enables you to leverage the advanced capabilities of Azure's AI services within your AnswerAgentAI workflows.

## Key Benefits

-   Access to state-of-the-art language models hosted on Azure
-   Seamless integration with AnswerAgentAI's existing features
-   Customizable parameters for fine-tuned AI responses

## How to Use

1. Connect your Azure OpenAI API credential:

    - Click on the "Connect Credential" option
    - Select or create an "azureOpenAIApi" credential
    - Enter your Azure OpenAI API key, instance name, deployment name, and API version

2. Configure the model settings:

    - Select the desired model name from the dropdown list
    - Adjust the temperature (default: 0.9) to control response randomness
    - Set optional parameters such as max tokens, top probability, and penalties

3. Enable additional features (optional):

    - Toggle "Allow Image Uploads" to enable image processing capabilities
    - Select the preferred image resolution (Low, High, or Auto)

4. Integrate the Azure ChatOpenAI node into your AnswerAgentAI workflow

<!-- TODO: Screenshot of the Azure ChatOpenAI node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/azure chatopen ai node configuration.png" alt="" /><figcaption><p>Azure ChatOpenAI Configuration &#x26; Drop UI</p></figcaption></figure>

<figure><img src="/.gitbook/assets/screenshots/azure chatopen ai in a workflow.png" alt="" /><figcaption><p>Azure ChatOpenAI Node in a workflow &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Experiment with different temperature settings to find the right balance between creativity and coherence in the AI's responses.
2. Use the max tokens parameter to control the length of the generated responses.
3. Enable image uploads when working with visual content to leverage multimodal capabilities.
4. Adjust frequency and presence penalties to fine-tune the AI's response style and repetitiveness.

## Troubleshooting

1. If you encounter authentication errors:

    - Double-check your Azure OpenAI API credentials
    - Ensure your Azure subscription has access to the selected model

2. If responses are cut off:

    - Increase the max tokens parameter
    - Check if you're hitting Azure OpenAI API rate limits

3. For slow response times:
    - Consider adjusting the timeout parameter
    - Verify your network connection to Azure services

Remember to refer to the Azure OpenAI documentation for more detailed information on API usage and best practices.

<!-- TODO: Add a screenshot of a successful Azure ChatOpenAI interaction in AnswerAgentAI -->
</file>

<file path="sidekick-studio/chatflows/chat-models/chatanthropic.md">
---
description: ChatAnthropic - Powerful AI Chat Model Integration
---

# ChatAnthropic

## Overview

ChatAnthropic is a powerful AI chat model integration in AnswerAgentAI that allows you to interact with Anthropic's large language models using the Chat endpoint. This feature provides advanced natural language processing capabilities for various applications, including conversational AI, text generation, and more.

## Key Benefits

-   Access to state-of-the-art language models from Anthropic
-   Customizable parameters for fine-tuned outputs
-   Optional image upload support for multimodal interactions

## How to Use

1. Connect your Anthropic API Credential:

    - Locate the "Connect Credential" option in the ChatAnthropic node settings.
    - Select or add your Anthropic API credentials.

2. Configure the Model:

    - Choose a model name from the available options (default: claude-3-5-sonnet@20240620).
    - Adjust the temperature setting (default: 0.9) to control the randomness of the output.

3. Set Additional Parameters (Optional):

    - Max Tokens: Limit the length of the generated response.
    - Top P: Control the diversity of the output.
    - Top K: Limit the vocabulary used in responses.

4. Enable Image Uploads (Optional):

    - Toggle "Allow Image Uploads" to enable multimodal interactions with claude-3-\* models.

5. Integrate with Other Components:
    - Connect the ChatAnthropic node to other components in your AnswerAgentAI workflow, such as LLMChain, Conversation Chain, ReAct Agent, or Conversational Agent.

<!-- TODO: Screenshot of the ChatAnthropic node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/chat anthropic.png" alt="" /><figcaption><p>Chat Anthropic Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different temperature settings to find the right balance between creativity and coherence in the AI's responses.
2. When using image uploads, ensure that your workflow is compatible (e.g., LLMChain, Conversation Chain, or Conversational Agent).
3. Monitor your token usage to optimize costs and performance.
4. Start with the default model and parameters, then adjust as needed for your specific use case.

## Troubleshooting

1. API Key Issues:

    - Ensure your Anthropic API key is correctly entered in the credentials section.
    - Verify that your API key has the necessary permissions for the selected model.

2. Performance Problems:

    - If responses are slow, try reducing the Max Tokens or adjusting the Top P and Top K values.
    - Check your internet connection, as API calls require a stable connection.

3. Unexpected Outputs:

    - Review your input prompts and adjust the temperature setting for more predictable results.
    - Ensure you're using the appropriate model for your task (e.g., claude-3-\* for image-related tasks).

4. Image Upload Errors:
    - Confirm that "Allow Image Uploads" is enabled for multimodal interactions.
    - Verify that you're using a compatible workflow component (LLMChain, Conversation Chain, or Conversational Agent).

By following this guide, you'll be able to effectively integrate and utilize the powerful ChatAnthropic feature in your AnswerAgentAI projects, enhancing your AI-driven applications with advanced language processing capabilities.
</file>

<file path="sidekick-studio/chatflows/chat-models/chatcohere.md">
---
description: ChatCohere - Interact with Cohere's Chat AI Models
---

# ChatCohere

## Overview

ChatCohere is a powerful feature in AnswerAgentAI that allows you to interact with Cohere's advanced chat AI models. This integration enables you to leverage Cohere's natural language processing capabilities for various conversational AI tasks.

## Key Benefits

-   Access to state-of-the-art chat AI models from Cohere
-   Customizable settings for fine-tuned responses
-   Seamless integration with AnswerAgentAI's workflow

## How to Use

Follow these steps to set up and use the ChatCohere feature:

1. Navigate to the Chat Models section in AnswerAgentAI.
2. Locate and select the ChatCohere node.

<!-- TODO: Screenshot of the ChatCohere node in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/chatcohere node configuration.png" alt="" /><figcaption><p>ChatCohere Node &#x26; Drop UI</p></figcaption></figure>

3. Configure the ChatCohere node with the following settings:

    a. **Connect Credential**: Link your Cohere API credentials.

    b. **Cache** (optional): Select a cache option if desired.

    c. **Model Name**: Choose a Cohere model (default is 'command-r').

    d. **Temperature**: Set the creativity level (default is 0.7, range 0-1).

<!-- TODO: Screenshot of the ChatCohere configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/chatcohere node in a workflow.png" alt="" /><figcaption><p> ChatCohere Node Configuration &#x26; Drop UI</p></figcaption></figure>

4. Connect the ChatCohere node to other nodes in your AnswerAgentAI workflow.
5. Run your workflow to start interacting with the Cohere chat model.

## Tips and Best Practices

1. Experiment with different temperature settings to find the right balance between creativity and coherence for your use case.
2. Use the cache option to improve response times for frequently asked questions.
3. Choose the appropriate model based on your specific requirements (e.g., language understanding, generation, or task-specific models).
4. Always ensure you have sufficient API credits in your Cohere account to avoid interruptions in your workflow.

## Troubleshooting

1. **API Key Issues**: If you encounter authentication errors, double-check your Cohere API key in the credential settings.

2. **Model Unavailable**: If a selected model is unavailable, try refreshing the model list or choose a different model.

3. **Unexpected Responses**: If you're getting unexpected results, try adjusting the temperature setting or reviewing your input prompts.

4. **Performance Issues**: If responses are slow, consider using the cache option or checking your internet connection.

For any persistent issues, refer to the AnswerAgentAI support documentation or contact our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/chat-models/chathuggingface.md">
---
description: ChatHuggingFace - Interact with Hugging Face language models
---

# ChatHuggingFace

## Overview

ChatHuggingFace is a powerful feature in AnswerAgentAI that allows you to interact with Hugging Face's large language models. This feature provides a user-friendly interface to leverage state-of-the-art natural language processing capabilities for various tasks such as text generation, question-answering, and more.

## Key Benefits

-   Access to a wide range of pre-trained language models
-   Customizable parameters for fine-tuned outputs
-   Option to use your own inference endpoint for specialized needs

## How to Use

1. Navigate to the ChatHuggingFace node in the AnswerAgentAI interface.
2. Configure the following settings:

    a. **Connect Credential**: Link your Hugging Face API credentials.

    b. **Model**: Enter the name of the Hugging Face model you want to use (e.g., "gpt2"). Leave this blank if using your own inference endpoint.

    c. **Endpoint** (Optional): If you're using your own inference endpoint, enter the URL here.

    d. **Additional Parameters** (Optional):

    - Temperature
    - Max Tokens
    - Top Probability
    - Top K
    - Frequency Penalty

    <!-- TODO: Screenshot of the ChatHuggingFace node configuration panel -->
    <figure><img src="/.gitbook/assets/screenshots/chathuggingface.png" alt="" /><figcaption><p> ChatHugging Face Node Configuration &#x26; Drop UI</p></figcaption></figure>

3. Connect the ChatHuggingFace node to other nodes in your AnswerAgentAI workflow.

4. Run your workflow to interact with the Hugging Face model.

## Tips and Best Practices

1. **Model Selection**: Choose the appropriate model for your task. Different models excel at different types of language processing tasks.

2. **Parameter Tuning**: Experiment with different parameter values to achieve the desired output. For example:

    - Increase temperature for more creative outputs
    - Decrease temperature for more focused and deterministic responses
    - Adjust max tokens to control the length of the generated text

3. **API Key Security**: Always keep your Hugging Face API key secure. Use the credential management system in AnswerAgentAI to safely store and use your API key.

4. **Custom Endpoints**: If you have specific requirements or want to use a fine-tuned model, consider setting up your own inference endpoint.

## Troubleshooting

1. **Model Not Found**: Ensure that you've entered the correct model name. Check the Hugging Face model hub for available models.

2. **API Key Issues**: Verify that your Hugging Face API key is correctly entered in the AnswerAgentAI credential manager.

3. **Endpoint Errors**: If using a custom endpoint, make sure the URL is correct and the endpoint is accessible.

4. **Unexpected Outputs**: Adjust the model parameters, especially temperature and max tokens, to fine-tune the output quality and length.

5. **Rate Limiting**: Be aware of the API rate limits for your Hugging Face account. If you're experiencing frequent timeouts, you may need to upgrade your plan or optimize your usage.

Remember that different models may have different available parameters. Always refer to the specific model's documentation on the Hugging Face website for the most accurate information on available settings and best practices.

<!-- TODO: Screenshot of a sample output from the ChatHuggingFace node -->

By leveraging the ChatHuggingFace feature in AnswerAgentAI, you can easily integrate powerful open-sourcelanguage models into your workflows, enabling sophisticated natural language processing capabilities for a wide range of applications.
</file>

<file path="sidekick-studio/chatflows/chat-models/chatlocalai.md">
---
description: Using ChatLocalAI with AnswerAgentAI
---

# ChatLocalAI Integration

## Overview

ChatLocalAI is a powerful feature in AnswerAgentAI that allows you to run language models locally or on-premise using consumer-grade hardware. It provides a drop-in replacement REST API compatible with OpenAI API specifications, supporting multiple model families in the ggml format.

## Key Benefits

-   Run language models locally without relying on external cloud services
-   Maintain data privacy and security by keeping everything on-premise
-   Reduce costs associated with cloud-based AI services
-   Support for various model families compatible with the ggml format

## How to Use

### Setting up LocalAI

1. Clone the LocalAI repository:

    ```bash
    git clone https://github.com/go-skynet/LocalAI
    ```

2. Navigate to the LocalAI directory:

    ```bash
    cd LocalAI
    ```

3. Copy your desired model to the `models/` directory. For example, to download the gpt4all-j model:

    ```bash
    wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j
    ```

    <!-- TODO: Screenshot showing the downloaded model in the models folder -->

4. Start the LocalAI service using Docker:

    ```bash
    docker compose up -d --pull always
    ```

5. Verify the API is accessible:

    ```bash
    curl http://localhost:8080/v1/models
    ```

### Integrating with AnswerAgentAI

1. Open your AnswerAgentAI canvas.

2. Drag and drop a new ChatLocalAI component onto the canvas.

    <!-- TODO: Screenshot of dragging ChatLocalAI component onto the canvas -->
     <figure><img src="/.gitbook/assets/screenshots/chatlocalainnode.png" alt="" /><figcaption><p> ChatLocalAI Node &#x26; Drop UI</p></figcaption></figure>

3. Configure the ChatLocalAI component:

    - Set the **Base Path** to `http://localhost:8080/v1`
    - Set the **Model Name** to the filename of your model (e.g., `ggml-gpt4all-j.bin`)

    <!-- TODO: Screenshot of the configured ChatLocalAI component -->
     <figure><img src="/.gitbook/assets/screenshots/chatlocalai.png" alt="" /><figcaption><p> ChatLocalAI Node Configuration &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different models to find the best balance between performance and resource usage for your specific use case.

2. If you're running both AnswerAgentAI and LocalAI on Docker, you may need to adjust the base path:

    - For Windows/macOS: Use `http://host.docker.internal:8080/v1`
    - For Linux: Use `http://172.17.0.1:8080/v1`

3. Regularly update your LocalAI installation to benefit from the latest improvements and model compatibility.

4. If you prefer a user-friendly interface for managing local models, consider using [LM Studio](https://lmstudio.ai/) in conjunction with LocalAI.

## Troubleshooting

1. **Issue**: Cannot connect to LocalAI API
   **Solution**: Ensure that the LocalAI service is running and that the base path is correct. Check your firewall settings if necessary.

2. **Issue**: Model not found
   **Solution**: Verify that the model file is present in the `models/` directory of your LocalAI installation and that the model name in AnswerAgentAI matches the filename exactly.

3. **Issue**: Poor performance or high resource usage
   **Solution**: Try using a smaller or more efficient model, or upgrade your hardware if possible.

For more detailed information and advanced usage, refer to the [LocalAI documentation](https://localai.io/basics/getting_started/index.html).

<!-- TODO: Embed video tutorial on using LocalAI with AnswerAgentAI -->
</file>

<file path="sidekick-studio/chatflows/chat-models/chatopenai-custom.md">
---
description: Use custom fine-tuned OpenAI models or try new models with AnswerAgentAI
---

# ChatOpenAI Custom

## Overview

The ChatOpenAI Custom feature allows you to use custom fine-tuned OpenAI models or experiment with new models before they are officially added to AnswerAgentAI's OpenAI model selection. This feature provides flexibility for advanced users who want to leverage their own fine-tuned models or test cutting-edge OpenAI offerings.

## Key Benefits

-   Use custom fine-tuned OpenAI models tailored to your specific needs
-   Experiment with new OpenAI models before they're officially supported in AnswerAgentAI
-   Customize various parameters to optimize model performance

## How to Use

1. In the AnswerAgentAI interface, locate and select the "ChatOpenAI Custom" node from the "Chat Models" category.

2. Configure the node with the following settings:

    a. Connect Credential: Link your OpenAI API credential (optional).

    b. Model Name: Enter the name of your custom or new model (e.g., "ft:gpt-3.5-turbo:my-org:custom_suffix:id").

    c. Temperature: Set the randomness of the model's output (default: 0.9).

    d. Max Tokens: Specify the maximum number of tokens in the response (optional).

    e. Additional Parameters: Configure advanced settings like Top Probability, Frequency Penalty, Presence Penalty, Timeout, BasePath, and BaseOptions as needed.

3. Connect the ChatOpenAI Custom node to other nodes in your AnswerAgentAI workflow.

4. Run your workflow to utilize the custom or new OpenAI model.

<!-- TODO: Add a screenshot of the ChatOpenAI Custom node configuration panel -->
 <figure><img src="/.gitbook/assets/screenshots/chatopenai custom node configuration.png" alt="" /><figcaption><p> ChatOpenAI Custom Node Configuration &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. When using fine-tuned models, ensure you use the correct model name format: "ft:base-model:org:custom-name:id".

2. Experiment with different temperature settings to find the right balance between creativity and coherence for your use case.

3. Use the BaseOptions parameter to pass any additional configuration options supported by the OpenAI API.

4. Keep your API key secure and never share it publicly.

## Troubleshooting

1. Model not found: Double-check the model name and ensure it's correctly formatted for fine-tuned models.

2. API errors: Verify your OpenAI API key is valid and has the necessary permissions to access the specified model.

3. Unexpected results: Adjust the temperature and other parameters to fine-tune the model's output.

## Important Notes

-   This feature does not support streaming responses. If you require streaming, use the standard OpenAI models provided in AnswerAgentAI.

-   Custom models may have different token limits or capabilities compared to standard OpenAI models. Refer to your model's documentation for specific details.

-   Be aware of any additional costs associated with using custom or experimental models through the OpenAI API.

By using the ChatOpenAI Custom feature, you can harness the power of specialized models and stay at the forefront of AI technology within your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/chat-models/chatopenai.md">
---
description: ChatOpenAI - Wrapper for OpenAI's Chat Models
---

# ChatOpenAI

## Overview

ChatOpenAI is a powerful wrapper around OpenAI's large language models that use the Chat endpoint. It serves as the default chat model for most workflows in AnswerAgentAI, providing a seamless integration with OpenAI's GPT models.

## Key Benefits

-   Easy integration with OpenAI's most advanced language models
-   Customizable parameters for fine-tuned responses
-   Support for image uploads and multi-modal interactions

## How to Use

1. In the AnswerAgentAI interface, locate the "ChatOpenAI" node in the "Chat Models" category.
2. Drag and drop the ChatOpenAI node into your workflow.
3. Connect your OpenAI API credentials:

    - Click on the node to open its settings.
    - Under "Connect Credential", select or add your OpenAI API key.

4. Configure the model parameters:

    - Choose a Model Name (default is "gpt-3.5-turbo")
    - Adjust Temperature, Max Tokens, and other parameters as needed
    - Enable or disable image uploads and set image resolution if required

5. Connect the ChatOpenAI node to other nodes in your workflow to utilize its capabilities.

<!-- TODO: Screenshot of ChatOpenAI node configuration panel -->
 <figure><img src="/.gitbook/assets/screenshots/chatopenai node configuration panel.png" alt="" /><figcaption><p> ChatOpenAI Node Configuration &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Experiment with different models: While ChatOpenAI with gpt-3.5-turbo is the default, don't hesitate to try other models. Some may be better suited for your specific use case and could potentially be more cost-effective.

2. Optimize temperature: A lower temperature (closer to 0) will make responses more deterministic, while a higher value (closer to 1) will make them more creative and diverse.

3. Use max tokens wisely: Set an appropriate max token limit to control response length and manage costs.

4. Leverage image capabilities: If your use case involves visual content, enable the "Allow Image Uploads" option to utilize multi-modal features.

5. Monitor and adjust: Keep an eye on your model's performance and costs. Adjust parameters or switch models if needed to optimize your workflow.

## Troubleshooting

1. API Key Issues:

    - Ensure your OpenAI API key is correctly entered and has sufficient credits.
    - Check if you have access to the selected model (some models may require special access).

2. Unexpected Responses:

    - Review and adjust the temperature and other parameters.
    - Ensure your prompts are clear and well-structured.

3. Token Limit Errors:

    - Increase the Max Tokens parameter or reduce the input length.
    - Consider using a model with a higher token limit if consistently hitting limits.

4. Performance Issues:
    - If responses are slow, check your internet connection and consider using a model with lower latency.
    - Adjust the timeout parameter if needed.

Remember, while ChatOpenAI is a great starting point, AnswerAgentAI offers a variety of other models that might better suit your specific needs. Don't hesitate to experiment with different options to find the perfect balance of performance, cost, and capabilities for your project.
</file>

<file path="sidekick-studio/chatflows/chat-models/google-ai.md">
---
description: Integrate Google's Gemini AI models for chat functionality in AnswerAgentAI
---

# ChatGoogleGenerativeAI

## Overview

ChatGoogleGenerativeAI is a wrapper around Google's Gemini large language models that use the Chat endpoint. This feature allows you to integrate powerful AI-driven chat capabilities into your AnswerAgentAI projects, leveraging Google's state-of-the-art language models.

## Key Benefits

-   Access to Google's advanced Gemini AI models for chat applications
-   Customizable settings for fine-tuning model behavior
-   Support for multimodal inputs, including text and images

## How to Use

1. Set up Google Generative AI credentials:

    - Obtain an API key from Google's Generative AI service
    - Configure the credential in AnswerAgentAI, naming it 'googleGenerativeAI'

2. Add the ChatGoogleGenerativeAI node to your flow:

    - Locate the node in the 'Chat Models' category
    - Drag and drop it into your workspace

3. Configure the node:

    - Connect the Google Generative AI credential
    - Select the desired model (e.g., 'gemini-pro')
    - Adjust parameters like temperature, max output tokens, etc.

4. Connect the node:

    - Link the ChatGoogleGenerativeAI node to other nodes in your flow, such as user inputs or processing steps

5. Run your flow:
    - Test the chat functionality with sample inputs
    - Observe the AI-generated responses based on your configuration

<!-- TODO: Add a screenshot of the ChatGoogleGenerativeAI node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/chatgooglegenerativeai.png" alt="" /><figcaption><p> ChatGoogleGenerativeAI Node Configuration &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Model Selection: Choose the appropriate model for your use case. 'gemini-pro' is suitable for most text-based chat applications, while 'gemini-pro-vision' or 'gemini-1.5-pro-latest' support multimodal inputs including images.

2. Temperature Setting: Adjust the temperature (0.0 to 1.0) to control the randomness of the output. Lower values produce more deterministic responses, while higher values increase creativity.

3. Safety Settings: Utilize the Harm Category and Harm Block Threshold options to control the safety level of the generated content. Refer to the [official Google AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes) for detailed information on these settings.

4. Token Management: Set an appropriate value for Max Output Tokens to control the length of the generated responses and manage your API usage.

5. Image Handling: If your application requires image inputs, enable the "Allow Image Uploads" option. This automatically switches to a vision-capable model when images are uploaded in supported chain types.

## Troubleshooting

1. API Key Issues:

    - Ensure your Google Generative AI API key is correctly set up in the AnswerAgentAI credentials.
    - Verify that the API key has the necessary permissions for the selected model.

2. Model Availability:

    - If a selected model is unavailable, check if it's supported in your region or if there are any service outages.

3. Content Filtering:

    - If responses are being filtered unexpectedly, review your safety settings and adjust the Harm Category and Harm Block Threshold options.

4. Performance Issues:

    - For slow responses, consider reducing the Max Output Tokens or adjusting other parameters that might impact processing time.

5. Image Upload Problems:
    - Ensure that images are provided as base64 encoded data URLs when using vision-capable models.
    - Verify that the "Allow Image Uploads" option is enabled for multimodal inputs.

For more advanced usage and detailed API documentation, refer to the [Google AI Gemini API documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference).
</file>

<file path="sidekick-studio/chatflows/chat-models/google-vertexai.md">
---
description: Integrate Google VertexAI with AnswerAgentAI for powerful language model capabilities
---

# Google VertexAI Integration

## Overview

Google VertexAI integration allows you to leverage Google's powerful language models within AnswerAgentAI. This feature enables you to use ChatGoogleVertexAI for various natural language processing tasks, enhancing your AI-powered applications.

## Key Benefits

-   Access to state-of-the-art language models from Google
-   Seamless integration with AnswerAgentAI's workflow
-   Customizable model parameters for fine-tuned outputs

## How to Use

### Prerequisites

1. Set up a Google Cloud Platform (GCP) account
2. Install the Google Cloud CLI

### Step 1: Enable Vertex AI API

1. Navigate to Vertex AI on your GCP console
2. Click "ENABLE ALL RECOMMENDED API"

<!-- TODO: Screenshot of Vertex AI API enable button -->

### Step 2: Create Credential File (Optional)

Choose one of the following methods:

#### Method 1: Use GCP CLI

1. Open your terminal and run:

    ```
    gcloud auth application-default login
    ```

2. Log in to your GCP account
3. Locate your credential file at `~/.config/gcloud/application_default_credentials.json`

#### Method 2: Use GCP Console

1. Go to GCP console and click "CREATE CREDENTIALS"
2. Create a service account
3. Fill in the service account details and click "CREATE AND CONTINUE"
4. Select an appropriate role (e.g., Vertex AI User) and click "DONE"
5. Click on the created service account, then "ADD KEY" -> "Create new key"
6. Select JSON format and click "CREATE" to download your credential file

### Step 3: Configure AnswerAgentAI

1. In AnswerAgentAI, go to the Credential page and click "Add credential"
2. Select "Google Vertex Auth"
3. Register your credential file using one of these options:
    - Enter the path to your credential file in "Google Application Credential File Path"
    - Copy and paste the content of your credential file into "Google Credential JSON Object"
4. Click "Add" to save the credential

### Step 4: Use ChatGoogleVertexAI in Your Workflow

1. In your AnswerAgentAI workflow, add a ChatGoogleVertexAI node
2. Configure the node parameters:
    - Select the credential you created
    - Choose a model name (e.g., "chat-bison")
    - Set temperature and other optional parameters
3. Connect the ChatGoogleVertexAI node to other nodes in your workflow

## Tips and Best Practices

-   Experiment with different temperature settings to balance creativity and coherence in outputs
-   Use the cache option to improve response times for repeated queries
-   Adjust maxOutputTokens to control the length of generated responses
-   Fine-tune topP and topK parameters for more diverse outputs

## Troubleshooting

-   If you encounter authentication errors, double-check your credential file and ensure it's correctly configured in AnswerAgentAI
-   For "Model not found" errors, verify that you've selected a valid model name from the available options
-   If you experience rate limiting, consider upgrading your GCP account or optimizing your usage

Remember to comply with Google's usage policies and monitor your API usage to manage costs effectively.
</file>

<file path="sidekick-studio/chatflows/chat-models/groqchat.md">
---
description: GroqChat - Blazing Fast LLM Inference with Groq
---

# GroqChat

## Overview

GroqChat is a powerful integration in AnswerAgentAI that leverages Groq's Lightning-fast Processing Unit (LPU) Inference Engine. This feature allows you to access state-of-the-art language models like Llama, Mixtral, and Gemma, including Groq's own fine-tuned versions, with unprecedented speed and efficiency.

## Key Benefits

-   **Blazing Fast Inference**: Experience lightning-quick responses from large language models.
-   **Access to Advanced Models**: Utilize cutting-edge models like Llama, Mixtral, and Gemma.
-   **Customization Options**: Fine-tune your interactions with adjustable parameters.

## How to Use

1. **Set up Groq API Credentials**:

    - Obtain a Groq API key from the Groq platform.
    - Add your Groq API credentials to AnswerAgentAI.

2. **Configure GroqChat Node**:

    - Drag and drop the GroqChat node into your workflow.
    - Connect it to your desired input and output nodes.

3. **Select Model and Parameters**:

    - Choose a model from the available options (e.g., llama3-70b-8192).
    - Adjust the temperature setting if needed (default is 0.9).
    - Optionally, connect a cache for improved performance.

4. **Run Your Workflow**:
    - Execute your workflow to start interacting with the GroqChat model.

<!-- TODO: Add a screenshot of the GroqChat node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/groq chat configuration.png" alt="" /><figcaption><p> GroqChat Node Configuration &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. **Model Selection**: Choose the appropriate model based on your specific use case. Larger models like llama3-70b-8192 offer more capabilities but may have longer processing times.

2. **Temperature Tuning**: Adjust the temperature to control the randomness of the output. Lower values (e.g., 0.2) produce more focused responses, while higher values (e.g., 0.8) generate more creative outputs.

3. **Caching**: Implement caching to improve response times for repeated queries and reduce API usage.

4. **API Key Security**: Always keep your Groq API key secure and never share it publicly.

## Troubleshooting

1. **Slow Responses**:

    - Ensure you have a stable internet connection.
    - Consider using a smaller model or implementing caching.

2. **API Key Issues**:

    - Verify that your Groq API key is correctly entered in the AnswerAgentAI credentials.
    - Check if your API key has the necessary permissions.

3. **Model Unavailability**:
    - If a specific model is unavailable, try selecting an alternative model from the list.
    - Check Groq's status page for any ongoing issues or maintenance.

GroqChat in AnswerAgentAI opens up new possibilities for blazing-fast language model interactions. By leveraging Groq's LPU technology, you can now process complex language tasks at unprecedented speeds, making your AI workflows more efficient and responsive than ever before.
</file>

<file path="sidekick-studio/chatflows/chat-models/mistral-ai.md">
---
description: Integrate Mistral AI's powerful chat models into your AnswerAgentAI workflows
---

# ChatMistralAI

## Overview

ChatMistralAI is a powerful integration that allows you to use Mistral AI's advanced language models within your AnswerAgentAI workflows. This feature enables you to leverage state-of-the-art natural language processing capabilities for various tasks such as text generation, conversation, and more.

## Key Benefits

-   Access to cutting-edge language models from Mistral AI
-   Seamless integration with other AnswerAgentAI components
-   Customizable parameters for fine-tuned outputs

## How to Use

Follow these steps to set up and use the ChatMistralAI feature in AnswerAgentAI:

1. **Prerequisites**

    - Create a Mistral AI account at [https://mistral.ai/](https://mistral.ai/)
    - Generate an API key from the [Mistral AI console](https://console.mistral.ai/user/api-keys/)

2. **Add ChatMistralAI to Your Workflow**
    - In the AnswerAgentAI interface, navigate to the "Chat Models" section
    - Drag and drop the "ChatMistralAI" node into your workflow canvas

<!-- TODO: Screenshot of dragging ChatMistralAI node into the workflow -->
<figure><img src="/.gitbook/assets/screenshots/chatmistralai.png" alt="" /><figcaption><p> ChatMistral AI Node &#x26; Drop UI</p></figcaption></figure>
3. **Configure Credentials**
   - Click on the ChatMistralAI node to open its settings
   - Under "Connect Credential", click "Create New"
   - Enter a name for your credential (e.g., "MistralAI API Key")
   - Paste your Mistral AI API key into the designated field
   - Click "Save" to store your credential securely

<!-- TODO: Screenshot of the credential configuration dialog -->
<figure><img src="/.gitbook/assets/screenshots/chatmistral node configuration credentials.png" alt="" /><figcaption><p> ChatMistral AI Node Configuration &#x26; Drop UI</p></figcaption></figure>

4. **Adjust Model Parameters**

    - In the ChatMistralAI node settings, you can customize various parameters:
        - Model Name: Choose from available Mistral AI models
        - Temperature: Adjust the randomness of the output (0.0 to 1.0)
        - Max Output Tokens: Set a limit on the generated text length
        - Top Probability: Fine-tune the token selection process
        - Random Seed: Ensure reproducible results (optional)
        - Safe Mode: Enable or disable content filtering
        - Override Endpoint: Use a custom API endpoint (advanced users)

5. **Connect to Other Nodes**
    - Link the ChatMistralAI node to other components in your workflow
    - Use the output from ChatMistralAI as input for subsequent processing or actions

## Tips and Best Practices

1. Experiment with different temperature settings to find the right balance between creativity and coherence for your use case.
2. Use the Max Output Tokens parameter to control the length of generated content and manage API usage.
3. Enable Safe Mode when working with potentially sensitive or public-facing applications.
4. Leverage the Random Seed parameter for consistent outputs in testing or when reproducibility is important.

## Troubleshooting

1. **API Key Issues**

    - Ensure your Mistral AI API key is correctly entered in the credential settings
    - Check that your API key has not expired or been revoked in the Mistral AI console

2. **Model Availability**

    - If a specific model is unavailable, it may be undergoing maintenance or updates. Try selecting a different model or check the Mistral AI status page for any announcements.

3. **Unexpected Outputs**

    - Review your input prompts and adjust the temperature or top probability settings
    - Consider using a different model that may be more suitable for your specific task

4. **Rate Limiting**
    - If you encounter rate limit errors, review your Mistral AI account usage and consider upgrading your plan if necessary

For any persistent issues or advanced configurations, refer to the [Mistral AI documentation](https://docs.mistral.ai/) or contact AnswerAgentAI support for assistance.
</file>

<file path="sidekick-studio/chatflows/chat-models/README.md">
---
description: Powerful Chat Models in AnswerAgentAI
---

# Chat Models

## Overview

Chat models are a cornerstone feature of AnswerAgentAI, providing powerful natural language processing capabilities for a wide range of applications. By integrating various state-of-the-art language models, AnswerAgentAI empowers users to create sophisticated conversational AI experiences with ease.

## Key Benefits

1. **Versatility**: Access a diverse array of chat models from leading providers, including OpenAI, Google, Anthropic, and more.
2. **Customization**: Fine-tune model parameters to achieve optimal performance for your specific use case.
3. **Scalability**: Easily switch between models or providers as your needs evolve.
4. **Privacy and Control**: Options for both cloud-based and on-premise deployments to meet various security requirements.
5. **Cost-Effectiveness**: Choose models that balance performance and cost for your specific needs.

## Best Practices for Chat Models in AnswerAgentAI

### 1. Model Selection

Choose the appropriate model based on your specific requirements:

-   **OpenAI GPT Models**: Ideal for general-purpose natural language tasks and creative content generation.
-   **Google VertexAI/PaLM**: Excellent for multilingual applications and complex reasoning tasks.
-   **Anthropic Claude**: Great for long-form content and tasks requiring strong ethical considerations.
-   **LocalAI**: Perfect for on-premise deployments and applications with strict data privacy requirements.
-   **Hugging Face Models**: Suitable for customized or domain-specific language tasks.

### 2. Parameter Tuning

Optimize your chat model's performance by adjusting these common parameters:

-   **Temperature**: Control the randomness of outputs. Lower values (0.1-0.5) for more focused responses, higher values (0.7-1.0) for more creative outputs.
-   **Max Tokens**: Limit the length of generated responses to manage costs and ensure concise outputs.
-   **Top P / Top K**: Fine-tune token selection for a balance between diversity and coherence in responses.

### 3. Prompt Engineering

Craft effective prompts to get the best results from your chosen model:

-   Be clear and specific in your instructions.
-   Provide context and examples when necessary.
-   Use consistent formatting and structure across similar tasks.

### 4. Error Handling and Fallbacks

Implement robust error handling:

-   Set up fallback options in case of API failures or unexpected responses.
-   Monitor and log model performance to identify and address issues proactively.

### 5. Cost Management

Optimize your usage to control costs:

-   Cache frequently requested information to reduce API calls.
-   Use smaller or more efficient models for simpler tasks.
-   Implement usage limits and monitoring to prevent unexpected expenses.

## Available Chat Models in AnswerAgentAI

AnswerAgentAI offers integration with a wide range of chat models to suit various needs:

### Chat Model Nodes

-   [AWS ChatBedrock](aws-chatbedrock.md)
-   [Azure ChatOpenAI](azure-chatopenai.md)
-   [Anthropic](chatanthropic.md)
-   [Cohere](chatcohere.md)
-   [Google GenerativeAI](google-ai.md)
-   [Google VertexAI](google-vertexai.md)
-   [GroqChat](groqchat.md)
-   [HuggingFace](chathuggingface.md)
-   [LocalAI](chatlocalai.md)
-   [MistralAI](mistral-ai.md)
-   [OpenAI](chatopenai.md)
-   [OpenAI Custom](chatopenai-custom.md)
</file>

<file path="sidekick-studio/chatflows/document-loaders/airtable.md">
---
description: Load data from Airtable tables into AnswerAgentAI
---

# Airtable Document Loader

## Overview

The Airtable Document Loader is a powerful feature in AnswerAgentAI that allows you to import data from your Airtable tables directly into your AI workflows. This integration enables you to leverage your existing Airtable data for various AI-powered tasks and analyses.

## Key Benefits

-   Seamlessly import Airtable data into AnswerAgentAI
-   Customize data retrieval with field selection and filtering options
-   Enhance your AI workflows with real-time Airtable information

## How to Use

Follow these steps to use the Airtable Document Loader in AnswerAgentAI:

1. Navigate to the Document Loaders section in AnswerAgentAI.
2. Select the Airtable loader from the available options.
3. Connect your Airtable credential:
    - If you haven't added your Airtable API key yet, you'll need to do so in the Credentials section.
4. Enter the required information:
    - Base ID: Find this in your Airtable URL (e.g., `app11RobdGoX0YNsC`)
    - Table ID: Also found in your Airtable URL (e.g., `tblJdmvbrgizbYICO`)
5. (Optional) Provide additional parameters:
    - View ID: Specify a particular view of your table
    - Include Only Fields: List specific fields you want to import
    - Return All: Choose whether to import all records or a limited number
    - Limit: Set a maximum number of records to import (if not returning all)
    - Additional Metadata: Add extra information to your imported documents
    - Omit Metadata Keys: Exclude certain metadata from the imported documents
6. Configure any text splitting options if needed.
7. Run the loader to import your Airtable data into AnswerAgentAI.

<!-- TODO: Screenshot of the Airtable Document Loader configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/airtabledocloader.png" alt="" /><figcaption><p> Airtable Document Loader &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Use field IDs instead of names if your field names contain commas.
2. Start with a small dataset to test your configuration before importing large tables.
3. Utilize the "Include Only Fields" option to focus on relevant data and improve performance.
4. Consider using a Text Splitter for large text fields to optimize processing.
5. Add custom metadata to enrich your imported documents for better context in AI tasks.

## Troubleshooting

Common issues and solutions:

1. **Authentication Error**: Ensure your Airtable API key is correctly added to AnswerAgentAI credentials.
2. **"Base ID or Table ID not found"**: Double-check the IDs in your Airtable URL and ensure they're correctly entered.
3. **No data imported**: Verify that the specified view and fields exist in your Airtable.
4. **Rate limiting**: If you're importing large datasets, you may hit Airtable's rate limits. Try reducing the batch size or adding delays between requests.

If you encounter persistent issues, check the AnswerAgentAI logs for more detailed error messages or contact support for assistance.

Remember, the Airtable Document Loader is a powerful tool to bridge your Airtable data with AnswerAgentAI's capabilities. Experiment with different configurations to find the best setup for your specific use case.
</file>

<file path="sidekick-studio/chatflows/document-loaders/api-loader.md">
---
description: Load data from an API using the API Loader in AnswerAgentAI
---

# API Loader

## Overview

The API Loader is a powerful feature in AnswerAgentAI that allows you to fetch data from external APIs and use it in your workflows. This tool supports both GET and POST requests, making it versatile for various data retrieval scenarios.

## Key Benefits

-   Easily integrate external data sources into your AnswerAgentAI workflows
-   Support for both GET and POST requests to accommodate different API requirements
-   Customizable headers and body parameters for flexible API interactions

## How to Use

Follow these steps to use the API Loader in AnswerAgentAI:

1. In the AnswerAgentAI interface, locate and select the "API Loader" node.
2. Configure the API Loader with the following settings:

    a. **Method**: Choose between GET or POST, depending on the API requirements.

    b. **URL**: Enter the full URL of the API endpoint you want to access.

    c. **Headers** (optional): Add any required headers for the API request in JSON format.

    d. **Body** (optional, for POST requests): Specify the request body in JSON format.

    e. **Text Splitter** (optional): Select a text splitter if you need to break down the API response into smaller chunks.

    f. **Additional Metadata** (optional): Add any extra metadata you want to include with the extracted documents.

    g. **Omit Metadata Keys** (optional): Specify any metadata keys you want to exclude from the final output.

3. Connect the API Loader node to other nodes in your workflow as needed.

4. Run your workflow to fetch data from the API and process it further.

<!-- TODO: Screenshot of the API Loader node configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/apiloader.png" alt="" /><figcaption><p> Airtable Document Loader &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Always check the API documentation for the correct URL, required headers, and body format before configuring the API Loader.

2. Use the "Headers" field to include authentication tokens or API keys if required by the external API.

3. For POST requests, ensure that the body is properly formatted as JSON to avoid errors.

4. When dealing with large API responses, consider using a Text Splitter to break down the content into more manageable chunks for further processing.

5. Utilize the "Additional Metadata" field to add context or identifying information to the documents created from the API response.

## Troubleshooting

1. **API request fails**:

    - Double-check the URL for typos
    - Verify that all required headers are included
    - Ensure your internet connection is stable

2. **Empty response**:

    - Confirm that the API endpoint is correct and returns data
    - Check if authentication is required and properly implemented

3. **Error parsing JSON**:

    - Verify that the API response is in valid JSON format
    - Consider using a JSON validator tool to check the response structure

4. **Rate limiting issues**:
    - Implement appropriate delays between requests if you're making multiple API calls
    - Check the API documentation for rate limiting guidelines

If you encounter persistent issues, consult the API's documentation or reach out to the AnswerAgentAI support team for assistance.
</file>

<file path="sidekick-studio/chatflows/document-loaders/apify-website-content-crawler.md">
---
description: Load website content using Apify Website Content Crawler
---

# Apify Website Content Crawler

The Apify Website Content Crawler is a powerful tool that allows you to extract and load content from websites for use in AnswerAgentAI. This feature is particularly useful for gathering large amounts of web data for analysis, research, or training language models.

## Overview

The Apify Website Content Crawler uses advanced web scraping techniques to navigate through websites, clean HTML content, and convert it into a format suitable for processing within AnswerAgentAI. It can crawl multiple pages, follow links, and extract text content efficiently.

## Key Benefits

-   Easily gather large amounts of web content from specified URLs
-   Clean and process HTML to extract relevant text content
-   Customize crawling behavior to suit your specific needs

## How to Use

1. In the AnswerAgentAI interface, locate and add the "Apify Website Content Crawler" node to your workflow.

2. Configure the node with the following settings:

    a. Text Splitter (Optional): Connect a Text Splitter node if you want to split the extracted content into smaller chunks.

    b. Start URLs: Enter one or more URLs where the crawler should begin, separated by commas.
    Example: `https://www.example.com, https://blog.example.com`

    c. Crawler Type: Choose the crawling engine that best suits your needs:

    - Headless web browser (Chrome+Playwright)
    - Stealthy web browser (Firefox+Playwright)
    - Raw HTTP client (Cheerio)
    - Raw HTTP client with JavaScript execution (JSDOM) [experimental]

    d. Max Crawling Depth: Set the maximum number of links the crawler should follow from the start URL(s).

    e. Max Crawl Pages: Specify the maximum number of pages to crawl.

    f. Additional Input (Optional): Provide any additional configuration options in JSON format.

    g. Additional Metadata (Optional): Add extra metadata to be included with the extracted documents.

    h. Omit Metadata Keys (Optional): Specify metadata keys to exclude from the final output.

3. Connect your Apify API credentials:

    - If you haven't already, create an Apify account and obtain an API token.
    - In AnswerAgentAI, create a new credential for the Apify API and enter your API token.

4. Connect the Apify Website Content Crawler node to other nodes in your workflow as needed.

5. Run your workflow to execute the web crawling and content extraction process.

## Tips and Best Practices

1. Start with a small crawl (low Max Crawling Depth and Max Crawl Pages) to test your configuration before scaling up.

2. Use the Stealthy web browser option if you encounter websites that block traditional web scrapers.

3. Respect website terms of service and robots.txt files when crawling.

4. Utilize the Additional Input option to fine-tune crawler behavior for specific websites.

5. Combine the Apify Website Content Crawler with Text Splitters and Vector Stores for efficient document processing and storage.

## Troubleshooting

1. If the crawler fails to extract content, try a different Crawler Type or adjust the Additional Input settings.

2. Ensure your Apify API token is correct and has sufficient permissions.

3. If you encounter rate limiting or blocking, reduce crawling speed or use the Stealthy web browser option.

4. For websites with complex JavaScript, use the Headless web browser or JSDOM options to ensure proper content rendering.

<!-- TODO: Add a screenshot of the Apify Website Content Crawler node configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/apifywebsitecontentcrawler.png" alt="" /><figcaption><p> Apify website Content Crawler Node Configuration &#x26; Drop UI</p></figcaption></figure>

By following these instructions, you can effectively use the Apify Website Content Crawler to gather web content for your AnswerAgentAI projects. This powerful tool opens up a world of possibilities for data collection and analysis within your workflows.

## Resources

-   [Website Content Crawler](https://apify.com/apify/website-content-crawler)
</file>

<file path="sidekick-studio/chatflows/document-loaders/cheerio-web-scraper.md">
---
description: Load and process web content using Cheerio Web Scraper in AnswerAgentAI
---

# Cheerio Web Scraper

## Overview

The Cheerio Web Scraper is a powerful tool in AnswerAgentAI that allows you to load and process data from web pages. It provides flexible options for scraping content, handling relative links, and customizing the extracted data.

## Key Benefits

-   Easily extract content from web pages using CSS selectors
-   Crawl websites or scrape XML sitemaps to process multiple pages
-   Customize metadata and control the amount of data extracted

## How to Use

1. Add the Cheerio Web Scraper node to your AnswerAgentAI workflow.
2. Configure the node with the following settings:

    a. **URL**: Enter the target web page URL.

    b. **Text Splitter** (optional): Connect a Text Splitter node to process the extracted content.

    c. **Get Relative Links Method** (optional): Choose between "Web Crawl" or "Scrape XML Sitemap" to process multiple pages.

    d. **Get Relative Links Limit** (optional): Set the maximum number of relative links to process (0 for unlimited).

    e. **Selector (CSS)** (optional): Specify a CSS selector to target specific content on the page.

    f. **Additional Metadata** (optional): Add custom metadata to the extracted documents.

    g. **Omit Metadata Keys** (optional): Exclude specific metadata keys from the output.

3. Connect the Cheerio Web Scraper node to other nodes in your workflow to process the extracted data further.

## Tips and Best Practices

1. Use specific CSS selectors to target the exact content you need, reducing noise in your extracted data.
2. When crawling multiple pages, start with a small limit to test your workflow before increasing the number of pages processed.
3. Combine the Cheerio Web Scraper with Text Splitters and other processing nodes to create more refined document chunks.
4. Utilize the Additional Metadata field to add custom information to your documents, such as source type or category.

## Troubleshooting

1. **Invalid URL error**: Ensure that the URL you've entered is correct and includes the proper protocol (http:// or https://).
2. **No relative links found**: Check that the target website allows crawling and that the XML sitemap is accessible (if using the Scrape XML Sitemap method).
3. **Slow performance**: When processing many pages, consider increasing the limit gradually to find the optimal balance between data quantity and processing time.

<!-- TODO: Screenshot of the Cheerio Web Scraper node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/cheeriowebscraper.png" alt="" /><figcaption><p> Cheerio Web Scraper Node Configuration &#x26; Drop UI</p></figcaption></figure>
## API Reference

For advanced usage, you can refer to the Cheerio Web Scraper API documentation:

```typescript
class Cheerio_DocumentLoaders implements INode {
    label: string
    name: string
    version: number
    description: string
    type: string
    icon: string
    category: string
    baseClasses: string[]
    inputs: INodeParams[]

    constructor()
    async init(nodeData: INodeData, _: string, options: ICommonObject): Promise<any>
}
```

The `init` method is the main function that processes the input and returns the extracted documents. It handles URL validation, web crawling or XML sitemap scraping, content extraction, and metadata customization.

Remember to handle errors and edge cases when integrating the Cheerio Web Scraper into your AnswerAgentAI workflows, especially when dealing with external websites that may change or become unavailable.

## Resources

-   [LangChain JS Cheerio](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/web_cheerio)
-   [Cheerio](https://cheerio.js.org/)
</file>

<file path="sidekick-studio/chatflows/document-loaders/confluence.md">
---
description: Load and process documents from Confluence in AnswerAgentAI
---

# Confluence Document Loader

## Overview

The Confluence Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process documents from your Confluence workspace. This tool seamlessly integrates with your Confluence account, enabling you to leverage your existing knowledge base within AnswerAgentAI.

## Key Benefits

-   Easily import Confluence pages and spaces into AnswerAgentAI
-   Maintain up-to-date information by connecting directly to your Confluence workspace
-   Customize document processing with text splitting and metadata options

## How to Use

Follow these steps to use the Confluence Document Loader:

1. In the AnswerAgentAI interface, locate and select the Confluence Document Loader node.

2. Configure the following required settings:

    - Connect Credential: Choose the appropriate Confluence API credential (Cloud or Server/DC).
    - Base URL: Enter your Confluence base URL (e.g., `https://example.atlassian.net/wiki`.
    - Space Key: Provide the Confluence Space Key you want to import documents from.

3. (Optional) Configure additional settings:

    - Text Splitter: Select a text splitter to process the imported documents.
    - Limit: Set a maximum number of documents to import (0 for no limit).
    - Additional Metadata: Add custom metadata to the imported documents.
    - Omit Metadata Keys: Specify metadata keys to exclude from the imported documents.

4. Connect the Confluence Document Loader node to other nodes in your AnswerAgentAI workflow.

5. Run your workflow to import and process the Confluence documents.

<!-- TODO: Add a screenshot of the Confluence Document Loader node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/confluence.png" alt="" /><figcaption><p> Confluence Document Loader Node Configuration &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Use the Space Key carefully: Make sure you have the correct Space Key to import the desired documents. You can find the Space Key by following the [official Atlassian guide](https://community.atlassian.com/t5/Confluence-questions/How-to-find-the-key-for-a-space/qaq-p/864760).

2. Optimize performance: If you're working with a large Confluence space, consider using the "Limit" option to import a smaller subset of documents initially.

3. Customize metadata: Use the "Additional Metadata" option to add relevant information to your imported documents, making them easier to organize and search within AnswerAgentAI.

4. Text splitting: If you're working with large Confluence pages, use a Text Splitter to break them into smaller, more manageable chunks for processing.

## Troubleshooting

1. Connection issues:

    - Ensure your Confluence API credentials are correct and up-to-date.
    - Check that your Base URL is accurate and includes the full path to your Confluence instance.

2. No documents imported:

    - Verify that the Space Key is correct and that you have permission to access the specified space.
    - Check if there are any documents in the selected space.

3. Metadata problems:
    - If you're not seeing expected metadata, make sure you haven't accidentally omitted important keys using the "Omit Metadata Keys" option.
    - When adding custom metadata, ensure your JSON format is correct in the "Additional Metadata" field.

By following this guide, you'll be able to efficiently import and process your Confluence documents within AnswerAgentAI, enhancing your workflows with valuable information from your existing knowledge base.
</file>

<file path="sidekick-studio/chatflows/document-loaders/contentful.md">
---
description: Load data from a Contentful Space into AnswerAgentAI
---

# Contentful Document Loader

## Overview

The Contentful Document Loader is a powerful feature that allows you to import content from your Contentful space directly into AnswerAgentAI. This integration enables you to leverage your existing Contentful content within AnswerAgentAI, making it easier to create AI-powered applications using your own data.

## Key Benefits

-   Seamlessly import content from Contentful into AnswerAgentAI
-   Customize content retrieval based on your specific needs
-   Maintain your content in Contentful while utilizing it in AI applications

## How to Use

1. Connect your Contentful account:

    - Select "Connect Credential" and choose your Contentful Delivery API credentials.

2. Configure the loader:

    - Choose between Delivery API or Preview API.
    - Set up your Content Type configuration using the Config Utility.
    - Adjust additional parameters as needed (e.g., Environment ID, Include Levels, Limit).

3. Run the loader to import your Contentful content into AnswerAgentAI.

<!-- TODO: Add a screenshot of the Contentful Document Loader configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/contentfuldocumentloader.png" alt="" /><figcaption><p> Contentful Document Loader Node Configuration &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use the Preview API during development to work with draft content.
2. Optimize performance by setting appropriate limits and include levels.
3. Utilize the Config Utility to fine-tune which content types and fields are imported.
4. Include field names in the output for better context in your AI applications.

## Troubleshooting

1. **Issue**: No content is being imported
   **Solution**: Verify that your API credentials are correct and that you have the necessary permissions in Contentful.

2. **Issue**: Unexpected content structure
   **Solution**: Review your Config Utility settings to ensure you're targeting the correct content types and fields.

3. **Issue**: Performance issues with large datasets
   **Solution**: Adjust the "Limit" parameter or use pagination to import data in smaller chunks.

## Advanced Configuration

### Config Utility

The Config Utility allows you to specify how content should be extracted from Contentful. Here's an example configuration:

```json
{
    "mainContentType": {
        "contentType": "blogPost",
        "fieldsToParse": ["title", "body", "author"]
    },
    "embeddedContentTypes": [
        {
            "contentType": "author",
            "fieldsToParse": ["name", "bio"]
        }
    ],
    "richTextParsingRules": {
        "embedded-asset-block": true,
        "embedded-entry-block": true,
        "embedded-entry-inline": true
    },
    "fieldsForCitation": {
        "titleField": "fields.title",
        "slugField": "fields.slug",
        "urlPrefix": "https://mywebsite.com/"
    }
}
```

This configuration tells the loader to:

-   Focus on the "blogPost" content type
-   Parse specific fields from the main content type and embedded content types
-   Handle rich text fields and other embedded and related content
-   Generate citations using specified fields

### Search Query

You can use the "Search Query" parameter to filter the content you import. For example:

```json
{
    "content_type": "blogPost",
    "fields.category": "technology",
    "order": "-sys.createdAt"
}
```

This query would import only blog posts in the "technology" category, ordered by creation date (newest first).

By mastering these configuration options, you can tailor the Contentful Document Loader to fit your specific use case in AnswerAgentAI.
</file>

<file path="sidekick-studio/chatflows/document-loaders/csv-file.md">
---
description: Load and process data from CSV files in AnswerAgentAI
---

# CSV File Document Loader

## Overview

The CSV File Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process data from CSV (Comma-Separated Values) files. This tool is essential for users who need to work with structured data stored in spreadsheets or tabular formats.

## Key Benefits

-   Easy import of structured data from CSV files
-   Flexible options for data extraction and processing
-   Seamless integration with other AnswerAgentAI features

## How to Use

1. Select the CSV File Document Loader from the Document Loaders category.

<!-- TODO: Screenshot of the CSV File Document Loader node in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/csvfiledocumentloader.png" alt="" /><figcaption><p> CSV File Document Loader  &#x26; Drop UI</p></figcaption></figure>

2. Configure the loader with the following options:

    a. CSV File: Upload your CSV file or provide a reference to a file in storage.

    b. Text Splitter (optional): Choose a text splitter if you want to break down the content into smaller chunks.

    c. Single Column Extraction (optional): Specify a column name if you want to extract data from a single column.

    d. Additional Metadata (optional): Add any extra metadata you want to include with the extracted documents.

    e. Omit Metadata Keys (optional): Specify any metadata keys you want to exclude from the output.

<!-- TODO: Screenshot of the configuration options for the CSV File Document Loader -->
<figure><img src="/.gitbook/assets/screenshots/csv file node configuraation.png" alt="" /><figcaption><p> CSV File Document Loader Node Configuration &#x26; Drop UI</p></figcaption></figure>

3. Connect the CSV File Document Loader to other nodes in your AnswerAgentAI workflow.

4. Run your workflow to process the CSV data.

## Tips and Best Practices

1. When working with large CSV files, consider using a Text Splitter to break down the content into manageable chunks.

2. If you only need data from a specific column, use the Single Column Extraction option to streamline your workflow.

3. Utilize the Additional Metadata option to add context or categorization to your documents.

4. Use the Omit Metadata Keys feature to remove any sensitive or unnecessary information from the output.

## Troubleshooting

1. File format issues:

    - Ensure your file has a .csv extension.
    - Check that the file is properly formatted with comma-separated values.

2. Column name errors:

    - If using Single Column Extraction, verify that the specified column name exactly matches the header in your CSV file.

3. Metadata parsing problems:

    - When adding Additional Metadata, make sure it's in valid JSON format.

4. Performance concerns:
    - For very large CSV files, consider splitting them into smaller files or using a Text Splitter to improve processing speed.

## Output Options

The CSV File Document Loader offers two output options:

1. Document: Returns an array of document objects containing metadata and page content.
2. Text: Provides a concatenated string of the page content from all documents.

Choose the output that best fits your workflow needs.

<!-- TODO: Screenshot showing how to select the output option -->
<figure><img src="/.gitbook/assets/screenshots/csvfile output.png" alt="" /><figcaption><p> CSV File Document Loader Output &#x26; Drop UI</p></figcaption></figure>

By following this guide, you'll be able to effectively use the CSV File Document Loader in AnswerAgentAI to import and process your structured data from CSV files.
</file>

<file path="sidekick-studio/chatflows/document-loaders/custom-document-loader.md">
---
description: Custom Document Loader in AnswerAgentAI
---

# Custom Document Loader

## Overview

The Custom Document Loader is a powerful feature in AnswerAgentAI that allows you to create custom functions for loading documents. This feature provides flexibility in handling various document formats and sources, enabling you to tailor the document loading process to your specific needs.

## Key Benefits

-   Customizable document loading: Create custom functions to load documents from any source or format.
-   Flexible output options: Choose between returning document objects or plain text.
-   Integration with AnswerAgentAI variables: Utilize input variables and system variables in your custom functions.

## How to Use

1. Navigate to the Custom Document Loader in the Document Loaders category.

<!-- TODO: Screenshot of the Custom Document Loader node in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/customdocumentloader.png" alt="" /><figcaption><p> Custom Document Loader  &#x26; Drop UI</p></figcaption></figure>

2. Configure the Input Variables (optional):

    - Click on the "Input Variables" field.
    - Enter a JSON object with key-value pairs representing your input variables.
    - These variables can be used in your custom function with a `$` prefix (e.g., `$var`).

3. Write your Javascript Function:

    - Click on the "Javascript Function" field.
    - Write your custom function to load and process documents.
    - Ensure your function returns the correct format based on your chosen output type:
        - For "Document" output: Return an array of document objects containing `pageContent` and `metadata`.
        - For "Text" output: Return a string.

4. Select the desired output type:

    - Choose between "Document" or "Text" in the output options.

5. Connect the Custom Document Loader to other nodes in your AnswerAgentAI workflow.

6. Run your workflow to test the custom document loading function.

## Tips and Best Practices

1. Use meaningful variable names in your custom function to improve readability.
2. Leverage the `$input`, `$vars`, and `$flow` objects to access input data, variables, and flow information within your function.
3. Handle errors gracefully in your custom function to provide meaningful feedback.
4. When working with large documents, consider implementing pagination or chunking to improve performance.
5. Use the available built-in dependencies and external dependencies as needed in your custom function.

## Troubleshooting

1. Invalid JSON in Input Variables:

    - Ensure that the JSON object in the Input Variables field is properly formatted.
    - Use double quotes for keys and string values.

2. Function not returning the expected format:

    - Double-check that your function returns an array of document objects for "Document" output or a string for "Text" output.
    - Verify that document objects contain both `pageContent` and `metadata` properties.

3. Dependency issues:

    - If you need to use external dependencies, make sure they are allowed in your AnswerAgentAI environment.
    - Check the `TOOL_FUNCTION_BUILTIN_DEP` and `TOOL_FUNCTION_EXTERNAL_DEP` environment variables for available dependencies.

4. Errors in the custom function:
    - Use console.log statements to debug your function (output will be visible in the AnswerAgentAI logs).
    - Ensure that all asynchronous operations are properly handled using async/await or promises.

## Example Custom Function

Here's an example of a custom function that loads a simple document:

```javascript
return [
    {
        pageContent: 'This is the content of my custom document.',
        metadata: {
            title: 'Custom Document',
            source: 'Custom Document Loader',
            date: new Date().toISOString()
        }
    }
]
```

This function returns a single document object with page content and metadata. You can expand on this example to load documents from files, databases, or APIs based on your specific requirements.

Remember to test your custom document loader thoroughly to ensure it integrates smoothly with the rest of your AnswerAgentAI workflow.
</file>

<file path="sidekick-studio/chatflows/document-loaders/document-store.md">
---
description: Load data from pre-configured document stores in AnswerAgentAI
---

# Document Store

## Overview

The Document Store feature in AnswerAgentAI allows you to load data from pre-configured document stores, also known as knowledge bases. This powerful tool enables you to group multiple document loaders together, creating a centralized repository of information that can be easily accessed and utilized within your AnswerAgentAI workflows.

## Key Benefits

-   Centralized Data Management: Organize and access multiple documents from a single source.
-   Efficient Information Retrieval: Quickly load relevant data for your AI tasks.
-   Flexible Output Options: Choose between document objects or concatenated text output.

## How to Use

1. Add the Document Store node to your workflow.
2. Configure the node:
    - Select the desired document store from the "Select Store" dropdown.
3. Connect the Document Store node to other nodes in your workflow.
4. Run your workflow to load data from the selected document store.

<!-- TODO: Add a screenshot of the Document Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/documentstorenode.png.png" alt="" /><figcaption><p> Document Store Node Configuration  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Organize your document stores logically to make finding and using the right information easier.
-   Use descriptive names for your document stores to quickly identify their contents.
-   Regularly update and maintain your document stores to ensure the most current information is available.
-   Combine document stores with other AnswerAgentAI features for more complex data processing and AI tasks.

## Troubleshooting

-   If you don't see any document stores in the dropdown, ensure that you have created and synchronized at least one document store in your AnswerAgentAI account.
-   If the loaded data is not what you expected, double-check that you've selected the correct document store and that its contents are up-to-date.
-   Ensure that when you add new content that you process it through the Document Store node to ensure it is added to the knowledge base.

## Additional Information

For more detailed information on how to use document stores (also referred to as knowledge bases), please refer to the full documentation available at [Document Stores](../../../sidekick-studio/documents/). This comprehensive guide will provide you with in-depth instructions on creating, managing, and optimizing your document stores within AnswerAgentAI.

<!-- TODO: Add a screenshot showing the Document Store node connected in a workflow -->
<figure><img src="/.gitbook/assets/screenshots/documentstoreinaworkflow.png" alt="" /><figcaption><p> Document Store Node Configuration  &#x26; Drop UI</p></figcaption></figure>

Remember that document stores, or knowledge bases, are a core feature of AnswerAgentAI, allowing you to efficiently manage and utilize large amounts of information across various AI-powered tasks and workflows.
</file>

<file path="sidekick-studio/chatflows/document-loaders/docx-file.md">
---
description: Load and process data from DOCX files in AnswerAgentAI
---

# Docx File Loader

## Overview

The Docx File Loader is a powerful feature in AnswerAgentAI that allows you to extract and process content from Microsoft Word (.docx) files. This tool is essential for users who need to work with document-based data in their AI workflows.

## Key Benefits

-   Easily import content from .docx files into your AnswerAgentAI projects
-   Process and split large documents for more efficient handling
-   Add custom metadata to enhance document organization and searchability

## How to Use

1. In the AnswerAgentAI interface, locate and select the "Docx File" node from the "Document Loaders" category.

<!-- TODO: Screenshot of the Docx File node in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/docxfileloader.png" alt="" /><figcaption><p> Docx File Loader  &#x26; Drop UI</p></figcaption></figure>

2. Configure the node by setting the following parameters:

    a. **Docx File**: Upload your .docx file or provide a reference to a file in storage.

    b. **Text Splitter** (optional): Choose a text splitter to break down large documents into smaller chunks.

    c. **Additional Metadata** (optional): Add extra information to your documents in JSON format.

    d. **Omit Metadata Keys** (optional): Specify metadata keys to exclude from the final output.

3. Connect the Docx File Loader node to other nodes in your workflow as needed.

4. Run your workflow to process the .docx file and use the extracted content in subsequent steps.

## Tips and Best Practices

1. Use text splitters for large documents to improve processing efficiency and enable more granular analysis.

2. Leverage the Additional Metadata feature to add context or categorization to your documents.

3. When working with multiple files, you can provide a list of file references in the Docx File input.

4. Use the Omit Metadata Keys option to remove unnecessary metadata and keep your document data clean.

## Troubleshooting

1. **File not loading**: Ensure that your .docx file is not corrupted and is properly formatted.

2. **Unexpected content**: Some complex formatting in Word documents may not be preserved. For best results, use simple formatting in your source documents.

3. **Performance issues**: If processing large files is slow, try using a text splitter to break the document into smaller chunks.

<!-- TODO: Screenshot showing a sample workflow with the Docx File Loader connected to other nodes -->
<figure><img src="/.gitbook/assets/screenshots/docxfileinaworkflow.png" alt="" /><figcaption><p> Docx File Loader In a Workflow &#x26; Drop UI</p></figcaption></figure>

By mastering the Docx File Loader, you can efficiently incorporate document-based data into your AnswerAgentAI projects, opening up new possibilities for document analysis and processing in your AI workflows.
</file>

<file path="sidekick-studio/chatflows/document-loaders/figma.md">
---
description: Load and process Figma file data in AnswerAgentAI
---

# Figma Document Loader

## Overview

The Figma Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process data from Figma files. This tool is essential for designers and developers who want to integrate Figma designs into their AnswerAgentAI workflows.

## Key Benefits

-   Seamlessly import Figma file data into AnswerAgentAI
-   Process and analyze design elements within your AI workflows
-   Enhance collaboration between design and development teams

## How to Use

### 1. Connect Your Figma Account

Before using the Figma Document Loader, you need to connect your Figma account to AnswerAgentAI:

1. Go to the Credentials section in AnswerAgentAI.
2. Select "Figma API" from the list of available integrations.
3. Enter your Figma API access token.
4. Save the credential.

<!-- TODO: Screenshot of the Credentials section with Figma API selected -->
<figure><img src="/.gitbook/assets/screenshots/figmaapicredentials.png" alt="" /><figcaption><p> Figma API Credentials &#x26; Drop UI</p></figcaption></figure>

### 2. Configure the Figma Document Loader

1. In your AnswerAgentAI workflow, add a new node and select "Figma" from the "Document Loaders" category.
2. Configure the following settings:

    a. **File Key**: Enter the Figma file key. You can find this in the Figma file URL: `https://www.figma.com/file/:key/:title`. For example, in `https://www.figma.com/file/12345/Website`, the file key is `12345`.

    b. **Node IDs**: Enter a comma-separated list of Node IDs you want to load. To find Node IDs, use the [Node Inspector plugin](https://www.figma.com/community/plugin/758276196886757462/Node-Inspector) in Figma.

    c. **Recursive** (optional): Toggle this on if you want to load nested nodes.

    d. **Text Splitter** (optional): Select a text splitter if you want to break down the loaded content into smaller chunks.

    e. **Additional Metadata** (optional): Add any extra metadata as a JSON object.

    f. **Omit Metadata Keys** (optional): Specify metadata keys you want to exclude, separated by commas. Use \* to omit all default metadata keys.

<!-- TODO: Screenshot of the Figma Document Loader configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/figmaconfiguration.png" alt="" /><figcaption><p> Figma Document Loader Configuration Panel &#x26; Drop UI</p></figcaption></figure>

3. Connect the Figma credential you created earlier to this node.

4. Connect the output of this node to other nodes in your workflow as needed.

## Tips and Best Practices

1. **Optimize Node Selection**: Be specific with your Node IDs to load only the necessary content, improving performance and reducing unnecessary data processing.

2. **Use Text Splitters**: For large Figma files, consider using a text splitter to break down the content into manageable chunks for further processing.

3. **Leverage Metadata**: Use the Additional Metadata field to add context to your Figma data, making it easier to process in subsequent nodes.

4. **Regular Token Updates**: Keep your Figma API access token up to date to ensure uninterrupted access to your Figma files.

## Troubleshooting

1. **Unable to Load File**:

    - Ensure your Figma API access token is valid and has the necessary permissions.
    - Double-check the File Key for accuracy.

2. **Missing Nodes**:

    - Verify that the Node IDs are correct and exist in the specified Figma file.
    - If using the Recursive option, make sure the parent node ID is included.

3. **Performance Issues**:

    - If loading large files is slow, try selecting specific Node IDs instead of loading the entire file.
    - Consider using a Text Splitter to process the content in smaller chunks.

4. **Metadata Errors**:
    - When using the Additional Metadata field, ensure the JSON format is correct.
    - If certain metadata is not appearing, check the Omit Metadata Keys field to ensure you haven't excluded it.

By following this guide, you should be able to effectively use the Figma Document Loader in AnswerAgentAI
</file>

<file path="sidekick-studio/chatflows/document-loaders/folder-with-files.md">
---
description: Load and process multiple files from a folder (Local Only)
---

# Folder with Files

## Overview

The Folder with Files feature in AnswerAgentAI allows you to load and process multiple documents from a specified folder on your system. This powerful tool supports various file formats and provides options for recursive folder scanning, text splitting, and metadata management.

## Key Benefits

-   Load multiple files of different formats from a single folder
-   Customize document processing with text splitters and metadata options
-   Support for recursive folder scanning to process nested directories

## How to Use

1. In the AnswerAgentAI interface, locate and select the "Folder with Files" option from the Document Loaders category.

2. Configure the following required settings:

    - Folder Path: Enter the full path to the folder containing your documents.
    - Recursive: Choose whether to include files from subfolders (true) or only the specified folder (false).

3. (Optional) Configure additional settings:

    - Text Splitter: Select a text splitter to break down large documents into smaller chunks.
    - PDF Usage: Choose how to process PDF files (one document per page or per file).
    - Additional Metadata: Add extra metadata to the extracted documents in JSON format.
    - Omit Metadata Keys: Specify metadata keys to exclude from the final output.

4. Connect the Folder with Files node to other nodes in your AnswerAgentAI workflow as needed.

5. Run your workflow to process the documents from the specified folder.

<!-- TODO: Add a screenshot showing the Folder with Files node configuration in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/folderwithfiles.png" alt="" /><figcaption><p> Folder with Files Node Configuration &#x26; Drop UI</p></figcaption></figure>

<figure><img src="/.gitbook/assets/screenshots/folder with files in a workflow.png" alt="" /><figcaption><p> Folder with Files Nodes Configuration In a workflow &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Organize your files: Keep your documents well-organized in folders to make it easier to process them in batches.

2. Use text splitters: For large documents, consider using a text splitter to break them into more manageable chunks for processing.

3. Leverage metadata: Use the Additional Metadata option to add relevant information to your documents, such as source, date, or category.

4. Optimize PDF processing: Choose the appropriate PDF Usage option based on your needs. Use "One document per page" for granular analysis or "One document per file" for treating each PDF as a single unit.

5. Be mindful of file types: The Folder with Files feature supports a wide range of file formats, including JSON, TXT, CSV, DOCX, PDF, and various programming language files. Ensure your folder contains supported file types.

## Troubleshooting

1. Issue: Files are not being processed
   Solution: Double-check the folder path and ensure it's correct. Also, verify that you have the necessary permissions to access the folder and its contents.

2. Issue: Subfolders are not included
   Solution: Make sure the "Recursive" option is set to true if you want to process files from subfolders.

3. Issue: PDF processing is slow
   Solution: For large PDF files, consider using the "One document per file" option to reduce processing time, especially if you don't need page-level granularity.

4. Issue: Unexpected metadata in output
   Solution: Use the "Omit Metadata Keys" option to exclude specific metadata fields. You can use a comma-separated list of keys or "\*" to omit all default metadata.

5. Issue: Out of memory errors
   Solution: If you're processing a large number of files or very large documents, consider using a text splitter to break them into smaller chunks. This can help manage memory usage more effectively.

<!-- TODO: Add a screenshot showing example output or error messages for common issues -->

By following these instructions and best practices, you can effectively use the Folder with Files feature in AnswerAgentAI to process multiple documents from a single folder, streamlining your document analysis workflows.
</file>

<file path="sidekick-studio/chatflows/document-loaders/gitbook.md">
---
description: Load and process documentation from GitBook
---

# GitBook Document Loader

## Overview

The GitBook Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process documentation from GitBook websites. This tool is particularly useful for organizations that use GitBook for their documentation and want to integrate it into their AnswerAgentAI workflows.

## Key Benefits

-   Easily import documentation from GitBook websites
-   Option to load content from a single page or an entire GitBook site
-   Customize metadata and content processing for your specific needs

## How to Use

1. In the AnswerAgentAI interface, locate and select the "GitBook" option from the Document Loaders category.

<!-- TODO: Screenshot of the GitBook loader in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/gitbookloader.png" alt="" /><figcaption><p> Gitbook Loader &#x26; Drop UI</p></figcaption></figure>

2. Configure the GitBook loader with the following settings:

    a. **Web Path**: Enter the URL of the GitBook page or site you want to load. For example:

    - To load a single page: `https://docs.gitbook.com/product-tour/navigation`
    - To load an entire site: `https://docs.gitbook.com/`

    b. **Should Load All Paths**: Toggle this option on if you want to load content from all pages in the GitBook site. Leave it off to load only the specified page.

    c. **Text Splitter** (optional): Select a text splitter if you want to break down the loaded content into smaller chunks.

    d. **Additional Metadata** (optional): Add any custom metadata as a JSON object to be included with the loaded documents.

    e. **Omit Metadata Keys** (optional): Specify any default metadata keys you want to exclude from the loaded documents.

3. Click "Run" or "Save" to execute the GitBook loader with your specified settings.

<!-- TODO: Screenshot of the configured GitBook loader settings -->
<figure><img src="/.gitbook/assets/screenshots/gitbookloaderinaworkflow.png" alt="" /><figcaption><p> Gitbook Loader Configuration &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. When loading an entire GitBook site, be mindful of the amount of content you're importing. Large sites may take longer to process.

2. Use the Text Splitter option if you plan to use the loaded content with AI models that have token limits.

3. Leverage the Additional Metadata feature to add relevant information to your documents, such as author, date, or category.

4. Use the Omit Metadata Keys option to remove any sensitive or unnecessary information from the imported documents.

## Troubleshooting

1. **Issue**: The loader fails to import content
   **Solution**: Double-check the Web Path URL and ensure you have internet access. Verify that the GitBook site is publicly accessible.

2. **Issue**: Loading takes too long
   **Solution**: If you're loading an entire site, try loading specific pages instead. You can also use the Text Splitter to process content in smaller chunks.

3. **Issue**: Imported content is missing expected metadata
   **Solution**: Review your Omit Metadata Keys settings to ensure you haven't accidentally excluded important metadata fields.

By using the GitBook Document Loader, you can easily integrate your existing GitBook documentation into AnswerAgentAI, enhancing your ability to process and analyze your content effectively.
</file>

<file path="sidekick-studio/chatflows/document-loaders/github.md">
---
description: Load and process documents from GitHub repositories
---

# GitHub Document Loader

## Overview

The GitHub Document Loader is a powerful feature in AnswerAgentAI that allows you to load and process data directly from GitHub repositories. This tool is perfect for users who want to analyze, search, or work with content stored in GitHub, whether it's documentation, code, or other text-based files.

## Key Benefits

-   Easy access to GitHub repository content without manual downloading
-   Flexible options for loading specific branches, files, or directories
-   Ability to process both public and private repositories

## How to Use

1. In the AnswerAgentAI interface, locate and select the "GitHub" option in the Document Loaders category.

<!-- TODO: Screenshot of the GitHub loader option in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/githubloader.png" alt="" /><figcaption><p> Github Loader Configuration &#x26; Drop UI</p></figcaption></figure>

2. Configure the loader with the following required settings:

    - Repo Link: Enter the full URL of the GitHub repository (e.g., `https://github.com/the-answerai/answers-ai`
    - Branch: Specify the branch you want to load (default is "main")

3. (Optional) Customize your loader with additional settings:

    - Recursive: Enable this to load files from subdirectories
    - Max Concurrency: Set the maximum number of concurrent requests
    - Ignore Paths: Specify file patterns to ignore (e.g., `["*.md"]`)
    - Max Retries: Set the maximum number of retry attempts for failed requests
    - Text Splitter: Choose a text splitter to process the loaded documents
    - Additional Metadata: Add custom metadata to the extracted documents
    - Omit Metadata Keys: Exclude specific metadata keys from the loaded documents

4. If you're accessing a private repository, connect your GitHub API credential:
    - Click on "Connect Credential"
    - Select or add your GitHub API credentials

<!-- TODO: Screenshot of the credential connection process -->
<figure><img src="/.gitbook/assets/screenshots/githubapi.png" alt="" /><figcaption><p> Github API Configuration &#x26; Drop UI</p></figcaption></figure>

5. Run the loader to fetch and process the documents from the specified GitHub repository.

## Tips and Best Practices

1. Use the "Recursive" option to load entire directory structures, but be cautious with large repositories as it may take longer to process.

2. Leverage the "Ignore Paths" feature to exclude unnecessary files (like images or binaries) that might slow down the loading process.

3. When working with large repositories, adjust the "Max Concurrency" setting to optimize loading speed while respecting GitHub's rate limits.

4. Utilize the "Text Splitter" option to break down large documents into more manageable chunks for further processing or analysis.

5. Take advantage of the "Additional Metadata" feature to add relevant information to your documents, making them easier to categorize or search later.

## Troubleshooting

1. If you encounter rate limit errors:

    - Reduce the "Max Concurrency" setting
    - Increase the "Max Retries" value
    - Ensure you're using authenticated requests for higher rate limits

2. For "Repository not found" errors:

    - Double-check the repository URL
    - Ensure you have the necessary permissions to access the repository
    - Verify that your GitHub API credentials are correctly set up for private repositories

3. If certain files are not being loaded:

    - Check the "Ignore Paths" setting to ensure you're not accidentally excluding desired files
    - Verify that the files are in the specified branch

4. For slow loading times:
    - Consider using the "Ignore Paths" feature to exclude large or unnecessary files
    - Adjust the "Max Concurrency" setting to find the optimal balance between speed and stability

By following these instructions, you'll be able to effectively use the GitHub Document Loader in AnswerAgentAI to access and process content from GitHub repositories, enhancing your workflow and data analysis capabilities.
</file>

<file path="sidekick-studio/chatflows/document-loaders/gmail.md">
---
description: Load and process emails from Gmail in AnswerAgentAI
---

# Gmail Document Loader

## Overview

The Gmail Document Loader enables you to extract and process email content from your Gmail account. This powerful integration allows you to bring email data into your AnswerAgentAI workflows, making it ideal for customer support analysis, email processing automation, and knowledge base creation from email conversations.

## Key Benefits

-   Access emails from specific Gmail labels/folders
-   Process email threads with complete conversation history
-   Extract email metadata (sender, subject, date, etc.)
-   Handle both HTML and plain text email content
-   Configurable message limits and filtering options

## Prerequisites

Before using the Gmail Document Loader, ensure you have:

1. **Google OAuth Configured** - Follow the [Google OAuth Setup Guide](../../../developers/authorization/google-oauth.md)
2. **Required Scopes** - Your OAuth application must include:
    - `https://www.googleapis.com/auth/gmail.readonly`
    - `https://www.googleapis.com/auth/gmail.modify` (if you need to mark emails as read)

## How to Use

### Step 1: Add Gmail Document Loader

1. **Locate the Node**

    - Navigate to the Document Loaders section in the node library
    - Find and drag the "Gmail" node onto your canvas

2. **Connect Credential**
    - In the node configuration, click "Connect Credential"
    - Select your existing Google OAuth credential or create a new one
    - If creating new, you'll be redirected to Google for authorization

### Step 2: Select Gmail Labels

1. **Label Selection Interface**

    - Once your credential is connected, the label picker will load
    - You'll see all available Gmail labels including:
        - System labels (Inbox, Sent, Drafts, etc.)
        - Custom labels you've created
        - Labels with visual indicators showing label type

2. **Refresh Labels**

    - Use the "Refresh Labels" button to reload available labels
    - This is helpful if you've recently created new labels in Gmail

3. **Choose Labels**
    - Select one or multiple labels to process
    - Each selected label will be processed separately
    - Popular choices include "Inbox", "Sent", or custom project labels

### Step 3: Configure Processing Options

#### Max Messages

-   **Purpose:** Limits the number of messages retrieved per label
-   **Default:** 100 messages
-   **Recommendation:** Start with a smaller number (10-50) for testing

#### Include Threads

-   **Purpose:** Includes entire conversation threads for each email
-   **Default:** false (disabled)
-   **When to use:** Enable for customer support scenarios where context matters
-   **Note:** This significantly increases the amount of data processed

#### Text Splitter (Optional)

-   **Purpose:** Breaks large email content into smaller, manageable chunks
-   **When to use:**
    -   Processing long email conversations
    -   Improving vector search performance
    -   Working with large email volumes

### Step 4: Advanced Configuration

#### Additional Metadata

-   Add custom metadata fields to your email documents
-   Useful for categorization and filtering
-   Example:
    ```json
    {
        "department": "support",
        "priority": "high",
        "processed_date": "2024-01-15"
    }
    ```

#### Omit Metadata Keys

-   **Purpose:** Exclude specific metadata fields from the final documents
-   **Options:**
    -   Comma-separated list: `threadId,labelId`
    -   Use `*` to omit all metadata except Additional Metadata
-   **Available metadata fields:**
    -   `source`: Gmail URL reference
    -   `messageId`: Unique Gmail message ID
    -   `threadId`: Gmail thread ID
    -   `labelId`: Label ID where message was found
    -   `labelName`: Human-readable label name
    -   `subject`: Email subject line
    -   `from`: Sender information
    -   `to`: Recipient information
    -   `date`: Email timestamp

## Understanding Email Processing

### Content Formatting

The Gmail loader formats email content in a readable structure:

```
Subject: [Email Subject]
From: [Sender Name] <sender@email.com>
To: [Recipient Name] <recipient@email.com>
Date: [Email Date]

[Email Body Content]

--- THREAD --- (if Include Threads is enabled)

From: [Previous Sender]
Date: [Previous Date]
[Previous Email Content]
```

### Thread Processing

When "Include Threads" is enabled:

-   The primary email content appears first
-   Thread messages are appended with clear separators
-   Each thread message includes sender and date information
-   Chronological order is maintained

## Use Cases

### Customer Support Analysis

```yaml
Configuration:
    Labels: ['Support', 'Customer Service']
    Include Threads: true
    Max Messages: 200
    Text Splitter: Enabled (for long conversations)

Purpose: Analyze support conversations for common issues and response patterns
```

### Knowledge Base Creation

```yaml
Configuration:
    Labels: ['Important', 'Project Communications']
    Include Threads: false
    Max Messages: 500
    Additional Metadata: { 'category': 'knowledge_base' }

Purpose: Extract important information for documentation
```

### Email Analytics

```yaml
Configuration:
    Labels: ['Inbox']
    Include Threads: false
    Max Messages: 1000
    Omit Metadata: '*'

Purpose: Focus on email content analysis without metadata noise
```

## Tips and Best Practices

### Performance Optimization

1. **Start Small**

    - Begin with 10-50 messages for testing
    - Gradually increase based on processing time and needs

2. **Use Specific Labels**

    - Avoid processing entire "Inbox" unless necessary
    - Create specific labels for the content you need

3. **Text Splitter Usage**
    - Enable for email threads longer than 2000 characters
    - Use recursive character splitter for best results

### Data Management

1. **Metadata Strategy**

    - Keep relevant metadata for search and filtering
    - Use "Omit Metadata Keys" to reduce noise
    - Add custom metadata for better organization

2. **Regular Updates**
    - Re-run the loader periodically for new emails
    - Consider date-based filtering in future versions

### Security Considerations

1. **Credential Management**

    - Use separate credentials for different email accounts
    - Regularly review and rotate OAuth tokens
    - Monitor access logs in Google Console

2. **Data Privacy**
    - Be mindful of sensitive information in emails
    - Consider email content filtering before processing
    - Comply with data protection regulations

## Troubleshooting

### Common Issues

1. **"Google access token not found"**

    - **Solution:** Reconnect your Google OAuth credential
    - **Check:** Ensure the credential is properly selected in the node

2. **"Labels not loading"**

    - **Solution:** Click "Refresh Labels" button
    - **Check:** Verify your OAuth credential has the correct scopes
    - **Try:** Re-authorize the credential if refresh fails

3. **"No emails found"**

    - **Check:** Selected labels contain messages
    - **Verify:** Your Gmail account has access to the selected labels
    - **Consider:** Increasing the max messages limit

4. **"Rate limit exceeded"**

    - **Solution:** Reduce the number of messages being processed
    - **Wait:** Gmail API has rate limits; try again later
    - **Optimize:** Use more specific labels to reduce API calls

5. **"Token expired" errors**
    - **Solution:** AnswerAgentAI automatically refreshes tokens
    - **Manual fix:** Re-authorize the credential if automatic refresh fails
    - **Check:** Ensure your OAuth application is still active

### Performance Issues

1. **Slow Processing**

    - Reduce max messages limit
    - Disable thread processing if not needed
    - Use text splitter for very long emails

2. **Memory Issues**
    - Process emails in smaller batches
    - Increase text splitter chunk size
    - Monitor system resources during processing

## Integration Examples

### Basic Email Ingestion

1. Gmail Document Loader  Vector Store
2. Use for: Simple email search and retrieval

### Email Analysis Pipeline

1. Gmail Document Loader  Text Splitter  Vector Store  Retrieval QA
2. Use for: Answering questions about email content

### Support Ticket Analysis

1. Gmail Document Loader (Support label, threads enabled)  Chat Model
2. Use for: Analyzing support patterns and generating insights

## API Reference

The Gmail integration uses the Gmail API v1 with the following endpoints:

-   `users.labels.list` - Retrieve available labels
-   `users.messages.list` - Get message lists per label
-   `users.messages.get` - Retrieve individual message content
-   `users.threads.get` - Get conversation threads (when enabled)

## Next Steps

After setting up Gmail document loading:

1. **Configure Vector Storage** - Store processed emails for search
2. **Set up Retrieval** - Enable question-answering over email content
3. **Add Analysis Tools** - Use chat models to analyze email patterns
4. **Automate Processing** - Schedule regular email ingestion

---

**Related Documentation:**

-   [Google OAuth Setup](../../../developers/authorization/google-oauth.md)
-   [Text Splitters](../text-splitters/README.md)
-   [Vector Stores](../vector-stores/README.md)
</file>

<file path="sidekick-studio/chatflows/document-loaders/google-drive.md">
---
description: Load and process documents from Google Drive in AnswerAgentAI
---

# Google Drive Document Loader

## Overview

The Google Drive Document Loader allows you to extract and process content from files stored in your Google Drive. This versatile integration supports multiple file formats and provides flexible processing options, making it ideal for knowledge base creation, document analysis, and content management workflows.

## Key Benefits

-   Support for multiple file formats (Google Docs, PDFs, Spreadsheets, plain text)
-   Selective file processing with file picker interface
-   Automatic format conversion for Google Workspace files
-   Configurable PDF processing options
-   Rich metadata extraction including file properties

## Supported File Types

| File Type         | Description              | Processing Method                  |
| ----------------- | ------------------------ | ---------------------------------- |
| **Google Docs**   | Native Google documents  | Converted to plain text            |
| **Google Sheets** | Spreadsheets and data    | Converted to CSV format            |
| **PDF Files**     | Portable Document Format | PDF parsing with page/file options |
| **Text Files**    | Plain text documents     | Direct text extraction             |
| **Other Formats** | Various document types   | Best-effort text extraction        |

## Prerequisites

Before using the Google Drive Document Loader, ensure you have:

1. **Google OAuth Configured** - Follow the [Google OAuth Setup Guide](../../../developers/authorization/google-oauth.md)
2. **Required Scopes** - Your OAuth application must include:
    - `https://www.googleapis.com/auth/drive.readonly`
    - `https://www.googleapis.com/auth/drive.file` (for app-created files)

## How to Use

### Step 1: Add Google Drive Document Loader

1. **Locate the Node**

    - Navigate to the Document Loaders section in the node library
    - Find and drag the "Google Drive" node onto your canvas

2. **Connect Credential**
    - In the node configuration, click "Connect Credential"
    - Select your existing Google OAuth credential or create a new one
    - If creating new, you'll be redirected to Google for authorization

### Step 2: Select Files

1. **File Selection Interface**

    - Once your credential is connected, a file picker will appear
    - Browse your Google Drive folders and files
    - The interface shows:
        - File names and types
        - File icons for easy identification
        - Folder navigation
        - Recent files access

2. **Choose Files**

    - Select individual files or multiple files
    - You can choose files from different folders
    - Selected files will be listed with their paths

3. **File Management**
    - Remove files from selection if needed
    - Verify file access permissions
    - Check file sizes for processing planning

### Step 3: Configure Processing Options

#### PDF Usage Options

For PDF files, choose how to process the content:

-   **One document per page** (Default)

    -   Each PDF page becomes a separate document
    -   Better for: Page-specific content, citations, detailed analysis
    -   Use when: You need to reference specific pages

-   **One document per file**
    -   Entire PDF becomes a single document
    -   Better for: Overall document understanding, summarization
    -   Use when: PDF pages are part of a cohesive document

#### Text Splitter (Optional)

-   **Purpose:** Breaks large documents into smaller, manageable chunks
-   **Recommended for:**
    -   Large Google Docs or PDFs
    -   Spreadsheets with extensive data
    -   Better vector search performance
-   **Types:** Choose based on your content type (character, token, or semantic splitting)

### Step 4: Advanced Configuration

#### Additional Metadata

Add custom metadata to enhance document organization:

```json
{
    "department": "marketing",
    "project": "q4-campaign",
    "processed_date": "2024-01-15",
    "priority": "high"
}
```

#### Omit Metadata Keys

Control which metadata fields to include:

-   **Available metadata fields:**

    -   `source`: Google Drive URL reference
    -   `fileId`: Unique Google Drive file ID
    -   `fileName`: Original file name
    -   `iconUrl`: File type icon URL
    -   `mimeType`: File MIME type
    -   `lastModified`: Last modification timestamp (sync mode)

-   **Options:**
    -   Comma-separated list: `fileId,iconUrl`
    -   Use `*` to omit all metadata except Additional Metadata

## File Processing Details

### Google Docs Processing

-   **Conversion:** Google Docs are exported as plain text
-   **Formatting:** Basic text formatting is preserved
-   **Images:** Text descriptions are included where available
-   **Links:** Link text is preserved, URLs may be included

### Google Sheets Processing

-   **Format:** Converted to CSV format for processing
-   **Structure:** Row and column data maintained
-   **Multiple Sheets:** Each sheet processed separately
-   **Data Types:** Numbers, text, and formulas included

### PDF Processing

-   **Text Extraction:** Uses advanced PDF parsing
-   **Images:** Text within images not extracted (OCR not included)
-   **Layout:** Attempts to preserve document structure
-   **Pages:** Page boundaries maintained in page-per-document mode

### Other File Types

-   **Best Effort:** AnswerAgentAI attempts to extract readable text
-   **Encoding:** UTF-8 encoding assumed
-   **Binary Files:** May not process correctly if containing binary data

## Use Cases

### Knowledge Base Creation

```yaml
Configuration:
    Files: ['Company Handbook.pdf', 'Process Documents/', 'FAQ.docx']
    PDF Usage: 'One document per page'
    Text Splitter: Enabled
    Additional Metadata: { 'category': 'knowledge_base' }

Purpose: Build searchable knowledge base from company documents
```

### Project Documentation

```yaml
Configuration:
    Files: ['Project Plans/', 'Meeting Notes/', 'Specifications.pdf']
    PDF Usage: 'One document per file'
    Text Splitter: Semantic splitting
    Additional Metadata: { 'project': 'alpha', 'team': 'engineering' }

Purpose: Create project-specific document repository
```

### Research Analysis

```yaml
Configuration:
    Files: ['Research Papers/', 'Data Sheets/']
    PDF Usage: 'One document per page'
    Text Splitter: Character splitting (1000 chars)
    Omit Metadata: 'iconUrl,mimeType'

Purpose: Analyze research documents with page-level granularity
```

## Tips and Best Practices

### File Organization

1. **Folder Structure**

    - Organize files logically in Google Drive before selection
    - Use descriptive folder names
    - Consider access permissions for shared folders

2. **File Naming**

    - Use clear, descriptive file names
    - Include version numbers if applicable
    - Avoid special characters that might cause issues

3. **File Selection Strategy**
    - Start with a small subset for testing
    - Group related files for batch processing
    - Consider file sizes and processing time

### Performance Optimization

1. **Large File Handling**

    - Enable text splitter for files larger than 100KB
    - Consider PDF page-level processing for large PDFs
    - Monitor processing time and memory usage

2. **Batch Processing**
    - Process related files together
    - Use consistent metadata for file groups
    - Consider file modification dates for updates

### Data Management

1. **Metadata Strategy**

    - Use additional metadata for categorization
    - Include project or department information
    - Add processing timestamps for tracking

2. **Version Control**
    - Track file modification dates
    - Re-process when source files change
    - Consider automated sync for frequently updated files

## Troubleshooting

### Common Issues

1. **"Failed to retrieve credentials"**

    - **Solution:** Reconnect your Google OAuth credential
    - **Check:** Ensure the credential has Drive API access
    - **Verify:** OAuth scopes include drive.readonly

2. **"File not found" or "Access denied"**

    - **Check:** File still exists in Google Drive
    - **Verify:** You have access permissions to the file
    - **Try:** Re-select the file in the file picker

3. **"Download failed" errors**

    - **Cause:** Large files or network issues
    - **Solution:** Try processing smaller files first
    - **Check:** Your internet connection stability

4. **"Unsupported file format"**

    - **Support:** Not all file types are supported
    - **Alternative:** Convert to supported format in Google Drive
    - **Workaround:** Export Google Workspace files as supported formats

5. **Empty or garbled content**
    - **PDF Issues:** Try different PDF usage options
    - **Encoding:** Ensure files use UTF-8 encoding
    - **Binary Files:** Verify files contain readable text

### Performance Issues

1. **Slow Processing**

    - Reduce number of files processed simultaneously
    - Use text splitter for very large documents
    - Check file sizes before processing

2. **Memory Issues**

    - Process files in smaller batches
    - Increase text splitter chunk size
    - Monitor system resources

3. **Rate Limiting**
    - Google Drive API has usage limits
    - Wait between large batch operations
    - Consider upgrading Google Cloud quotas if needed

## Integration Examples

### Document Search System

1. Google Drive Document Loader  Text Splitter  Vector Store  Retrieval QA
2. Use for: Searchable company document repository

### Content Analysis Pipeline

1. Google Drive Document Loader  Chat Model  Summary Generator
2. Use for: Automated document summarization and insights

### Knowledge Extraction

1. Google Drive Document Loader  Custom Processing  Knowledge Graph
2. Use for: Extracting structured information from documents

## Sync and Refresh Capabilities

The Google Drive Document Loader includes built-in sync capabilities:

-   **Automatic Detection:** Identifies when source files have been modified
-   **Incremental Updates:** Only processes changed files
-   **Metadata Tracking:** Maintains last modification timestamps
-   **Efficient Processing:** Avoids re-processing unchanged content

To enable sync:

1. Use the `syncAndRefresh` method instead of `init`
2. Metadata will include `lastModified` timestamps
3. Compare timestamps to determine if re-processing is needed

## Security and Privacy

### Access Control

-   **Credential Isolation:** Each user's credential only accesses their files
-   **Scope Limitation:** OAuth scopes limit access to necessary permissions
-   **File Permissions:** Respects Google Drive sharing and permission settings

### Data Privacy

-   **Local Processing:** File content processed locally in AnswerAgentAI
-   **No Storage:** Original files not permanently stored
-   **Metadata Only:** Only necessary metadata retained
-   **Compliance:** Follow your organization's data handling policies

## Next Steps

After setting up Google Drive document loading:

1. **Configure Vector Storage** - Store processed documents for search
2. **Set up Text Splitting** - Optimize for your document types
3. **Add Retrieval Systems** - Enable question-answering over documents
4. **Implement Sync** - Set up regular document updates

---

**Related Documentation:**

-   [Google OAuth Setup](../../../developers/authorization/google-oauth.md)
-   [Text Splitters](../text-splitters/README.md)
-   [Vector Stores](../vector-stores/README.md)
-   [PDF Document Loader](./pdf-file.md)
</file>

<file path="sidekick-studio/chatflows/document-loaders/json-file.md">
---
description: Load and process data from JSON files in AnswerAgentAI
---

# JSON File Document Loader

## Overview

The JSON File Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process data from JSON files. This tool is particularly useful when you need to extract specific information from JSON-structured data and convert it into a format that can be used within AnswerAgentAI's workflows.

## Key Benefits

-   Easily import and process JSON data from files
-   Extract specific data points using pointers
-   Customize metadata and control what information is included in the output

## How to Use

1. In the AnswerAgentAI interface, locate and select the "JSON File" node from the "Document Loaders" category.

2. Configure the node by setting the following parameters:

    a. JSON File: Upload your JSON file or select one from storage.

    b. Text Splitter (optional): Choose a text splitter if you want to break down large JSON documents into smaller chunks.

    c. Pointers Extraction (optional): Enter comma-separated pointers to extract specific data from the JSON structure.

    d. Additional Metadata (optional): Add any extra metadata you want to include with the extracted documents.

    e. Omit Metadata Keys (optional): Specify any metadata keys you want to exclude from the output.

3. Connect the JSON File node to other nodes in your workflow as needed.

4. Run your workflow to process the JSON data.

<!-- TODO: Add a screenshot of the JSON File node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/jsonfilenode.png" alt="" /><figcaption><p> JSON File Node Configuration In a Workflow &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use pointers to extract specific data: If you only need certain information from your JSON file, use the "Pointers Extraction" field to specify the exact data you want to extract. This can help reduce processing time and improve efficiency.

2. Leverage text splitters for large documents: When dealing with large JSON files, use a text splitter to break the content into more manageable chunks. This can help improve processing speed and prevent memory issues.

3. Customize metadata: Use the "Additional Metadata" field to add context to your extracted data. This can be helpful for tracking the source of information or adding other relevant details.

4. Control metadata output: Use the "Omit Metadata Keys" field to exclude any sensitive or unnecessary metadata from your output. You can use a comma-separated list of keys or use "\*" to omit all metadata except what you specify in the "Additional Metadata" field.

## Troubleshooting

1. Issue: The JSON file isn't loading properly.
   Solution: Ensure that your JSON file is properly formatted and valid. You can use online JSON validators to check your file's structure.

2. Issue: Extracted data is not what you expected.
   Solution: Double-check your pointer syntax in the "Pointers Extraction" field. Make sure you're using the correct path to the data you want to extract.

3. Issue: Processing large JSON files is slow or causes errors.
   Solution: Try using a text splitter to break down the document into smaller chunks. This can help improve processing speed and prevent memory-related issues.

Remember, the JSON File Document Loader is a versatile tool that can handle various JSON structures and sizes. Experiment with different configurations to find the best setup for your specific use case.

<!-- TODO: Add a screenshot of a sample workflow using the JSON File node -->
<figure><img src="/.gitbook/assets/screenshots/jsonfilenodeinaworkflow.png" alt="" /><figcaption><p> JSON File Node Configuration &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/document-loaders/json-lines-file.md">
---
description: Load and process data from JSON Lines files in AnswerAgentAI
---

# JSON Lines File Loader

## Overview

The JSON Lines File Loader is a powerful feature in AnswerAgentAI that allows you to load and process data from JSON Lines (.jsonl) files. This tool is particularly useful when working with large datasets or when you need to extract specific information from structured JSON data.

## Key Benefits

-   Efficiently load and process large JSON Lines files
-   Extract specific data using pointer extraction
-   Customize metadata and control which metadata keys are included in the output

## How to Use

Follow these steps to use the JSON Lines File Loader in AnswerAgentAI:

1. In the AnswerAgentAI interface, locate and select the "Json Lines File" node from the "Document Loaders" category.

<!-- TODO: Screenshot of the JSON Lines File node in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/jsonlinesfile.png" alt="" /><figcaption><p> JSON Lines File Node Configuration &#x26; Drop UI</p></figcaption></figure>

2. Configure the node by setting the following parameters:

    a. **Jsonlines File**: Upload your .jsonl file or select one from the file storage.

    b. **Text Splitter** (optional): Choose a text splitter if you want to break down the loaded content into smaller chunks.

    c. **Pointer Extraction**: Enter the pointer name to extract specific data from the JSON structure (e.g., "text" to extract the "text" field from each JSON object).

    d. **Additional Metadata** (optional): Add any extra metadata you want to include with the extracted documents. Enter this as a JSON object.

    e. **Omit Metadata Keys** (optional): Specify any metadata keys you want to exclude from the output. Enter keys separated by commas, or use "\*" to omit all default metadata keys.

<!-- TODO: Screenshot of the configured JSON Lines File node with all parameters filled out -->
<figure><img src="/.gitbook/assets/screenshots/jsonlinesfileinaworkflow.png" alt="" /><figcaption><p> JSON Lines File Node In A Workflow &#x26; Drop UI</p></figcaption></figure>

3. Connect the JSON Lines File node to other nodes in your AnswerAgentAI workflow as needed.

4. Run your workflow to process the JSON Lines file and use the extracted data in subsequent steps.

## Tips and Best Practices

1. **Pointer Extraction**: Carefully choose your pointer name to extract the most relevant data from your JSON Lines file. For example, if your file contains user comments, you might use "comment" as the pointer name.

2. **Text Splitter**: If you're working with large JSON Lines files, consider using a text splitter to break down the content into more manageable chunks for processing.

3. **Metadata Management**: Use the "Additional Metadata" field to add context to your extracted data, and leverage the "Omit Metadata Keys" option to keep your output clean and focused on the most important information.

4. **File Storage**: For large files or frequently used datasets, consider using AnswerAgentAI's file storage feature. This allows you to easily reuse the same file across multiple workflows.

## Troubleshooting

1. **File not loading**: Ensure that your file is in the correct JSON Lines (.jsonl) format, with each line containing a valid JSON object.

2. **Pointer extraction not working**: Double-check that the pointer name you've entered exactly matches the field name in your JSON data. Pointers are case-sensitive.

3. **Unexpected metadata**: If you're seeing unwanted metadata in your output, use the "Omit Metadata Keys" field to exclude specific keys, or use "\*" to omit all default metadata.

4. **Performance issues with large files**: If processing large files is slow, try using a text splitter to break the content into smaller chunks, or consider pre-processing your data to reduce its size before loading it into AnswerAgentAI.

By following this guide, you should be able to effectively use the JSON Lines File Loader in AnswerAgentAI to process and extract valuable information from your JSON Lines files.
</file>

<file path="sidekick-studio/chatflows/document-loaders/notion-database.md">
---
description: Load and process data from Notion Databases
---

# Notion Database

## Overview

The Notion Database feature in AnswerAgentAI allows you to load and process data from your Notion databases. This powerful integration enables you to seamlessly import information from Notion into your AnswerAgentAI workflows, treating each row in the database as a separate document with all its properties as metadata.

## Key Benefits

-   Easily import structured data from Notion databases into AnswerAgentAI
-   Maintain metadata from Notion, enhancing the context of your imported data
-   Customize the imported data with additional metadata and filtering options

## How to Use

Follow these steps to use the Notion Database feature in AnswerAgentAI:

1. Navigate to the Document Loaders section in AnswerAgentAI.
2. Locate and select the "Notion Database" option.

<!-- TODO: Screenshot of the Document Loaders section with Notion Database highlighted -->
<figure><img src="/.gitbook/assets/screenshots/notiondatabase.png" alt="" /><figcaption><p> Notion Database Node &#x26; Drop UI</p></figcaption></figure>

3. Configure the Notion Database loader with the following settings:

    a. Connect Credential: Link your Notion API credentials.
    b. Text Splitter (optional): Choose a text splitter if you want to break down large documents.
    c. Notion Database Id: Enter the ID of your Notion database.
    d. Additional Metadata (optional): Add any extra metadata you want to include with your documents.
    e. Omit Metadata Keys (optional): Specify any metadata keys you want to exclude from the imported documents.

<!-- TODO: Screenshot of the Notion Database configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/notiondatabaseinaworkflow.png" alt="" /><figcaption><p> Notion Database Node In a Workflow &#x26; Drop UI</p></figcaption></figure>

4. Click "Save" or "Apply" to confirm your settings.
5. Run your workflow to import the data from your Notion database.

## Tips and Best Practices

1. Obtain the correct Database ID: To find your Notion Database ID, look at the URL of your database. If it looks like `https://www.notion.so/abcdefh?v=long_hash_2`, then `abcdefh` is your database ID.

2. Use Text Splitters wisely: If you're dealing with large documents in your Notion database, consider using a Text Splitter to break them down into more manageable chunks.

3. Leverage Additional Metadata: Use the Additional Metadata field to add context or categorization to your imported documents. This can be helpful for organizing and filtering your data within AnswerAgentAI.

4. Optimize Metadata: Use the Omit Metadata Keys feature to exclude unnecessary metadata, keeping your imported data clean and relevant. You can use a comma-separated list of keys to omit, or use "\*" to omit all default metadata keys.

## Troubleshooting

1. Authentication Issues:

    - Ensure that your Notion API credentials are correctly set up in AnswerAgentAI.
    - Verify that your Notion integration has the necessary permissions to access the database.

2. Database Not Found:

    - Double-check that you've entered the correct Database ID.
    - Confirm that your Notion integration has access to the specific database you're trying to import.

3. Unexpected Data:
    - If you're not seeing all the data you expect, check your Omit Metadata Keys settings to ensure you haven't accidentally excluded important information.
    - Verify that the database columns in Notion match the structure you're expecting in AnswerAgentAI.

By following this guide, you should be able to effectively use the Notion Database feature in AnswerAgentAI to import and process your Notion data. Remember to respect Notion's API usage limits and your organization's data handling policies when working with imported data.
</file>

<file path="sidekick-studio/chatflows/document-loaders/notion-folder.md">
---
description: Load and process documents from exported Notion folders
---

# Notion Folder Document Loader

## Overview

The Notion Folder Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process content from exported Notion folders. This tool is perfect for users who want to leverage their Notion content within AnswerAgentAI for various natural language processing tasks.

## Key Benefits

-   Easily import content from Notion into AnswerAgentAI
-   Process and split large Notion documents for improved handling
-   Customize metadata for better organization and searchability

## How to Use

1. Export your Notion content:

    - In Notion, select the pages you want to export
    - Choose "Export" from the menu and select your preferred format
    - Download and unzip the exported folder

2. Configure the Notion Folder Document Loader in AnswerAgentAI:

    - Navigate to the Document Loaders section
    - Select "Notion Folder" from the available options

3. Set up the loader:

    - Enter the path to your unzipped Notion folder in the "Notion Folder" field
    - (Optional) Select a Text Splitter if you want to break down large documents
    - (Optional) Add additional metadata in JSON format
    - (Optional) Specify metadata keys to omit

4. Run the loader to import your Notion content into AnswerAgentAI

<!-- TODO: Add a screenshot of the Notion Folder Document Loader configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/notionfolder.png" alt="" /><figcaption><p> Notion Folder Document Loader Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Organize your Notion content before exporting to make it easier to process in AnswerAgentAI
2. Use a Text Splitter for large documents to improve processing efficiency
3. Add relevant metadata to enhance searchability and organization of your imported content
4. Regularly update your imported Notion content to keep your AnswerAgentAI data current

## Troubleshooting

1. If the loader fails to import your content:

    - Double-check the folder path to ensure it's correct
    - Verify that the exported Notion folder is unzipped
    - Ensure you have the necessary permissions to access the folder

2. If metadata is not appearing as expected:

    - Review your JSON format for additional metadata
    - Check the "Omit Metadata Keys" field for any conflicts

3. If documents are too large or small after splitting:
    - Adjust the Text Splitter settings to find the right balance for your content

Remember, the Notion Folder Document Loader is a powerful tool to bridge your Notion workspace with AnswerAgentAI's capabilities. Experiment with different settings to find the best configuration for your specific use case.
</file>

<file path="sidekick-studio/chatflows/document-loaders/notion-page.md">
---
description: Load and process data from Notion pages
---

# Notion Page Document Loader

## Overview

The Notion Page Document Loader is a powerful feature in AnswerAgentAI that allows you to import content from Notion pages, including child pages, and process them as separate documents. This tool is perfect for users who want to leverage their Notion content within AnswerAgentAI for various applications such as analysis, summarization, or integration with other AI-powered features.

## Key Benefits

-   Easily import content from Notion pages into AnswerAgentAI
-   Process parent and child pages as separate documents
-   Customize metadata and control which metadata is included in the final output

## How to Use

1. Connect your Notion API credential:

    - Ensure you have a Notion API integration token
    - In AnswerAgentAI, go to the Credentials section and add a new "Notion API" credential
    - Enter your Notion integration token

2. Set up the Notion Page Document Loader:

    - In your AnswerAgentAI workflow, add a new "Notion Page" node
    - Connect your Notion API credential to the node

3. Configure the Notion Page loader:

    - Enter the Notion Page ID in the "Notion Page Id" field
    - (Optional) Add a Text Splitter if you want to split the loaded content into smaller chunks
    - (Optional) Add additional metadata in JSON format
    - (Optional) Specify metadata keys to omit from the final output

4. Run your workflow to load and process the Notion page content

<!-- TODO: Add a screenshot of the Notion Page Document Loader node configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/notionpage.png" alt="" /><figcaption><p> Notion Page Document Loader Node &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Page ID: To find the Notion Page ID, look at the URL of your Notion page. The ID is the last 32-character hexadecimal string in the URL path. For example, in `https://www.notion.so/username/Page-Title-b34ca03f219c4420a6046fc4bdfdf7b4`, the Page ID is `b34ca03f219c4420a6046fc4bdfdf7b4`.

2. Text Splitter: If you're working with large Notion pages, consider using a Text Splitter to break the content into smaller, more manageable chunks. This can be especially useful for processing with language models or other AI tools.

3. Metadata: Use the "Additional Metadata" field to add custom metadata to your documents. This can be helpful for categorizing or filtering your content later in your workflow.

4. Omitting Metadata: If you want to exclude certain metadata fields from the final output, use the "Omit Metadata Keys" field. You can specify multiple keys by separating them with commas. To omit all default metadata and only include your custom metadata, use an asterisk (\*) in this field.

## Troubleshooting

1. Issue: Unable to connect to Notion
   Solution: Double-check that your Notion API integration token is correct and that you've granted the necessary permissions to the integration in your Notion workspace.

2. Issue: Page content not loading
   Solution: Verify that you've entered the correct Page ID and that you have access to the page in your Notion workspace.

3. Issue: Unexpected metadata in output
   Solution: Review your "Omit Metadata Keys" settings and ensure you've correctly specified which keys to exclude. Remember to use commas to separate multiple keys.

By following this guide, you'll be able to efficiently load and process your Notion pages within AnswerAgentAI, opening up new possibilities for leveraging your Notion content in your AI-powered workflows.
</file>

<file path="sidekick-studio/chatflows/document-loaders/pdf-file.md">
---
description: Load and process PDF documents in AnswerAgentAI
---

# PDF Document Loader

## Overview

The PDF Document Loader is a powerful feature in AnswerAgentAI that allows you to extract and process text content from PDF files. This tool is essential for users who need to work with information stored in PDF format, enabling easy integration of PDF content into their AnswerAgentAI workflows.

## Key Benefits

-   Easily extract text content from PDF files
-   Flexible options for processing PDF documents (per page or per file)
-   Ability to split large documents into manageable chunks

## How to Use

1.  **Select the PDF File**

    -   Choose the PDF file(s) you want to process.

    <!-- TODO: Screenshot of file selection interface -->
    <figure><img src="/.gitbook/assets/screenshots/pdffile.png" alt="" /><figcaption><p> Pdf Loader File Selection Interface &#x26; Drop UI</p></figcaption></figure>

2.  **Configure Text Splitter (Optional)**

    -   If needed, select a Text Splitter to break down large documents.
        <!-- TODO: Screenshot of Text Splitter configuration -->
            <figure><img src="/.gitbook/assets/screenshots/textsplitter.png" alt="" /><figcaption><p> Text Splitter Node &#x26; Drop UI</p></figcaption></figure>

3.  **Choose Usage Option**

    -   Select either "One document per page" or "One document per file" based on your needs.
        <!-- TODO: Screenshot of usage option selection -->
        <figure><img src="/.gitbook/assets/screenshots/pdfusage.png" alt="" /><figcaption><p> Pdf Usage &#x26; Drop UI</p></figcaption></figure>

4.  **Set Additional Parameters (Optional)**
    -   Use Legacy Build: Enable if you're working with older PDF formats.
    -   Additional Metadata: Add extra information to the extracted documents.
    -   Omit Metadata Keys: Specify which metadata keys to exclude.
        <!-- TODO: Screenshot of additional parameters configuration -->
        <figure><img src="/.gitbook/assets/screenshots/pdfadditionalparameters.png" alt="" /><figcaption><p> Pdf Additional Parameters &#x26; Drop UI</p></figcaption></figure>
5.  **Process the PDF**
    -   Run the PDF Document Loader to extract the content.
        <!-- TODO: Screenshot of the process initiation button -->

## Tips and Best Practices

1. Use the Text Splitter option for large PDFs to create more manageable chunks of text.
2. When working with multiple PDFs, consider using the "One document per file" option for easier organization.
3. Utilize the Additional Metadata feature to add context or categorization to your extracted documents.
4. If you encounter issues with older PDFs, try enabling the "Use Legacy Build" option.

## Troubleshooting

1. **PDF not loading properly:**

    - Ensure the PDF file is not corrupted or password-protected.
    - Try using the "Use Legacy Build" option for older PDF formats.

2. **Extracted text is jumbled or incorrectly formatted:**

    - This can happen with complex PDF layouts. Try adjusting the Text Splitter settings or processing the document page by page.

3. **Missing metadata:**
    - Check the "Omit Metadata Keys" field to ensure you haven't accidentally excluded important metadata.

## Note on Image Handling

Please be aware that the current version of the PDF Document Loader does not handle images within PDF files very well. Our team is actively working on improving image processing capabilities to provide a more comprehensive PDF handling experience in future updates.
</file>

<file path="sidekick-studio/chatflows/document-loaders/plain-text.md">
---
description: Load and process plain text data in AnswerAgentAI
---

# Plain Text Document Loader

## Overview

The Plain Text Document Loader is a versatile feature in AnswerAgentAI that allows you to load and process plain text data. This tool is particularly useful for importing text content into your workflows, splitting it into manageable chunks, and adding metadata to your documents.

## Key Benefits

-   Easily import and process plain text data
-   Customize text splitting for optimal processing
-   Add and manage metadata for your documents
-   Ideal for creating dynamic, variable-driven workflows

## How to Use

1. Navigate to the Document Loaders section in AnswerAgentAI.
2. Select the "Plain Text" loader.
3. Enter your text in the "Text" field.
4. (Optional) Configure a Text Splitter if you want to split your text into smaller chunks.
5. (Optional) Add additional metadata in JSON format.
6. (Optional) Specify metadata keys to omit, if needed.
7. Choose the desired output format (Document or Text).
8. Connect the loader to other nodes in your workflow.

<!-- TODO: Screenshot of the Plain Text Document Loader configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/plain text node.png" alt="" /><figcaption><p> Plain text Node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use Text Splitters: For long texts, consider using a Text Splitter to break the content into more manageable chunks. This can improve processing efficiency and enable more granular analysis.

2. Leverage Metadata: Take advantage of the additional metadata feature to enrich your documents with extra information. This can be useful for categorization, filtering, or providing context to your AI models.

3. Variables in Sidekick Workflows: The Plain Text loader is excellent for incorporating variables into your sidekick workflows. This allows you to create dynamic, flexible workflows that can adapt to different inputs.

    Example:
    Let's say you want to create a workflow that generates a personalized email. You can use the Plain Text loader to input variables like the recipient's name, company, and specific details:

    ```

    {
      "recipientName": "John Doe",
      "companyName": "Acme Corp",
      "productName": "WidgetPro 2000"
    }

    ```

    Then, in a subsequent Prompt Template node, you can use these variables:

    ```

    Dear {{recipientName}},

    I hope this email finds you well. I wanted to reach out to you about our latest product, the {{productName}}, which I believe could be a great fit for {{companyName}}.

    [Rest of the email template...]

    ```

    This approach allows you to reuse the same workflow for different recipients and products, making your sidekick more versatile and efficient.

<!-- TODO: Screenshot showing the connection between a Plain Text loader (with variables) and a Prompt Template node -->
<figure><img src="/.gitbook/assets/screenshots/plaintextwithprompt.png" alt="" /><figcaption><p> Plain text Node Connection with prompt template  &#x26; Drop UI</p></figcaption></figure>

<figure><img src="/.gitbook/assets/screenshots/plaintextin a workflow.png" alt="" /><figcaption><p> Plain text Node Connection In a Workflow  &#x26; Drop UI</p></figcaption></figure>

4. Omit Unnecessary Metadata: Use the "Omit Metadata Keys" feature to remove any default metadata that isn't relevant to your use case. This keeps your documents clean and focused on the information you need.

## Troubleshooting

1. Text Not Splitting Correctly: If your text isn't splitting as expected, double-check your Text Splitter configuration. Adjust parameters like chunk size or overlap to fine-tune the splitting process.

2. Metadata Not Appearing: Ensure that your additional metadata is in valid JSON format. If you're not seeing expected metadata, verify that you haven't accidentally omitted it using the "Omit Metadata Keys" feature.

3. Output Format Issues: If you're not getting the output type you expect (Document vs. Text), make sure you've selected the correct output in the node configuration.

4. If you are using a Plain Text loader as a variable, be sure to select Text as the output type and not Document or you will receive a JSON error

By mastering the Plain Text Document Loader, you can efficiently import, process, and enrich text data in your AnswerAgentAI workflows, enabling more dynamic and powerful AI-driven applications.
</file>

<file path="sidekick-studio/chatflows/document-loaders/playwright-web-scraper.md">
---
description: Load and scrape web content using Playwright
---

# Playwright Web Scraper

## Overview

The Playwright Web Scraper is a powerful tool in AnswerAgentAI that allows you to load and extract data from web pages. It uses the Playwright library, which automates web browsers for efficient web scraping. This feature is particularly useful for gathering information from websites, performing web research, or collecting data for analysis.

## Key Benefits

-   Efficiently scrape content from single or multiple web pages
-   Customize scraping parameters for precise data extraction
-   Integrate with text splitters for advanced document processing

## How to Use

### Scraping a Single URL

1. In the AnswerAgentAI interface, locate and select the "Playwright Web Scraper" node.
2. In the "URL" field, enter the web address you want to scrape.
3. (Optional) Connect a Text Splitter node if you need to process the scraped content further.
4. Run the workflow to initiate the scraping process.

<!-- TODO: Screenshot of the Playwright Web Scraper node configuration for a single URL -->
<figure><img src="/.gitbook/assets/screenshots/playwright.png" alt="" /><figcaption><p> Playwright Web Scrapper Node Configuration  &#x26; Drop UI</p></figcaption></figure>

### Crawling and Scraping Multiple URLs

To scrape multiple pages, you can use the web crawling feature:

1. In the "Get Relative Links Method" dropdown, select either "Web Crawl" or "Scrape XML Sitemap".
2. Set the "Get Relative Links Limit" to specify how many pages to crawl (use 0 for unlimited).
3. Run the workflow to start the crawling and scraping process.

<!-- TODO: Screenshot of the Playwright Web Scraper node configuration for multiple URLs -->
<figure><img src="/.gitbook/assets/screenshots/playwrightscraperwebcrawl.png" alt="" /><figcaption><p> Playwright Web Scrapper Node Configuration  &#x26; Drop UI</p></figcaption></figure>

### Advanced Options

-   **Wait Until**: Choose when to consider the page loaded (e.g., "Load", "DOM Content Loaded", "Network Idle").
-   **Wait for selector to load**: Specify a CSS selector to wait for before scraping.
-   **Additional Metadata**: Add custom metadata to the extracted documents.
-   **Omit Metadata Keys**: Exclude specific metadata keys from the output.

## Tips and Best Practices

1. Always respect website terms of service and robots.txt files when scraping.
2. Use the "Wait for selector to load" option for dynamic websites that load content asynchronously.
3. Implement proper error handling in your workflows to manage potential scraping issues.
4. Consider using text splitters to break down large scraped documents into more manageable chunks.
5. When crawling multiple pages, start with a small limit and gradually increase to avoid overwhelming the target website.

## Troubleshooting

1. **Error: Invalid URL**: Ensure the URL is correctly formatted and includes the protocol (http:// or https://).
2. **No data scraped**: Check if the website requires JavaScript to load content. You may need to adjust the "Wait Until" option.
3. **Scraping takes too long**: For large websites, consider using the "Get Relative Links Limit" to restrict the number of pages scraped.
4. **Blocked by website**: Some websites may block automated scraping. Consider adding delays between requests or using rotating IP addresses.

## Output

The Playwright Web Scraper outputs the scraped content as Document objects, which can be further processed or analyzed in your AnswerAgentAI workflow.

<!-- TODO: Screenshot or code snippet showing an example of the scraped output -->
<figure><img src="/.gitbook/assets/screenshots/playrightwebscreapperlinks.png" alt="" /><figcaption><p> Playwright Web Scrapper Node Configuration  &#x26; Drop UI</p></figcaption></figure>

Remember to use web scraping responsibly and in compliance with website policies and legal requirements.
</file>

<file path="sidekick-studio/chatflows/document-loaders/puppeteer-web-scraper.md">
---
description: Load data from webpages using Puppeteer Web Scraper
---

# Puppeteer Web Scraper

The Puppeteer Web Scraper is a powerful tool in AnswerAgentAI that allows you to extract data from webpages. It uses Puppeteer, a Node.js library that controls Chrome or Chromium browsers, to load and scrape web content.

## Overview

This feature enables you to load data from single or multiple webpages, with options to crawl relative links or scrape XML sitemaps. It's particularly useful for gathering large amounts of web data for analysis or processing.

## Key Benefits

-   Easily extract content from complex, JavaScript-rendered websites
-   Flexible options for handling multiple pages and sitemaps
-   Customizable settings for wait times and CSS selectors

## How to Use

1. In the AnswerAgentAI interface, locate and select the "Puppeteer Web Scraper" node.

2. Configure the following required settings:

    - **URL**: Enter the webpage URL you want to scrape.

3. (Optional) Connect a Text Splitter node if you want to split the extracted text.

4. (Optional) Configure additional settings:

    - **Get Relative Links Method**: Choose between "Web Crawl" or "Scrape XML Sitemap" to extract multiple pages.
    - **Get Relative Links Limit**: Set the maximum number of links to process (0 for unlimited).
    - **Wait Until**: Select when to consider the page as loaded.
    - **Wait for selector to load**: Specify a CSS selector to wait for before scraping.
    - **Additional Metadata**: Add extra metadata to the extracted documents.
    - **Omit Metadata Keys**: Exclude specific metadata keys from the output.

5. Run the node to start the web scraping process.

## Tips and Best Practices

1. Always review and comply with the website's terms of service and policies to ensure ethical and legal use of the data.

2. Use the "Wait Until" option to ensure dynamic content is loaded before scraping.

3. When scraping multiple pages, start with a small limit to test performance before increasing.

4. Utilize the Text Splitter for large documents to create more manageable chunks of text.

5. Add relevant metadata to help organize and identify your scraped content.

## Troubleshooting

1. **Invalid URL error**: Ensure the URL is correctly formatted and includes the protocol (http:// or https://).

2. **No data extracted**: Check if the website requires JavaScript. Ensure "Wait Until" is set appropriately, or use "Wait for selector to load" for specific elements.

3. **Scraping takes too long**: Reduce the "Get Relative Links Limit" or optimize your selector choices.

4. **Blocked by website**: Some websites may block automated scraping. Ensure you're complying with the site's robots.txt and consider adding delays between requests.

<!-- TODO: Screenshot of the Puppeteer Web Scraper node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/puppeteer.png" alt="" /><figcaption><p> Playwright Web Scrapper Node Configuration  &#x26; Drop UI</p></figcaption></figure>

Remember to use web scraping responsibly and in compliance with website policies and legal requirements.

-   [Puppeteer](https://pptr.dev/)
</file>

<file path="sidekick-studio/chatflows/document-loaders/README.md">
---
description: Document Loaders - Enhancing AI Responses with Your Data
---

# Document Loaders

Document loaders are a powerful feature of AnswerAgentAI that allow you to integrate your own data from various sources, significantly enhancing the quality and relevance of AI responses. By incorporating files, web content, or third-party services, you can create a rich knowledge base tailored to your specific needs.

## Overview

Document loaders serve as a bridge between your data sources and AnswerAgentAI's language models. They enable the system to ingest, process, and understand information from diverse formats and locations, making this data available for AI-powered analysis and response generation.

## Key Benefits

1. **Customized Knowledge Base**: Integrate your specific data to create AI responses that are highly relevant to your domain or use case.
2. **Diverse Data Sources**: Access information from a wide range of formats and platforms, from local files to web content and third-party services.
3. **Enhanced AI Responses**: Improve the accuracy, relevance, and depth of AI-generated answers by incorporating your own trusted data sources.

## How to Use

1. Choose the appropriate document loader based on your data source.
2. Configure the loader with necessary parameters (e.g., file paths, API keys, URLs).
3. Use the loader to extract and process the data.
4. Integrate the loaded documents into your AnswerAgentAI workflows or knowledge bases.

## Types of Document Loaders

AnswerAgentAI offers a variety of document loaders to accommodate different data sources:

### File-based Loaders

-   [PDF Files](pdf-file.md)
-   [CSV File](csv-file.md)
-   [Text File](text-file.md)
-   [Docx File](docx-file.md)
-   [Json File](json-file.md)
-   [Json Lines File](json-lines-file.md)

### Web and API-based Loaders

-   [API Loader](api-loader.md)
-   [Cheerio Web Scraper](cheerio-web-scraper.md)
-   [Playwright Web Scraper](playwright-web-scraper.md)
-   [Puppeteer Web Scraper](puppeteer-web-scraper.md)
-   [SearchApi For Web Search](searchapi-for-web-search.md)
-   [SerpApi For Web Search](serpapi-for-web-search.md)

### Third-party Service Loaders

-   [Airtable](airtable.md)
-   [Confluence](confluence.md)
-   [Contentful](contentful.md)
-   [Figma](figma.md)
-   [Github](github.md)
-   [Gmail](gmail.md)
-   [Google Drive](google-drive.md)
-   [Notion Database](notion-database.md)
-   [Notion Folder](notion-folder.md)
-   [Notion Page](notion-page.md)
-   [S3 File Loader](s3-file-loader.md)

### Specialized Loaders

-   [Apify Website Content Crawler](apify-website-content-crawler.md)
-   [Custom Document Loader](custom-document-loader.md)
-   [Document Store](document-store.md)
-   [Folder with Files](folder-with-files.md)
-   [GitBook](gitbook.md)
-   [Unstructured File Loader](unstructured-file-loader.md)
-   [Unstructured Folder Loader](unstructured-folder-loader.md)
-   [VectorStore To Document](vectorstore-to-document.md)

## Using Document Loaders in AnswerAgentAI Workflows

Document loaders can be seamlessly integrated into AnswerAgentAI workflows to create powerful, data-driven AI applications:

1. **Information Retrieval**: Use document loaders to create a custom knowledge base, then use retrieval techniques to find relevant information for user queries.

2. **Document Analysis**: Load multiple documents and use AI to summarize, compare, or extract key information.

3. **Content Generation**: Incorporate loaded documents as context for AI-generated content, ensuring outputs are grounded in specific data or domain knowledge.

4. **Question Answering**: Build Q&A systems that can reference your loaded documents to provide accurate, context-aware answers.

For detailed information on creating workflows with document loaders, refer to our [Sidekick Studio Docs](../../README.md).

## Creating Knowledge Bases with Document Loaders

Document loaders are essential for building comprehensive knowledge bases in AnswerAgentAI. By combining different loaders, you can create a rich, diverse pool of information:

1. Choose relevant document loaders for your data sources.
2. Load and process documents from each source.
3. Combine the loaded documents into a unified knowledge base.
4. Use vector stores or other indexing methods to make the knowledge base searchable.

For in-depth guidance on creating and managing knowledge bases with document loaders, see our [Document Management Documentation](../../../sidekick-studio/documents/).

## Tips and Best Practices

1. Choose the right loader for your data source to ensure optimal extraction and processing.
2. Regularly update your loaded documents to keep your knowledge base current.
3. Use a combination of loaders to create a diverse and comprehensive knowledge base.
4. Consider data privacy and security when loading sensitive information.
5. Optimize large document loads for performance by using text splitters.

## Troubleshooting

-   If a loader fails to extract data, check the source format and loader configuration.
-   For web-based loaders, ensure you have the necessary permissions and that the target site allows scraping.
-   When dealing with large volumes of data, monitor system resources and consider splitting the load into smaller batches using text splitters.

By leveraging document loaders effectively, you can significantly enhance the capabilities of AnswerAgentAI, creating AI-powered solutions that are deeply integrated with your specific data and knowledge domains.
</file>

<file path="sidekick-studio/chatflows/document-loaders/s3-file-loader.md">
---
description: Load and process files from Amazon S3 buckets
---

# S3 File Loader

## Overview

The S3 File Loader is a powerful feature that allows you to retrieve files from Amazon S3 buckets and process them using Unstructured.io. This feature enables you to work with a wide range of file types, including PDF, XML, DOCX, and CSV, converting them into structured Document objects ready for vector embeddings.

## Key Benefits

-   Access and process files directly from your S3 buckets
-   Support for multiple file types through Unstructured.io integration
-   Seamless integration with AnswerAgentAI's document processing pipeline

## How to Use

1. Set up Unstructured.io:

    - Choose between the hosted API or running locally via Docker:

        - For hosted API, visit the [Unstructured API documentation](https://unstructured-io.github.io/unstructured/api.html)
        - For Docker, run:

            ```
            docker run -p 8000:8000 -d --rm --name unstructured-api quay.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0
            ```

2. Configure S3 File Loader:
   a. Drag and drop the S3 File Loader onto the canvas
   <!-- TODO: Screenshot of S3 File Loader node on canvas -->
   <figure><img src="/.gitbook/assets/screenshots/s3loader.png" alt="" /><figcaption><p> S3 File Loader Node &#x26; Drop UI</p></figcaption></figure>

    b. Set up AWS Credentials:

    - Create a new credential for your AWS account
    - Provide the access key and secret key
    - Ensure proper S3 bucket policy is granted to the associated account
      <!-- TODO: Screenshot of AWS Credential configuration -->
      <figure><img src="/.gitbook/assets/screenshots/s3awsconfiguration.png" alt="" /><figcaption><p> S3 File Loader Node AWS COnfiguration &#x26; Drop UI</p></figcaption></figure>

    c. Configure S3 settings:

    - Enter your S3 Bucket name
    - Specify the Object Key (unique identifier for your file in S3)
    - Select the appropriate AWS Region
      <!-- TODO: Screenshot of S3 configuration settings -->
      <figure><img src="/.gitbook/assets/screenshots/s3configuration.png" alt="" /><figcaption><p> S3 File Loader Node Settings &#x26; Drop UI</p></figcaption></figure>

    d. Set up Unstructured API:

    - Enter the Unstructured API URL
    - If using the Hosted API, provide the API key

3. Start using the S3 File Loader:
    - Once configured, you can begin interacting with your S3-stored files through AnswerAgentAI
    - The system will automatically handle document chunking using Unstructured

## Tips and Best Practices

1. Ensure your AWS credentials have the necessary permissions to access the S3 bucket and objects.
2. When using the hosted Unstructured API, keep your API key secure and don't share it publicly.
3. Choose the appropriate AWS region to minimize latency and comply with data residency requirements.
4. Regularly review and update your S3 bucket policies to maintain security.

## Troubleshooting

1. Issue: Unable to access S3 bucket
   Solution: Verify your AWS credentials and ensure the correct bucket policies are in place.

2. Issue: File processing errors
   Solution: Check that the file type is supported by Unstructured.io and that the Unstructured API is correctly configured.

3. Issue: Slow performance
   Solution: Consider using an AWS region closer to your location or upgrading your Unstructured API plan for better performance.

By leveraging the S3 File Loader, you can seamlessly integrate your Amazon S3-stored documents into your AnswerAgentAI workflows, enabling powerful document processing and analysis capabilities.
</file>

<file path="sidekick-studio/chatflows/document-loaders/searchapi-for-web-search.md">
---
description: Load data from real-time search results using SearchAPI
---

# SearchAPI For Web Search

## Overview

The SearchAPI For Web Search feature in AnswerAgentAI allows you to load data from real-time search results across multiple search engines. This powerful tool enables you to incorporate up-to-date information from the web directly into your AnswerAgentAI workflows.

## Key Benefits

-   Access real-time search results from multiple search engines
-   Customize search parameters for precise data retrieval
-   Seamlessly integrate web search data into your AnswerAgentAI projects

## How to Use

1. Connect your SearchAPI credential:

    - Click on the "Connect Credential" option
    - Select or add your SearchAPI credential

2. Configure the search parameters:

    - Enter your search query in the "Query" field
    - (Optional) Add custom parameters in JSON format to fine-tune your search

3. (Optional) Add a Text Splitter:

    - Connect a Text Splitter node to break down large texts into smaller chunks

4. (Optional) Add additional metadata:

    - Input any extra metadata you want to include with your search results

5. (Optional) Omit specific metadata keys:

    - List any metadata keys you want to exclude from the results

6. Run your workflow to retrieve and process the search results

<!-- TODO: Add a screenshot showing the SearchAPI For Web Search node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/searchapi.png" alt="" /><figcaption><p> Search API Node Configuration Node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use specific and targeted search queries to get the most relevant results
2. Experiment with custom parameters to refine your search (refer to the [SearchAPI documentation](https://www.searchapi.io/docs/google) for available options)
3. When dealing with large amounts of text, use a Text Splitter to make the content more manageable for further processing
4. Utilize the additional metadata feature to add context or categorization to your search results
5. Use the "Omit Metadata Keys" option to remove any sensitive or unnecessary information from the results

## Troubleshooting

1. If you're not getting any results:

    - Double-check your SearchAPI credential to ensure it's valid
    - Verify that your search query is not too specific or using unsupported syntax
    - Check the SearchAPI documentation to ensure you're using supported parameters

2. If the results are not as expected:

    - Review and adjust your custom parameters
    - Try modifying your search query to be more specific or use different keywords

3. If you're experiencing rate limiting or quota issues:
    - Check your SearchAPI plan limits
    - Consider implementing caching or rate limiting in your workflow to manage API usage

Remember that the quality and relevance of your search results depend on your query and the data available on the web at the time of the search. Always verify the information retrieved for critical applications.
</file>

<file path="sidekick-studio/chatflows/document-loaders/serpapi-for-web-search.md">
---
description: Load and process data from web search results using SerpAPI
---

# SerpAPI For Web Search

## Overview

The SerpAPI For Web Search feature in AnswerAgentAI allows you to load and process data from web search results. This powerful tool enables you to integrate web search capabilities into your workflows, providing access to a wide range of up-to-date information from the internet.

## Key Benefits

-   Access real-time web search results within AnswerAgentAI
-   Easily integrate web data into your workflows
-   Customize search queries and process results according to your needs

## How to Use

1. Add the SerpAPI For Web Search node to your workflow.
2. Connect your SerpAPI credential:
    <!-- TODO: Screenshot of connecting SerpAPI credential -->
    <figure><img src="/.gitbook/assets/screenshots/serpapi.png" alt="" /><figcaption><p> Search API Node Configuration Node  &#x26; Drop UI</p></figcaption></figure>

3. Configure the node with the following inputs:

    - Query: Enter the search query you want to perform.
    - Text Splitter (optional): Connect a Text Splitter node if you want to split the search results into smaller chunks.
    - Additional Metadata (optional): Add any extra metadata you want to include with the search results.
    - Omit Metadata Keys (optional): Specify any metadata keys you want to exclude from the results.

4. Connect the SerpAPI node to other nodes in your workflow to process or use the search results.

## Tips and Best Practices

1. Use specific and targeted search queries to get the most relevant results.
2. Experiment with different Text Splitters to optimize the processing of search results for your specific use case.
3. Utilize the Additional Metadata feature to add context or categorization to your search results.
4. Use the Omit Metadata Keys option to streamline the data you receive and focus on the most important information.

## Troubleshooting

1. If you're not getting any results, double-check that your SerpAPI credential is correctly configured and that you have a valid API key.
2. Ensure that your search query is not too broad or too narrow. Adjust it if you're not getting the expected results.
3. If you're experiencing rate limiting issues, consider implementing a delay between requests or upgrading your SerpAPI plan for higher usage limits.

<!-- TODO: Screenshot of a complete workflow using SerpAPI For Web Search -->
<figure><img src="/.gitbook/assets/screenshots/serpapiinaworkflow.png" alt="" /><figcaption><p> Search API Node Configuration Node  &#x26; Drop UI</p></figcaption></figure>

By following this documentation, you should be able to effectively use the SerpAPI For Web Search feature in AnswerAgentAI to enhance your workflows with web search capabilities.
</file>

<file path="sidekick-studio/chatflows/document-loaders/text-file.md">
---
description: Load and process text files in AnswerAgentAI
---

# Text File Document Loader

## Overview

The Text File Document Loader is a powerful feature in AnswerAgentAI that allows you to import and process data from various text-based file formats. This tool is essential for users who need to work with existing text documents, source code files, or any other text-based data in their AnswerAgentAI workflows.

## Key Benefits

-   Supports a wide range of text-based file formats, including .txt, .html, .cpp, .py, and many more
-   Offers flexible text splitting options for optimal processing
-   Allows addition of custom metadata to extracted documents

## How to Use

1. In your AnswerAgentAI workflow, add the "Text File" node from the "Document Loaders" category.
2. Configure the node with the following settings:

    - Txt File: Upload or select the text file(s) you want to process.
    - Text Splitter (optional): Choose a text splitter if you want to break the document into smaller chunks.
    - Additional Metadata (optional): Add any extra metadata as a JSON object.
    - Omit Metadata Keys (optional): Specify metadata keys to exclude from the output.

3. Connect the Text File node to other nodes in your workflow as needed.
4. Run your workflow to process the text file(s) and use the extracted data in subsequent steps.

<!-- TODO: Add a screenshot of the Text File node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/textfilenode.png" alt="" /><figcaption><p> Text File Node Configuration   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   When working with large text files, consider using a Text Splitter to break the content into more manageable chunks.
-   Use the Additional Metadata option to add relevant information to your documents, such as source, date, or category.
-   If you're processing multiple files with similar structures, consider using a loop or batch processing technique in your workflow.

## Troubleshooting

-   If your file isn't recognized, double-check that its extension is supported by the Text File node.
-   For large files, if you encounter memory issues, try using a Text Splitter with smaller chunk sizes.

## Comparison with Plain Text Input

The Text File Document Loader differs from plain text input in several key aspects:

1. File-based input: This loader requires you to upload or specify a .txt file (or other supported text-based file formats), whereas plain text input allows you to paste text directly into the node.

2. Multiple file support: The Text File loader can process multiple files at once, which is not possible with plain text input.

3. Broader file format support: While plain text is limited to, well, plain text, this loader supports various text-based file formats including source code files (.cpp, .py, etc.) and markup languages (.html, .md, etc.).

4. Metadata handling: The Text File loader automatically extracts metadata from the files (such as file name, size, etc.) and allows you to add or omit specific metadata fields. Plain text input typically doesn't include this metadata management.

5. Use case: Use the Text File loader when you have existing files you need to process or when working with code or structured text files. Use plain text input when you want to quickly input or test with a small amount of text without creating a separate file.

Choose between the Text File Document Loader and plain text input based on your specific use case, the format of your data, and whether you need the additional features provided by the file-based loader.
</file>

<file path="sidekick-studio/chatflows/document-loaders/unstructured-file-loader.md">
---
description: Load and process various file types using Unstructured.io
---

# Unstructured File Loader

## Overview

The Unstructured File Loader is a powerful feature in AnswerAgentAI that allows you to load and process various file types using Unstructured.io. This tool enables you to extract structured data from unstructured documents, making it easier to work with a wide range of file formats.

## Key Benefits

-   Supports multiple file formats, including PDF, DOCX, JPG, HTML, and more
-   Offers flexible processing options to optimize data extraction
-   Integrates seamlessly with other AnswerAgentAI features for enhanced document analysis

## How to Use

1. Navigate to the Document Loaders section in AnswerAgentAI.
2. Select the "Unstructured File Loader" option.
3. Configure the loader settings:
    - Choose between uploading files or specifying a file path
    - Set the Unstructured API URL (default: `http://localhost:8000/general/v0/general`
    - Select a processing strategy (Hi-Res, Fast, OCR Only, or Auto)
    - Adjust additional parameters as needed (e.g., encoding, chunking strategy)
4. Upload your files or provide the file path.
5. Run the loader to process your documents.

<!-- TODO: Add a screenshot of the Unstructured File Loader configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/unstructuredfileloader.png" alt="" /><figcaption><p> Unstructured File Loader Configuration   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use the File Upload option for easier file management within AnswerAgentAI.
2. Experiment with different processing strategies to find the best balance between speed and accuracy for your specific documents.
3. Utilize the "Additional Metadata" field to add custom information to your extracted documents.
4. When working with multi-language documents, specify the appropriate OCR languages for better text recognition.

## Troubleshooting

1. If you encounter issues with file processing:

    - Ensure that the Unstructured API URL is correct and accessible
    - Check if the file format is supported by the loader
    - Verify that you have the necessary credentials if using the Unstructured API key

2. For slow processing times:

    - Try using the "Fast" strategy for quicker results
    - Consider breaking large documents into smaller chunks before processing

3. If extracted text is incomplete or inaccurate:
    - Experiment with different processing strategies
    - Adjust the OCR languages if working with non-English documents
    - Use the "Hi-Res" strategy for better accuracy, especially with complex layouts

## Additional Configuration Options

The Unstructured File Loader offers several advanced configuration options to fine-tune your document processing:

1. **Skip Infer Table Types**: Choose which document types to skip table extraction for.
2. **Hi-Res Model Name**: Select the inference model used for high-resolution processing.
3. **Chunking Strategy**: Determine how the extracted elements should be chunked.
4. **Coordinates**: Enable to return coordinates for each extracted element.
5. **XML Keep Tags**: Retain XML tags in the output for XML documents.
6. **Include Page Breaks**: Add page break elements to the output when supported by the file type.

<!-- TODO: Add a screenshot showcasing the advanced configuration options -->
<figure><img src="/.gitbook/assets/screenshots/unstructuredfileloaderadvanced.png" alt="" /><figcaption><p> Unstructured File Loader Advanced Configuration   &#x26; Drop UI</p></figcaption></figure>

By leveraging these options, you can customize the Unstructured File Loader to best suit your specific document processing needs in AnswerAgentAI.
</file>

<file path="sidekick-studio/chatflows/document-loaders/unstructured-folder-loader.md">
---
description: Load and process documents from a folder using Unstructured.io
---

# Unstructured Folder Loader

## Overview

The Unstructured Folder Loader is a powerful feature in AnswerAgentAI that allows you to load and process documents from a specified folder using Unstructured.io. This tool is particularly useful for extracting and structuring data from various file types, making it easier to work with large volumes of unstructured documents.

## Key Benefits

-   Process multiple documents from a single folder
-   Support for various file formats (e.g., PDF, DOCX, TXT)
-   Customizable options for document processing and chunking

## How to Use

1. In the AnswerAgentAI interface, locate and select the "Unstructured Folder Loader" node.

2. Configure the following required settings:

    - Folder Path: Enter the path to the folder containing your documents.
    - Unstructured API URL: Provide the URL for the Unstructured API (default: `http://localhost:8000/general/v0/general`.

3. (Optional) Connect your Unstructured API credentials if you're using the hosted version.

4. Adjust additional settings as needed:

    - Strategy: Choose between "Hi-Res," "Fast," "OCR Only," or "Auto" (default: Auto).
    - Encoding: Specify the encoding method (default: utf-8).
    - Skip Infer Table Types: Select file types to skip table extraction.
    - Hi-Res Model Name: Choose the inference model for hi-res strategy.
    - Chunking Strategy: Select "None" or "By Title" for document chunking.

5. Configure any other optional parameters to fine-tune the document processing.

6. Connect the Unstructured Folder Loader node to other nodes in your AnswerAgentAI workflow.

7. Run your workflow to process the documents in the specified folder.

## Tips and Best Practices

1. Organize your documents in a dedicated folder for easier management.
2. Start with the default settings and adjust as needed for optimal results.
3. Use the "Skip Infer Table Types" option to improve processing speed for documents without tables.
4. Experiment with different chunking strategies to find the best approach for your specific use case.
5. Add custom metadata to enrich your processed documents for better searchability and context.

## Troubleshooting

1. If documents fail to load, ensure that the folder path is correct and accessible.
2. Verify that the Unstructured API URL is correct and the service is running.
3. For issues with specific file types, check if they are supported by the current version of Unstructured.io.
4. If processing is slow, try adjusting the strategy or using a faster hi-res model.

<!-- TODO: Add a screenshot of the Unstructured Folder Loader node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/unstructuredfolderloader.png" alt="" /><figcaption><p> Unstructured Folder Loader Configuration   &#x26; Drop UI</p></figcaption></figure>

Remember to keep your Unstructured.io installation up-to-date for the best performance and compatibility with various file types. For more information on Unstructured.io and its capabilities, visit the [official documentation](https://unstructured-io.github.io/unstructured/introduction.html).
</file>

<file path="sidekick-studio/chatflows/document-loaders/vectorstore-to-document.md">
---
description: Search and customize documents from vector stores
---

# VectorStore to Document

## Overview

The VectorStore to Document feature allows you to retrieve and customize documents from a vector store based on a given query. This powerful tool enables you to fine-tune the context returned from your vector database and manipulate the data through various operations.

## Key Benefits

-   Customizable document retrieval from vector stores
-   Flexible integration with other chains for data manipulation
-   Can be used as an ending node for easy application integration

## How to Use

1. Connect a Vector Store to the "Vector Store" input.
2. (Optional) Provide a specific query in the "Query" input. If left empty, the user's question will be used.
3. (Optional) Set a "Minimum Score (%)" to filter out less relevant documents.
4. Choose the desired output format: "Document," "Text," or "Ending Node."

<!-- TODO: Add a screenshot of the node configuration UI -->
<figure><img src="/.gitbook/assets/screenshots/vector store to document.png" alt="" /><figcaption><p> Unstructured Folder Loader Configuration   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. **Customizing Context**: Use the "Query" input to refine the context retrieved from the vector store. This allows you to focus on specific aspects of your data.

2. **Data Manipulation**: Leverage the "Document" output to chain this node with other nodes for further data processing. For example:

    - Use a text splitter to break down long documents
    - Apply a summarization chain to condense the information
    - Implement custom filtering or transformation logic

3. **Integration as an Ending Node**: Utilize the "Ending Node" output to seamlessly integrate this feature into your applications. This is particularly useful for:

    - Retrieving relevant documents for display in a user interface
    - Providing context to other AI models or tools

4. **Fine-tuning Relevance**: Adjust the "Minimum Score (%)" to control the quality of returned documents. A higher percentage will result in more relevant but potentially fewer documents.

5. **Optimizing for ChatGPT Integration**: When using this feature as a tool for ChatGPT:
    - Use the "Text" output to provide a concatenated string of relevant information
    - Adjust the query and minimum score to ensure the most pertinent information is passed to ChatGPT

## Troubleshooting

1. **No documents returned**:

    - Check if your minimum score is set too high
    - Verify that your query is relevant to the content in your vector store
    - Ensure your vector store is properly populated with data

2. **Irrelevant documents**:

    - Try increasing the minimum score
    - Refine your query to be more specific
    - Review the contents of your vector store to ensure it contains the expected data

3. **Performance issues**:
    - If retrieval is slow, consider optimizing your vector store or reducing the number of documents returned

## Example Use Cases

1. **Custom Knowledge Base**: Use this feature to create a tailored knowledge base for a chatbot, retrieving only the most relevant information based on user queries.

2. **Document Summarization Pipeline**: Chain this node with a summarization node to automatically generate summaries of the most relevant documents for a given topic.

3. **ChatGPT Integration**: Implement this as a tool for ChatGPT to provide it with up-to-date, internal information from your vector store, allowing for more accurate and context-aware responses.

4. **Dynamic Content Filtering**: Use this node as part of a larger chain to dynamically filter and present content in a user interface based on user preferences or behavior.

By leveraging the VectorStore to Document feature, you can create powerful, context-aware applications that make the most of your vector database while providing flexibility in how you process and present the retrieved information.
</file>

<file path="sidekick-studio/chatflows/embeddings/aws-bedrock-embeddings.md">
---
description: Generate embeddings using AWS Bedrock embedding models
---

# AWS Bedrock Embeddings

## Overview

AWS Bedrock Embeddings is a powerful feature in AnswerAgentAI that allows you to generate embeddings for a given text using AWS Bedrock embedding models. Embeddings are vector representations of text that capture semantic meaning, making them useful for various natural language processing tasks.

## Key Benefits

-   Access to state-of-the-art embedding models from AWS Bedrock
-   Flexible configuration options for different use cases
-   Seamless integration with other AnswerAgentAI components

## How to Use

To use AWS Bedrock Embeddings in AnswerAgentAI, follow these steps:

1. Add the AWS Bedrock Embeddings node to your workflow.
2. Configure the node settings:

    - Select the AWS Region
    - Choose the Model Name
    - (Optional) Provide a Custom Model Name
    - (For Cohere models) Select the Cohere Input Type

3. Connect the node to other components in your workflow that require text embeddings.

<!-- TODO: Screenshot of the AWS Bedrock Embeddings node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/aws chatbedrock in a workflow.png" alt="" /><figcaption><p> AWS Bedrock Embeddings Node   &#x26; Drop UI</p></figcaption></figure>

### Configuration Options

#### AWS Credential

-   Label: AWS Credential
-   Type: Credential
-   Optional: Yes

If provided, use your AWS API credentials for authentication.

#### Region

-   Label: Region
-   Type: Dropdown (Async)
-   Default: us-east-1

Select the AWS region where you want to run the embedding model.

#### Model Name

-   Label: Model Name
-   Type: Dropdown (Async)
-   Default: amazon.titan-embed-text-v1

Choose the embedding model you want to use. The list of available models is dynamically loaded.

#### Custom Model Name

-   Label: Custom Model Name
-   Type: String
-   Optional: Yes

If you want to use a specific model not listed in the Model Name dropdown, you can provide its name here. This will override the selected Model Name.

#### Cohere Input Type

-   Label: Cohere Input Type
-   Type: Dropdown
-   Optional: Yes (Required for Cohere models v3 and higher)

Specifies the type of input passed to Cohere embedding models. Options include:

-   search_document: For encoding documents to store in a vector database for search use-cases
-   search_query: For querying your vector database to find relevant documents
-   classification: For using embeddings as input to a text classifier
-   clustering: For clustering the embeddings

## Tips and Best Practices

1. Choose the appropriate region based on your data residency requirements and latency considerations.
2. When using Cohere models, always select the appropriate Input Type for your use case to ensure optimal performance.
3. Experiment with different models to find the one that works best for your specific task.
4. If you're using a custom or fine-tuned model, use the Custom Model Name field to specify it.

## Troubleshooting

1. **Error: Input Type must be selected for Cohere models**

    - Make sure you've selected an Input Type when using Cohere embedding models.

2. **Error: An invalid response was returned by Bedrock**

    - Check your AWS credentials and ensure you have the necessary permissions to access the selected model.
    - Verify that the selected region supports the chosen model.

3. **Unexpected embedding results**
    - Ensure that the input text is properly formatted and cleaned (the node automatically replaces newlines with spaces).
    - Try a different model or adjust the input type (for Cohere models) to see if it improves results.

For any persistent issues, consult the AWS Bedrock documentation or contact AnswerAgentAI support for assistance.
</file>

<file path="sidekick-studio/chatflows/embeddings/azure-openai-embeddings.md">
---
description: Generate embeddings using Azure OpenAI API
---

# Azure OpenAI Embeddings

## Prerequisite

1. [Log in](https://portal.azure.com/) or [sign up](https://azure.microsoft.com/en-us/free/) to Azure
2. [Create](https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI) your Azure OpenAI and wait for approval approximately 10 business days
3. Your API key will be available at **Azure OpenAI** > click **name_azure_openai** > click **Click here to manage keys**

## Overview

The Azure OpenAI Embeddings feature in AnswerAgentAI allows you to generate embeddings for given text using the Azure OpenAI API. Embeddings are vector representations of text that capture semantic meaning, which can be used for various natural language processing tasks.

## Key Benefits

-   Leverage Azure's powerful OpenAI models for generating high-quality embeddings
-   Easily integrate embeddings into your AnswerAgentAI workflows
-   Customize batch size and timeout settings for optimal performance

## How to Use

1. Add the Azure OpenAI Embeddings node to your AnswerAgentAI workflow.

<!-- TODO: Screenshot of adding the Azure OpenAI Embeddings node to the workflow -->
<figure><img src="/.gitbook/assets/screenshots/azureopenaiembeddinginaworkflow.png" alt="" /><figcaption><p> Azure OpenAI Embeddings Node In a Workflow   &#x26; Drop UI</p></figcaption></figure>

2. Connect your Azure OpenAI API credential:
    - Click on the node to open its settings
    - In the "Connect Credential" field, select your Azure OpenAI API credential or create a new one

<!-- TODO: Screenshot of connecting the Azure OpenAI API credential -->
<figure><img src="/.gitbook/assets/screenshots/azurecredentialsembedding.png" alt="" /><figcaption><p> Azure OpenAI Embeddings Credentials   &#x26; Drop UI</p></figcaption></figure>

3. (Optional) Configure additional parameters:
    - Batch Size: Set the number of texts to process in a single API call (default: 100)
    - Timeout: Specify the maximum time (in milliseconds) to wait for the API response

<!-- TODO: Screenshot of configuring additional parameters -->
<figure><img src="/.gitbook/assets/screenshots/azureopenaiembeddingnadditionalparameters.png" alt="" /><figcaption><p> Azure OpenAI Embeddings Node Additional Parameters   &#x26; Drop UI</p></figcaption></figure>

4. Connect the Azure OpenAI Embeddings node to other nodes in your workflow that require text embeddings.

5. Run your workflow to generate embeddings for your input text.

## Tips and Best Practices

-   Choose an appropriate batch size based on your input data and API rate limits. A larger batch size can improve efficiency, but may increase the risk of timeouts.
-   Monitor your Azure OpenAI API usage to ensure you stay within your quota limits.
-   Use embeddings for tasks such as semantic search, text classification, or clustering to improve the performance of your natural language processing applications.

## Troubleshooting

1. API Key Issues:

    - Ensure that your Azure OpenAI API key is correct and has the necessary permissions.
    - Check that the API key is properly stored in your AnswerAgentAI credentials.

2. Timeout Errors:

    - If you encounter timeout errors, try increasing the timeout value in the node settings.
    - Alternatively, reduce the batch size to process smaller chunks of text at a time.

3. Deployment Name Mismatch:

    - Verify that the Azure OpenAI API deployment name in your credentials matches the one in your Azure account.

4. API Version Compatibility:
    - Make sure you're using a compatible Azure OpenAI API version. Check the Azure documentation for the latest supported versions.

If you continue to experience issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/embeddings/cohere-embeddings.md">
---
description: Generate embeddings for text using Cohere API
---

# Cohere Embeddings

## Overview

Cohere Embeddings is a powerful feature in AnswerAgentAI that allows you to generate embeddings for a given text using the Cohere API. Embeddings are numerical representations of text that capture semantic meaning, making them useful for various natural language processing tasks.

## Key Benefits

-   Generate high-quality embeddings for text analysis and processing
-   Flexible options for different use cases, including search, classification, and clustering
-   Seamless integration with Cohere's advanced embedding models

## How to Use

To use the Cohere Embeddings feature in AnswerAgentAI, follow these steps:

1. Add the Cohere Embeddings node to your workflow.
2. Configure the node settings:
   a. Connect your Cohere API credential.
   b. Select the embedding model.
   c. Choose the input type for your use case.
3. Connect the node to your input source and output destination.
4. Run your workflow to generate embeddings.

<!-- TODO: Screenshot of the Cohere Embeddings node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/cohereembeddingnode.png" alt="" /><figcaption><p> Cohere Embeddings Node Configuration Panel  &#x26; Drop UI</p></figcaption></figure>

### Connecting Your Credential

1. Click on the "Connect Credential" dropdown.
2. Select an existing Cohere API credential or create a new one.
3. If creating a new credential, enter your Cohere API key.

### Selecting the Model

1. Click on the "Model Name" dropdown.
2. Choose from the available embedding models (e.g., "embed-english-v2.0").

### Choosing the Input Type

1. Click on the "Type" dropdown.
2. Select the appropriate input type for your use case:
    - `search_document`: For encoding documents to store in a vector database for search use-cases.
    - `search_query`: For querying your vector database to find relevant documents.
    - `classification`: For using embeddings as input to a text classifier.
    - `clustering`: For clustering embeddings.

## Tips and Best Practices

1. Choose the appropriate input type based on your specific use case to optimize embedding performance.
2. Experiment with different embedding models to find the best fit for your task.
3. When using embeddings for search, use `search_document` for indexing and `search_query` for querying to ensure optimal results.
4. Keep your Cohere API key secure and never share it publicly.

## Troubleshooting

1. If you encounter authentication errors, double-check your Cohere API key in the credential settings.
2. Ensure you have an active Cohere API subscription with sufficient quota for your embedding needs.
3. If you experience slow performance, consider using a more powerful embedding model or optimizing your input data.

For more information on Cohere embeddings and their capabilities, refer to the [official Cohere documentation](https://docs.cohere.com/reference/embed).
</file>

<file path="sidekick-studio/chatflows/embeddings/googlegenerativeai-embeddings.md">
---
description: Generate embeddings using Google Generative AI
---

# GoogleGenerativeAI Embeddings

## Overview

The GoogleGenerativeAI Embeddings feature in AnswerAgentAI allows you to generate embeddings for given text using Google's Generative AI API. Embeddings are vector representations of text that capture semantic meaning, which can be used for various natural language processing tasks.

## Key Benefits

-   Leverage Google's advanced AI technology for generating high-quality embeddings
-   Customize embeddings for specific task types to improve performance
-   Seamlessly integrate with other AnswerAgentAI features

## How to Use

1. Connect your Google Generative AI Credential:

    - Ensure you have a valid Google Generative AI API key
    - Configure your credential in AnswerAgentAI

2. Configure the GoogleGenerativeAI Embeddings node:
   a. Select the model:

    - Click on the "Model Name" dropdown
    - Choose the desired embedding model (default is "embedding-001")

    b. Choose the task type:

    - Click on the "Task Type" dropdown
    - Select the appropriate task type for your use case (default is "TASK_TYPE_UNSPECIFIED")

3. Connect the GoogleGenerativeAI Embeddings node to other nodes in your workflow as needed

<!-- TODO: Add a screenshot of the GoogleGenerativeAI Embeddings node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/googlegennerativeaiembeddings.png" alt="" /><figcaption><p> GoogleGenerativeAI Embeddings Node Configuration Panel  &#x26; Drop UI</p></figcaption></figure>
## Task Types

The following task types are available:

-   TASK_TYPE_UNSPECIFIED: General-purpose embedding
-   RETRIEVAL_QUERY: Optimized for retrieval queries
-   RETRIEVAL_DOCUMENT: Optimized for document retrieval
-   SEMANTIC_SIMILARITY: Optimized for semantic similarity tasks
-   CLASSIFICATION: Optimized for classification tasks
-   CLUSTERING: Optimized for clustering tasks

## Tips and Best Practices

1. Choose the appropriate task type for your specific use case to optimize embedding performance
2. Experiment with different models to find the best fit for your needs
3. Ensure your Google Generative AI API key has the necessary permissions for embedding generation
4. Monitor your API usage to stay within your quota limits

## Troubleshooting

1. If you encounter authentication errors:

    - Double-check that your Google Generative AI credential is correctly configured in AnswerAgentAI
    - Verify that your API key is valid and has the necessary permissions

2. If embedding generation is slow:

    - Consider using a more efficient model for your task
    - Check your network connection and API rate limits

3. If you're not getting expected results:
    - Verify that you've selected the appropriate task type for your use case
    - Try using a different model to see if it produces better results

For any persistent issues, please consult the AnswerAgentAI documentation or contact support for assistance.

<!-- TODO: Add a screenshot of a sample workflow using the GoogleGenerativeAI Embeddings node -->
<figure><img src="/.gitbook/assets/screenshots/googlegenerativeaiembeddingnodeinaowrkflow.png" alt="" /><figcaption><p> GoogleGenerativeAI Embeddings Node In A Workflow  &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/embeddings/googlevertexai-embeddings.md">
---
description: Generate embeddings using Google Vertex AI API
---

# GoogleVertexAI Embeddings

## Overview

The GoogleVertexAI Embeddings feature allows you to generate embeddings for given text using Google's Vertex AI API. Embeddings are vector representations of text that capture semantic meaning, which can be useful for various natural language processing tasks.

## Key Benefits

-   Access to powerful, state-of-the-art embedding models from Google
-   Seamless integration with Google Cloud services
-   High-quality embeddings for improved text analysis and processing

## How to Use

1. Navigate to the Embeddings section in AnswerAgentAI.
2. Locate and select the "GoogleVertexAI Embeddings" node.
3. Configure the node with the following settings:

    a. **Connect Credential**: (Optional) If you're not using a GCP service or don't have default credentials set up, provide your Google Vertex AI credential.

    b. **Model Name**: Select the desired embedding model from the dropdown list. The default is "textembedding-gecko@001".

<!-- TODO: Add a screenshot of the GoogleVertexAI Embeddings node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/googlevertexembeddingsnode.png" alt="" /><figcaption><p> GoogleVertexAI Embeddings Node In A Workflow  &#x26; Drop UI</p></figcaption></figure>

4. Connect the GoogleVertexAI Embeddings node to your input text source and any subsequent nodes that will use the generated embeddings.
5. Run your workflow to generate embeddings for your input text.

## Tips and Best Practices

1. If you're running AnswerAgentAI on Google Cloud Platform services like Cloud Run, you may not need to provide explicit credentials.
2. Choose the appropriate embedding model based on your specific use case and performance requirements.
3. Be mindful of API usage and costs associated with generating embeddings, especially for large volumes of text.
4. Consider caching embeddings for frequently used text to improve performance and reduce API calls.

## Troubleshooting

1. **Authentication Issues**:

    - Ensure you have the correct credentials set up, either through the AnswerAgentAI interface or by using default GCP credentials.
    - Verify that your Google Cloud project has the necessary APIs enabled for Vertex AI.

2. **Model Not Found**:

    - Check that you've selected a valid model name from the dropdown list.
    - Ensure your Google Cloud project has access to the selected model.

3. **Quota Exceeded**:

    - If you encounter quota errors, check your Google Cloud Console for your current usage and limits.
    - Consider requesting a quota increase if needed.

4. **Performance Issues**:
    - If embedding generation is slow, try using a different model or optimizing your input text (e.g., batching requests).

For any persistent issues, consult the Google Vertex AI documentation or contact AnswerAgentAI support for further assistance.
</file>

<file path="sidekick-studio/chatflows/embeddings/huggingface-inference-embeddings.md">
---
description: Generate embeddings using HuggingFace Inference API
---

# HuggingFace Inference Embeddings

## Overview

The HuggingFace Inference Embeddings feature in AnswerAgentAI allows you to generate embeddings for a given text using the HuggingFace Inference API. Embeddings are numerical representations of text that capture semantic meaning, which can be useful for various natural language processing tasks.

## Key Benefits

-   Access to state-of-the-art embedding models from HuggingFace
-   Flexibility to use pre-defined models or your own custom inference endpoint
-   Easy integration with other AnswerAgentAI components for advanced NLP workflows

## How to Use

1. Connect your HuggingFace API Credential:

    - Click on the "Connect Credential" option
    - Select or add your HuggingFace API credentials

2. Configure the Embedding Settings:

    - Model (Optional): Enter the name of the HuggingFace model you want to use for generating embeddings. If left blank, a default model will be used.

        - Example: `sentence-transformers/distilbert-base-nli-mean-tokens`

    - Endpoint (Optional): If you're using your own inference endpoint, enter the URL here. Leave this blank if you're using a standard HuggingFace model.
        - Example: `https://xyz.eu-west-1.aws.endpoints.huggingface.cloud/sentence-transformers/all-MiniLM-L6-v2`

3. Connect the HuggingFace Inference Embeddings node to other components in your AnswerAgentAI workflow that require text embeddings.

<!-- TODO: Screenshot of the HuggingFace Inference Embeddings node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/huggingface.png" alt="" /><figcaption><p> HuggingFace Embeddings Node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose the right model: Different embedding models are optimized for different tasks. Research and select a model that best fits your specific use case.

2. Custom endpoints: If you have fine-tuned your own embedding model, you can deploy it to a custom endpoint and use it with this feature by providing the endpoint URL.

3. API usage: Be mindful of your API usage, especially if you're processing large volumes of text. Check your HuggingFace account for usage limits and pricing details.

4. Caching: Consider implementing caching mechanisms for frequently used embeddings to reduce API calls and improve performance.

## Troubleshooting

1. Authentication errors:

    - Ensure that you have entered the correct HuggingFace API key in your credentials.
    - Verify that your API key has the necessary permissions to access the Inference API.

2. Model not found:

    - Double-check the model name for any typos.
    - Ensure that the model you're trying to use is available through the HuggingFace Inference API.

3. Endpoint connection issues:

    - If using a custom endpoint, verify that the URL is correct and the endpoint is accessible.
    - Check your network connection and firewall settings.

4. Unexpected results:
    - Verify that the input text is in the correct format expected by the model.
    - Try using a different model to see if the issue persists.

For any persistent issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/embeddings/localai-embeddings.md">
---
description: Use local embeddings models with AnswerAgentAI
---

# LocalAI Embeddings

## Overview

LocalAI Embeddings is a powerful feature in AnswerAgentAI that allows you to use local embeddings models, such as those compatible with the ggml format. This feature enables you to run Language Learning Models (LLMs) locally or on-premises using consumer-grade hardware, providing a more private and customizable alternative to cloud-based solutions.

## Key Benefits

-   **Privacy and Security**: Run embeddings locally, keeping your data on your own hardware.
-   **Customization**: Use a wide variety of models compatible with the ggml format.
-   **Cost-effective**: Eliminate the need for expensive cloud API calls by using your own computing resources.

## How to Use

### Step 1: Set up LocalAI

1. Clone the LocalAI repository:

    ```bash
    git clone https://github.com/go-skynet/LocalAI
    ```

2. Navigate to the LocalAI directory:

    ```bash
    cd LocalAI
    ```

3. Download a model using LocalAI's API endpoint. For this example, we'll use the BERT Embeddings model:

<!-- TODO: Screenshot showing the LocalAI API endpoint for downloading the BERT Embeddings model -->

4. Verify that the model has been downloaded to the `/models` folder:

<!-- TODO: Screenshot showing the downloaded model in the /models folder -->

5. Test the embeddings by running:

    ```bash
    curl http://localhost:8080/v1/embeddings -H "Content-Type: application/json" -d '{
        "input": "Test",
        "model": "text-embedding-ada-002"
      }'
    ```

6. You should receive a response similar to this:

<!-- TODO: Screenshot showing the response from the curl command -->

### Step 2: Configure AnswerAgentAI

1. In the AnswerAgentAI canvas, drag and drop a new LocalAIEmbeddings component:

<!-- TODO: Screenshot showing the LocalAIEmbeddings component being added to the canvas -->
<figure><img src="/.gitbook/assets/screenshots/localembeddingsai.png" alt="" /><figcaption><p> LocalAIEmbeddings Node  &#x26; Drop UI</p></figcaption></figure>

2. Fill in the required fields:
    - **Base Path**: Enter the base URL for LocalAI (e.g., `http://localhost:8080/v1`)
    - **Model Name**: Specify the model you want to use (e.g., `text-embedding-ada-002`)

## Tips and Best Practices

1. Ensure that the model you specify in AnswerAgentAI matches the one you've downloaded to the `/models` folder in LocalAI.
2. Keep your local models up to date to benefit from the latest improvements in embedding technology.
3. Experiment with different models to find the best balance between performance and resource usage for your specific use case.

## Troubleshooting

1. **Model not found error**: Make sure the model name specified in AnswerAgentAI exactly matches the filename in the `/models` folder of LocalAI.
2. **Connection issues**: Verify that LocalAI is running and accessible at the specified base path.
3. **Slow performance**: Consider using a more powerful machine or optimizing your LocalAI setup for better performance.

For more detailed information on LocalAI and its capabilities, refer to the [LocalAI documentation](https://localai.io/models/index.html#embeddings-bert).
</file>

<file path="sidekick-studio/chatflows/embeddings/mistralai-embeddings.md">
---
description: Generate embeddings for text using MistralAI API
---

# MistralAI Embeddings

## Overview

MistralAI Embeddings is a powerful feature in AnswerAgentAI that allows you to generate vector representations (embeddings) of text using the MistralAI API. These embeddings can be used for various natural language processing tasks, such as semantic search, text classification, and more.

## Key Benefits

-   High-quality embeddings: Leverage MistralAI's advanced language models for accurate text representations.
-   Customizable: Adjust parameters like batch size and model selection to suit your specific needs.
-   Efficient processing: Optimize your workflow with batch processing and optional performance tweaks.

## How to Use

1. Add the MistralAI Embeddings node to your AnswerAgentAI workflow.

<!-- TODO: Screenshot of adding MistralAI Embeddings node to the workflow -->
<figure><img src="/.gitbook/assets/screenshots/mistralembeddings.png" alt="" /><figcaption><p> MistralAI Embeddings Node  &#x26; Drop UI</p></figcaption></figure>

2. Configure the node settings:
   a. Connect your MistralAI API credential.
   b. Select the desired model from the "Model Name" dropdown.
   c. (Optional) Adjust additional parameters like Batch Size and Strip New Lines.

<!-- TODO: Screenshot of the node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/mistralembeddingconfiguration.png" alt="" /><figcaption><p> MistralAI Embeddings Node Configuration  &#x26; Drop UI</p></figcaption></figure>

3. Connect the MistralAI Embeddings node to your input text source and output destination.

4. Run your workflow to generate embeddings for your text data.

## Tips and Best Practices

1. Choose the appropriate model: Select the model that best fits your use case and performance requirements.

2. Optimize batch size: Adjust the batch size to balance between processing speed and memory usage.

3. Consider stripping new lines: Enable the "Strip New Lines" option to remove unnecessary whitespace, which can improve embedding quality for certain types of text.

4. Use embeddings effectively: Incorporate the generated embeddings into downstream tasks like similarity search, clustering, or as input features for machine learning models.

## Troubleshooting

1. API Key Issues:

    - Ensure that you have entered the correct MistralAI API key in the credential settings.
    - Verify that your API key has the necessary permissions to access the embedding models.

2. Model Availability:

    - If a specific model is not available, try refreshing the model list or check the MistralAI documentation for any recent changes.

3. Performance Concerns:

    - If processing is slow, try reducing the batch size or using a smaller, faster model.
    - For large-scale processing, consider using the override endpoint option to connect to a dedicated MistralAI instance.

4. Embedding Quality:
    - If the embeddings are not producing expected results, experiment with different models or adjust the input text preprocessing (e.g., try with and without stripping new lines).

Remember to respect MistralAI's usage policies and rate limits when using this feature. For any persistent issues, consult the AnswerAgentAI support documentation or contact our support team.
</file>

<file path="sidekick-studio/chatflows/embeddings/openai-embeddings-custom.md">
---
description: OpenAI Embeddings Custom - Generate tailored embeddings for your text
---

# OpenAI Embeddings Custom

## Overview

The OpenAI Embeddings Custom feature in AnswerAgentAI allows you to generate customized embeddings for your text using OpenAI's powerful API. This feature provides flexibility in creating embeddings tailored to your specific needs, enabling more accurate and relevant text representations for various natural language processing tasks.

[Customizing OpenAI Embeddings](https://cookbook.openai.com/examples/customizing_embeddings)

## Key Benefits

-   Customizable embeddings for improved text representation
-   Fine-tuned control over embedding generation parameters
-   Seamless integration with OpenAI's state-of-the-art language models

## How to Use

1. Navigate to the Embeddings section in AnswerAgentAI.
2. Select "OpenAI Embeddings Custom" from the available options.
3. Connect your OpenAI API credential.
4. Configure the embedding parameters:
    - Strip New Lines: Choose whether to remove newline characters from the input text.
    - Batch Size: Set the number of text chunks to process in a single API call.
    - Timeout: Specify the maximum time (in milliseconds) to wait for the API response.
    - Base Path: Enter a custom base path for the API endpoint, if necessary.
    - Model Name: Select the OpenAI model to use for generating embeddings.
    - Dimensions: Specify the number of dimensions for the output embeddings.
5. Input your text data for embedding generation.
6. Run the embedding process to obtain your custom embeddings.

<!-- TODO: Add a screenshot of the OpenAI Embeddings Custom configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/openaiembeddingscustom.png" alt="" /><figcaption><p> OpenAI Embeddings Custom Node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different model names to find the best fit for your specific use case.
2. Adjust the number of dimensions to balance between embedding richness and computational efficiency.
3. Use batch processing for large datasets to optimize API usage and improve performance.
4. Consider stripping new lines for texts where line breaks are not semantically important.

## Customizing Embeddings

AnswerAgentAI's OpenAI Embeddings Custom feature allows you to create tailored embeddings, similar to the process described in the OpenAI Cookbook. Here's how you can leverage this functionality:

1. **Model Selection**: Choose from various OpenAI models to generate embeddings that best suit your needs. Different models may produce embeddings with varying characteristics.

2. **Dimensionality**: Adjust the number of dimensions in your embeddings. Higher dimensions can capture more nuanced relationships but may increase computational costs.

3. **Input Preprocessing**: Use the "Strip New Lines" option to clean your input text, ensuring consistent embedding generation across different text formats.

4. **Fine-tuning for Specific Tasks**: By carefully selecting your input text and model parameters, you can create embeddings that are optimized for specific tasks such as semantic search, text classification, or content recommendation.

5. **API Customization**: Utilize the "Base Path" option to point to a specific API endpoint, allowing for potential use of fine-tuned models or custom deployment scenarios.

<!-- TODO: Add a diagram illustrating the process of customizing embeddings -->

## Troubleshooting

1. **API Key Issues**: Ensure your OpenAI API key is correctly entered in the credential settings.
2. **Timeout Errors**: If you encounter timeout errors, try increasing the timeout value or reducing the batch size.
3. **Dimension Mismatch**: Make sure the specified number of dimensions is supported by the chosen model.
4. **Rate Limiting**: If you hit API rate limits, consider adjusting your batch size or implementing a backoff strategy in your workflows.

By leveraging the OpenAI Embeddings Custom feature in AnswerAgentAI, you can create powerful, tailored text representations that enhance the performance of your natural language processing tasks.
</file>

<file path="sidekick-studio/chatflows/embeddings/openai-embeddings.md">
---
description: OpenAI Embeddings for Text Processing in AnswerAgentAI
---

# OpenAI Embeddings

## Overview

OpenAI Embeddings is a powerful feature in AnswerAgentAI that generates vector representations (embeddings) of text. These embeddings are crucial for various natural language processing tasks, including semantic search, text classification, and content recommendation.

## Key Benefits

-   High-quality text representations for improved language understanding
-   Seamless integration with other AnswerAgentAI features
-   Versatile applications across various NLP tasks

## How to Use

1. In your AnswerAgentAI workflow, locate the Embeddings section.
2. Select "OpenAI Embeddings" from the available options.
3. Configure the embedding parameters:
    - Model Name: Choose the desired OpenAI embedding model (default is "text-embedding-ada-002").
    - Strip New Lines: Enable to remove newline characters from the input text.
    - Batch Size: Set the number of texts to process in a single API call.
    - Timeout: Specify the maximum time (in milliseconds) to wait for the API response.
    - BasePath: Enter a custom API endpoint if needed.
    - Dimensions: Specify the desired embedding dimensions (if supported by the chosen model).
4. Connect your OpenAI API credentials.
5. Save and run your workflow.

<!-- TODO: Add a screenshot of the OpenAI Embeddings configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/openaiembeddingconfigurationpanel.png" alt="" /><figcaption><p> OpenAI Embeddings Configuration Panel &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. **Default Option**: OpenAI Embeddings is often the default choice for many sidekick workflows in AnswerAgentAI due to its high quality and versatility.

2. **Experiment with Alternatives**: While OpenAI Embeddings is a solid choice, consider experimenting with different embedding models for your specific use case. AnswerAgentAI offers various embedding options that may perform better for certain tasks or domains.

3. **Model Selection**: The "text-embedding-ada-002" model is a good starting point, but be aware of newer models that OpenAI may release, as they could offer improved performance.

4. **Optimize Batch Size**: Adjust the batch size to balance between processing speed and API usage. Larger batch sizes can be more efficient but may increase latency.

5. **Monitor Token Usage**: Keep an eye on your OpenAI API token usage, especially when processing large volumes of text.

## Troubleshooting

1. **API Key Issues**: Ensure your OpenAI API key is correctly configured in your AnswerAgentAI credentials.

2. **Rate Limiting**: If you encounter rate limit errors, try reducing the batch size or implementing a retry mechanism with exponential backoff.

3. **Timeout Errors**: For large texts or slow connections, increase the timeout value to allow more time for the API to respond.

4. **Unsupported Models**: If you receive an error about an unsupported model, check the OpenAI API documentation for the latest available embedding models.

Remember, while OpenAI Embeddings is an excellent default choice, the best embedding model for your project may vary depending on your specific requirements, data characteristics, and performance needs. Don't hesitate to experiment with different embedding options available in AnswerAgentAI to find the optimal solution for your use case.
</file>

<file path="sidekick-studio/chatflows/embeddings/README.md">
---
description: LangChain Embedding Nodes in AnswerAgentAI
---

# Embeddings

## Overview

Embeddings are a crucial component in modern natural language processing and machine learning applications. They represent textual data as vectors of floating-point numbers, allowing for numerical analysis of semantic relationships between different pieces of text.

## Key Benefits

-   **Semantic Understanding**: Embeddings capture the meaning and context of text, enabling more nuanced analysis than simple keyword matching.
-   **Versatility**: They can be applied to a wide range of tasks, from search and recommendation systems to classification and anomaly detection.
-   **Efficiency**: Once text is converted to embeddings, many operations become computationally efficient vector calculations.

## How to Use

Embedding nodes in AnswerAgentAI are typically used as part of a larger workflow. Here's a general process for using embeddings:

1. Choose an appropriate embedding node based on your requirements and available APIs.
2. Configure the node with necessary credentials and parameters.
3. Connect the embedding node to your text input source.
4. Use the output embeddings for downstream tasks such as similarity comparison or clustering.

<!-- TODO: Add a screenshot of a simple workflow using an embedding node -->
<figure><img src="/.gitbook/assets/screenshots/uredis embedding cache in a workflow.png" alt="" /><figcaption><p> Simple WOrkflow Using an Embedding Node   &#x26; Drop UI</p></figcaption></figure>

## Applications of Embeddings

Embeddings are powerful tools that can be applied to various natural language processing tasks:

1. **Semantic Search**: Rank search results based on the similarity between query and document embeddings.
2. **Text Clustering**: Group similar texts by comparing their embedding vectors.
3. **Recommendations**: Suggest related items by finding texts with similar embeddings.
4. **Anomaly Detection**: Identify outliers by comparing embedding distances.
5. **Diversity Measurement**: Analyze the distribution of embeddings to assess text diversity.
6. **Classification**: Assign labels to text based on the similarity of their embeddings to known categories.

## Tips and Best Practices

1. **Choose the Right Model**: Different embedding models have various strengths. Choose one that aligns with your specific use case and language requirements.
2. **Normalize Embeddings**: For some applications, it's beneficial to normalize embedding vectors to unit length to focus on direction rather than magnitude.
3. **Dimensionality**: Higher-dimensional embeddings can capture more information but require more computational resources. Balance accuracy with efficiency.
4. **Fine-tuning**: Some embedding models allow for fine-tuning on domain-specific data, which can improve performance for specialized tasks.
5. **Caching**: If you're repeatedly embedding the same text, consider caching the results to save on API calls and computation time.

## Troubleshooting

-   **Inconsistent Results**: Ensure you're using the same model and parameters across your application for consistent embeddings.
-   **Poor Performance**: If embeddings aren't performing well for your task, try a different model or consider fine-tuning on domain-specific data.
-   **API Errors**: Check your API key and usage limits if you encounter errors when calling external embedding services.

## Available Embedding Nodes in AnswerAgentAI

AnswerAgentAI offers a variety of embedding nodes to suit different needs and integrate with various services:

-   [AWS Bedrock Embeddings](aws-bedrock-embeddings.md)
-   [Azure OpenAI Embeddings](azure-openai-embeddings.md)
-   [Cohere Embeddings](cohere-embeddings.md)
-   [Google GenerativeAI Embeddings](googlegenerativeai-embeddings.md)
-   [Google VertexAI Embeddings](googlevertexai-embeddings.md)
-   [HuggingFace Inference Embeddings](huggingface-inference-embeddings.md)
-   [LocalAI Embeddings](localai-embeddings.md)
-   [MistralAI Embeddings](mistralai-embeddings.md)
-   [OpenAI Embeddings](openai-embeddings.md)
-   [OpenAI Embeddings Custom](openai-embeddings-custom.md)
-   [TogetherAI Embedding](togetherai-embedding.md)
-   [VoyageAI Embeddings](voyageai-embeddings.md)

Each of these nodes provides unique features and capabilities. Refer to their individual documentation for specific usage instructions and best practices.

<!-- TODO: Add a comparison table of different embedding nodes with their key features -->

By leveraging these embedding nodes, you can enhance your AnswerAgentAI workflows with powerful text analysis and processing capabilities, opening up a wide range of applications in natural language understanding and generation.
</file>

<file path="sidekick-studio/chatflows/embeddings/togetherai-embedding.md">
---
description: Generate embeddings for text using TogetherAI models
---

# TogetherAI Embedding

## Overview

TogetherAI Embedding is a powerful feature in AnswerAgentAI that allows you to generate embeddings for a given text using TogetherAI's embedding models. Embeddings are numerical representations of text that capture semantic meaning, making them useful for various natural language processing tasks.

## Key Benefits

-   Access to state-of-the-art embedding models from TogetherAI
-   Easily integrate embeddings into your AnswerAgentAI workflows
-   Improve the performance of text-based tasks such as similarity search and text classification

## How to Use

To use the TogetherAI Embedding feature in AnswerAgentAI, follow these steps:

1. Add the TogetherAI Embedding node to your workflow.
2. Connect your TogetherAI API credential:

    - Click on the "Connect Credential" field.
    - Select an existing TogetherAI API credential or create a new one.
    - If creating a new credential, enter your TogetherAI API key.

3. Configure the embedding settings:

    - (Optional) Connect a Cache node to the "Cache" input if you want to cache embedding results.
    - Enter the desired model name in the "Model Name" field (e.g., "sentence-transformers/msmarco-bert-base-dot-v5").

4. Connect the TogetherAI Embedding node to other nodes in your workflow that require text embeddings.

<!-- TODO: Add a screenshot of the TogetherAI Embedding node configuration in the AnswerAgentAI interface -->
<figure><img src="/.gitbook/assets/screenshots/togetheraiembeddingnode.png" alt="" /><figcaption><p> TogetherAI Embedding Node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose the appropriate embedding model for your use case. Refer to the [TogetherAI embedding models documentation](https://docs.together.ai/docs/embedding-models) for a list of available models and their characteristics.

2. Use caching to improve performance, especially if you're generating embeddings for the same text multiple times.

3. Experiment with different embedding models to find the one that works best for your specific task.

4. Be mindful of the input text length limitations for the chosen embedding model.

## Troubleshooting

1. **Error: Invalid API key**

    - Make sure you've entered the correct TogetherAI API key in your credential settings.
    - Verify that your TogetherAI account is active and has the necessary permissions.

2. **Error: Model not found**

    - Double-check the model name you've entered in the "Model Name" field.
    - Ensure that the model name is spelled correctly and is available in the TogetherAI embedding models list.

3. **Slow performance**
    - Consider using a cache to store and reuse embedding results for frequently used text.
    - Check your internet connection, as embedding generation requires API calls to TogetherAI servers.

If you encounter any other issues or have questions about using TogetherAI Embedding in AnswerAgentAI, please refer to our support documentation or contact our customer support team for assistance.
</file>

<file path="sidekick-studio/chatflows/embeddings/voyageai-embeddings.md">
---
description: Generate embeddings using VoyageAI API
---

# VoyageAI Embeddings

## Overview

VoyageAI Embeddings is a powerful feature in AnswerAgentAI that allows you to generate embeddings for given text using the Voyage AI API. Embeddings are numerical representations of text that capture semantic meaning, making them useful for various natural language processing tasks.

## Key Benefits

-   Generate high-quality embeddings for text analysis and processing
-   Utilize state-of-the-art models from VoyageAI for improved accuracy
-   Easily integrate embeddings into your AnswerAgentAI workflows

## How to Use

To use VoyageAI Embeddings in AnswerAgentAI, follow these steps:

1. Add the VoyageAI Embeddings node to your workflow.
    <!-- TODO: Screenshot of adding VoyageAI Embeddings node to the workflow -->
    <figure><img src="/.gitbook/assets/screenshots/voyageaiembedding.png" alt="" /><figcaption><p> VoyageAI Embedding Node  &#x26; Drop UI</p></figcaption></figure>

2. Connect your VoyageAI API credential:

    - Click on the VoyageAI Embeddings node to open its settings.
    - In the "Connect Credential" field, select your VoyageAI API credential or create a new one if you haven't already.
      <!-- TODO: Screenshot of connecting VoyageAI API credential -->
      <figure><img src="/.gitbook/assets/screenshots/voyageaiapicredentials.png" alt="" /><figcaption><p> VoyageAI Embedding Node &#x26; Drop UI</p></figcaption></figure>

3. Choose the model:

    - In the "Model Name" dropdown, select the desired VoyageAI model for generating embeddings.
    - The default model is "voyage-2", but you can choose other available models.
      <!-- TODO: Screenshot of model selection dropdown -->
      <figure><img src="/.gitbook/assets/screenshots/voyagemodelselection.png" alt="" /><figcaption><p> VoyageAI Embedding Node Model Selection &#x26; Drop UI</p></figcaption></figure>

4. Connect the VoyageAI Embeddings node to other nodes in your workflow as needed.

5. Run your workflow to generate embeddings for your text input.

## Tips and Best Practices

1. Experiment with different models to find the one that works best for your specific use case.
2. Use the generated embeddings for tasks such as semantic search, text classification, or clustering.
3. Consider the length of your input text when choosing a model, as some models may have length limitations.
4. If you're working with a large volume of text, consider batching your requests to optimize performance.

## Troubleshooting

1. API Key Issues:

    - If you encounter authentication errors, double-check that your VoyageAI API key is correct and properly configured in your credentials.
    - Ensure that your API key has the necessary permissions to access the chosen model.

2. Model Availability:

    - If a specific model is not available in the dropdown, it may not be supported by the current version of AnswerAgentAI or your API access level. Try updating AnswerAgentAI or contacting VoyageAI support for more information.

3. Endpoint Errors:

    - If you're using a custom endpoint and experiencing connection issues, verify that the endpoint URL is correct and accessible from your network.

4. Input Text Limitations:
    - If you receive errors related to input text, check if your text exceeds the maximum length supported by the chosen model. Try breaking longer text into smaller chunks if necessary.

By following this guide, you should be able to effectively use VoyageAI Embeddings in your AnswerAgentAI workflows to generate high-quality text embeddings for various natural language processing tasks.
</file>

<file path="sidekick-studio/chatflows/memory/buffer-memory.md">
---
description: Buffer Memory for Quick Testing in AnswerAgentAI
---

# Buffer Memory

## Overview

Buffer Memory is a simple, built-in memory solution in AnswerAgentAI that allows for quick setup and testing of workflows without the need for external dependencies. It stores chat messages temporarily, making it ideal for development and testing purposes.

## Key Benefits

-   Quick and easy setup for testing workflows
-   No external dependencies required
-   Seamless integration with AnswerAgentAI's core functionality

## How to Use

1. In the AnswerAgentAI Studio, drag the "Buffer Memory" node onto your canvas.
2. Connect the Buffer Memory node to other nodes in your workflow.
3. Configure the node settings:
    - Session ID (optional): Specify a custom session ID or leave blank for a random ID.
    - Memory Key: Set the key used to store and retrieve chat history (default: "chat_history").

<!-- TODO: Screenshot of Buffer Memory node on the canvas with configuration panel open -->
<figure><img src="/.gitbook/assets/screenshots/buffermemorynode.png" alt="" /><figcaption><p> Buffer Memory Nodes &#x26; Drop UI</p></figcaption></figure>

buffermemorynode

## Tips and Best Practices

1. Use Buffer Memory for rapid prototyping and testing of your workflows.
2. For production environments, consider using more robust memory solutions like Redis or Upstash.
3. Clear the buffer memory regularly during testing to ensure a clean slate for each test run.

## Troubleshooting

-   If chat history is not persisting between sessions, ensure you're using the same Session ID.
-   If memory retrieval is slow, consider switching to a more scalable solution for larger datasets.

## Important Note

While Buffer Memory is convenient for quick testing, it is not recommended for production use. For production environments, we strongly advise using long-term memory solutions such as Redis or Upstash. These options provide better persistence, scalability, and reliability for handling user interactions in live applications.

## Transitioning to Production

When moving your AnswerAgentAI workflow from development to production:

1. Replace the Buffer Memory node with a production-ready memory solution (e.g., Redis Memory or Upstash Memory).
2. Update your workflow connections to use the new memory node.
3. Configure the production memory node with appropriate settings for your use case.
4. Test thoroughly to ensure smooth operation with the new memory solution.

<!-- TODO: Side-by-side comparison screenshot of Buffer Memory vs. Redis/Upstash Memory node configuration -->
<figure><img src="/.gitbook/assets/screenshots/buffervsupstashmemory.png" alt="" /><figcaption><p> Buffer Memory Nodes vs Redis/Upstash Memory node configuration &#x26; Drop UI</p></figcaption></figure>

By following these guidelines, you can leverage Buffer Memory for efficient development and testing, while ensuring robust and scalable memory management in your production AnswerAgentAI applications.
</file>

<file path="sidekick-studio/chatflows/memory/buffer-window-memory.md">
---
description: Buffer Window Memory for AnswerAgentAI
---

# Buffer Window Memory

## Overview

The Buffer Window Memory node is a memory component in AnswerAgentAI that stores and retrieves a fixed number of recent conversation turns. It uses a window of size k to surface the last k back-and-forth exchanges to use as memory for the AI model.

## Key Benefits

-   Maintains context by keeping recent conversation history
-   Customizable memory size to balance between context and efficiency
-   Improves the coherence and relevance of AI responses

## How to Use

1. Add the Buffer Window Memory node to your AnswerAgentAI workflow canvas.
2. Configure the node settings:
    - Set the "Size" parameter to determine how many conversation turns to remember (default is 4).
    - Optionally, specify a "Session ID" for managing multiple conversations.
    - Customize the "Memory Key" if needed (default is "chat_history").
3. Connect the Buffer Window Memory node to other nodes in your workflow that require conversation history.

<!-- TODO: Add a screenshot of the Buffer Window Memory node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/bufferwindowmemory.png" alt="" /><figcaption><p> Buffer Window Memory Nodes &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Choose an appropriate window size:
    -   Larger sizes provide more context but may slow down processing.
    -   Smaller sizes are more efficient but may miss important earlier context.
-   Use unique session IDs for different conversations to keep memories separate.
-   The memory automatically stores both user inputs and AI responses.

## Troubleshooting

-   If the AI seems to be forgetting important information:
    -   Increase the window size to retain more conversation history.
    -   Check if the correct session ID is being used.
-   If the workflow is running slowly:
    -   Try reducing the window size to improve performance.
    -   Ensure you're not storing unnecessary information in the memory.

## Advanced Usage

The Buffer Window Memory node in AnswerAgentAI is built on top of Langchain's BufferWindowMemory class and includes additional functionality for database integration. Here are some advanced features:

-   Database Integration: The node can store and retrieve conversation history from a database, allowing for persistent memory across sessions.
-   Flexible Retrieval: You can retrieve chat messages as either simple message objects or as BaseMessage instances for more advanced processing.
-   Prepend Messages: The node allows you to prepend additional messages to the retrieved history, useful for adding context or system messages.

<!-- TODO: Add a diagram showing how Buffer Window Memory integrates with other components in a typical AnswerAgentAI workflow -->
<figure><img src="/.gitbook/assets/screenshots/bufferwindowmemoryinaworkflow.png" alt="" /><figcaption><p> Buffer Window Memory Node in a workflow&#x26; Drop UI</p></figcaption></figure>

Remember that while this node provides powerful memory capabilities, it's important to use it responsibly and in compliance with data privacy regulations, especially when storing user conversations.
</file>

<file path="sidekick-studio/chatflows/memory/conversation-summary-buffer-memory.md">
---
description: Conversation Summary Buffer Memory in AnswerAgentAI
---

# Conversation Summary Buffer Memory

## Overview

The Conversation Summary Buffer Memory is a powerful feature in AnswerAgentAI that uses token length to decide when to summarize conversations. This memory type helps manage long conversations efficiently by summarizing older parts of the conversation when a token limit is reached.

## Key Benefits

-   Maintains context for long conversations without exceeding token limits
-   Automatically summarizes older parts of the conversation
-   Allows customization of token limits and memory settings

## How to Use

1. Add the Conversation Summary Buffer Memory node to your AnswerAgentAI workflow canvas.
2. Configure the node with the following settings:

    a. **Chat Model**: Select the language model to use for summarization.

    b. **Max Token Limit**: Set the maximum number of tokens before summarization occurs (default is 2000).

    c. **Session ID** (optional): Specify a unique identifier for the conversation. If not provided, a random ID will be generated.

    d. **Memory Key** (optional): Set the key used to store the chat history (default is 'chat_history').

3. Connect the memory node to other nodes in your workflow that require conversation history.

<!-- TODO: Add a screenshot of the Conversation Summary Buffer Memory node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/conversationsummarybuffermemory.png" alt="" /><figcaption><p> Conversation Summary Buffer Memory Node in a workflow&#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose an appropriate Max Token Limit based on your use case and the complexity of your conversations.
2. Use a consistent Session ID for related conversations to maintain context across multiple interactions.
3. Experiment with different Chat Models to find the best balance between summarization quality and performance.
4. Monitor the summarized content to ensure important information is not lost during the summarization process.

## Troubleshooting

1. **Issue**: Conversation context seems to be lost unexpectedly.
   **Solution**: Check if the Max Token Limit is set too low. Increase the limit to allow for more context retention.

2. **Issue**: Summarization is not occurring as expected.
   **Solution**: Verify that the Chat Model is properly connected and functioning. Ensure that the conversation length is actually reaching the Max Token Limit.

3. **Issue**: Memory is not persisting across sessions.
   **Solution**: Make sure you're using a consistent Session ID for related conversations. If the issue persists, check your database configuration and connections.

Remember to handle the memory operations carefully, especially when dealing with sensitive conversation data.
</file>

<file path="sidekick-studio/chatflows/memory/conversation-summary-memory.md">
---
description: Summarizes the conversation and stores the current summary in memory
---

# Conversation Summary Memory

## Overview

The Conversation Summary Memory node is a powerful feature in AnswerAgentAI that summarizes the conversation and stores the current summary in memory. This node is particularly useful for maintaining context in long conversations or when dealing with large amounts of information.

## Key Benefits

-   Maintains context across long conversations
-   Reduces memory usage by storing summaries instead of full conversation history
-   Improves the relevance of AI responses by providing a concise context

## How to Use

1. Drag the Conversation Summary Memory node onto your canvas in the AnswerAgentAI Studio.
2. Connect the node to your chat model and other relevant nodes in your workflow.
3. Configure the node settings:
    - **Chat Model**: Select the chat model to use for summarization.
    - **Session ID** (optional): Specify a unique identifier for the conversation. If not provided, a random ID will be used.
    - **Memory Key**: Set the key used to store the summary in memory (default is 'chat_history').

<!-- TODO: Add a screenshot showing the Conversation Summary Memory node on the canvas with its configuration panel open -->
<figure><img src="/.gitbook/assets/screenshots/conversationsummarybuffermemory.pngconfiguration.png" alt="" /><figcaption><p> Conversation Summary Buffer Memory Configuration &#x26; Drop UI</p></figcaption></figure>

4. Run your workflow to start using the Conversation Summary Memory.

## Tips and Best Practices

1. Use this node when dealing with long conversations or complex topics that require maintaining context.
2. Experiment with different chat models to find the one that provides the most accurate and concise summaries.
3. If you're building a multi-user application, make sure to use unique session IDs for each conversation to prevent cross-talk between different users' contexts.
4. Consider using this node in combination with other memory types for more sophisticated context management.

## Troubleshooting

1. **Summaries are too short or missing important details**: Try using a more capable chat model or adjust the model's parameters to generate more detailed summaries.
2. **High latency in responses**: If you notice increased response times, it might be due to the summarization process. Consider using this node less frequently or optimizing your workflow.
3. **Inconsistent context**: Ensure that you're using the correct session ID for each conversation to maintain consistent context.
</file>

<file path="sidekick-studio/chatflows/memory/dynamodb-chat-memory.md">
---
description: Store conversation history in Amazon DynamoDB
---

# DynamoDB Chat Memory

## Overview

The DynamoDB Chat Memory feature allows you to store and retrieve conversation history using Amazon DynamoDB. This powerful integration enables long-term persistence of chat messages, making it ideal for applications that require durable and scalable conversation storage.

## Key Benefits

-   **Persistent Storage**: Safely store conversation history in a reliable and scalable AWS service.
-   **Seamless Integration**: Easily incorporate long-term memory into your AnswerAgentAI workflows.
-   **Flexible Configuration**: Customize settings to match your specific DynamoDB setup and requirements.

## How to Use

1. Add the "DynamoDB Chat Memory" node to your AnswerAgentAI canvas.
2. Configure the node with the following required parameters:

    - Table Name: The name of your DynamoDB table
    - Partition Key: The primary key for your table
    - Region: The AWS region where your table is located (e.g., "us-east-1")

3. (Optional) Set additional parameters:

    - Session ID: A unique identifier for the conversation (if not specified, a random ID will be generated)
    - Memory Key: The key used to store the chat history (default is "chat_history")

4. Connect your AWS credentials:

    - Click on the "Connect Credential" option
    - Select or create a credential of type "dynamodbMemoryApi"
    - Provide your AWS Access Key ID and Secret Access Key

5. Connect the DynamoDB Chat Memory node to other nodes in your workflow that require access to conversation history.

<!-- TODO: Screenshot of the DynamoDB Chat Memory node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/dynamochatmemory.png" alt="" /><figcaption><p> Dynamo Chat Memory Node Configuration &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure your DynamoDB table is set up with the correct partition key before using this feature.
2. Use a consistent session ID across interactions to maintain continuity in long-running conversations.
3. Implement proper security measures to protect your AWS credentials and access to the DynamoDB table.
4. Monitor your DynamoDB usage to optimize cost and performance.

## Troubleshooting

1. **Connection Issues**:

    - Verify that your AWS credentials are correct and have the necessary permissions to access the DynamoDB table.
    - Check if the specified AWS region matches the location of your DynamoDB table.

2. **Data Not Persisting**:

    - Ensure the table name and partition key are correctly specified.
    - Verify that the session ID is consistent across interactions if you're trying to retrieve previous conversation history.

3. **Performance Concerns**:
    - If you experience slow response times, consider optimizing your DynamoDB table's read and write capacity units.

For more advanced usage and integration details, refer to the AnswerAgentAI API documentation on DynamoDB Chat Memory implementation.

<!-- TODO: Screenshot showing a successful integration of DynamoDB Chat Memory in an AnswerAgentAI workflow -->
<figure><img src="/.gitbook/assets/screenshots/dynamochatmemoryinaworkflow.png" alt="" /><figcaption><p> Dynamo Chat Memory Node Configuration In a Workflow &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/memory/mongodb-atlas-chat-memory.md">
---
description: Store conversation history in MongoDB Atlas for AnswerAgentAI workflows
---

# MongoDB Atlas Chat Memory

## Overview

The MongoDB Atlas Chat Memory node allows you to store conversation history in a MongoDB Atlas database. This feature enables long-term memory storage for your AnswerAgentAI workflows, allowing chatbots and AI assistants to maintain context across multiple sessions or interactions.

## Key Benefits

-   Persistent storage of conversation history in a scalable, cloud-based database
-   Ability to maintain context across multiple user sessions
-   Flexible configuration options for database and collection management

## How to Use

1. Add the MongoDB Atlas Chat Memory node to your AnswerAgentAI workflow canvas.
2. Configure the node with the following settings:

    a. Connect Credential: Select or add your MongoDB Atlas connection credentials.
    b. Database: Enter the name of the database you want to use.
    c. Collection Name: Specify the name of the collection to store conversation history.
    d. Session Id (optional): Enter a unique identifier for the conversation session.
    e. Memory Key (optional): Set a custom key for storing the chat history (default is 'chat_history').

3. Connect the MongoDB Atlas Chat Memory node to other nodes in your workflow that require access to conversation history.

<!-- TODO: Add a screenshot of the MongoDB Atlas Chat Memory node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/monogdbchatmemory.png" alt="" /><figcaption><p> MonoDB Atlas Chat Memory Node Configuration In a Workflow &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use unique session IDs for different users or conversation threads to keep histories separate.
2. Implement proper security measures to protect sensitive conversation data stored in MongoDB Atlas.
3. Regularly monitor and manage your MongoDB Atlas database to ensure optimal performance and storage utilization.
4. Consider implementing a data retention policy to manage the growth of your conversation history database.

## Troubleshooting

1. Connection issues:

    - Verify that your MongoDB Atlas connection URL is correct and that your IP address is whitelisted in the Atlas dashboard.
    - Ensure that your MongoDB Atlas cluster is running and accessible.

2. Data not being stored or retrieved:

    - Double-check that the database name and collection name are correctly specified in the node configuration.
    - Verify that the session ID is being properly set and managed in your workflow.

3. Performance concerns:
    - If you experience slow response times, consider indexing frequently queried fields in your MongoDB collection.
    - For large-scale applications, implement database sharding to distribute data across multiple servers.

<!-- TODO: Add a screenshot showing where to find logs or error messages related to the MongoDB Atlas Chat Memory node -->

By utilizing the MongoDB Atlas Chat Memory node, you can enhance your AnswerAgentAI workflows with robust, scalable, and persistent conversation storage, enabling more context-aware and personalized AI interactions.
</file>

<file path="sidekick-studio/chatflows/memory/README.md">
---
description: LangChain Memory Nodes
---

# Memory Nodes

Memory nodes in AnswerAgentAI allow you to create chatbots with persistent memory, enabling more natural and context-aware conversations. This feature simulates human-like memory, allowing the AI to recall information from previous interactions within the same conversation.

## Overview

Memory nodes store and retrieve conversation history, providing context to the AI model. This enables the AI to maintain coherent, contextually relevant dialogues across multiple exchanges.

<div style={{ backgroundColor: 'blue', color: 'white', padding: '5px', borderRadius: '5px', marginBottom: '10px' }}>
  Human: hi i am bob
</div>

<div style={{ backgroundColor: 'green', color: 'white', padding: '5px', borderRadius: '5px', marginBottom: '10px' }}>
  AI: Hello Bob! It's nice to meet you. How can I assist you today?
</div>

<div style={{ backgroundColor: 'blue', color: 'white', padding: '5px', borderRadius: '5px', marginBottom: '10px' }}>
  Human: what's my name?
</div>

<div style={{ backgroundColor: 'green', color: 'white', padding: '5px', borderRadius: '5px', marginBottom: '10px' }}>
  AI: Your name is Bob, as you mentioned earlier.
</div>

Under the hood, these conversations are stored in arrays or databases, and provided as context to prompts. For example:

```
You are an assistant to a human, powered by a large language model trained by OpenAI.

Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.

Current conversation:
{chat_history}
```

## Key Benefits

-   Improved conversation continuity and context awareness
-   More natural and human-like interactions
-   Ability to recall and reference information from earlier in the conversation

## How to Use

1. Select a memory node from the available options in the AnswerAgentAI canvas.
2. Connect the memory node to your conversation flow.
3. Configure the memory node settings as needed (e.g., memory size, storage method).
4. Test your chatbot to ensure it correctly remembers and uses information from previous exchanges.

<!-- TODO: Screenshot of the AnswerAgentAI canvas showing where to find and how to connect a memory node -->
<figure><img src="/.gitbook/assets/screenshots/memorynodes.png" alt="" /><figcaption><p> Memory Nodes &#x26; Drop UI</p></figcaption></figure>

### Memory Nodes

-   [Buffer Memory](buffer-memory.md)
-   [Buffer Window Memory](buffer-window-memory.md)
-   [Conversation Summary Memory](conversation-summary-memory.md)
-   [Conversation Summary Buffer Memory](conversation-summary-buffer-memory.md)
-   [DynamoDB Chat Memory](dynamodb-chat-memory.md)
-   [MongoDB Atlas Chat Memory](mongodb-atlas-chat-memory.md)
-   [Redis-Backed Chat Memory](redis-backed-chat-memory.md)
-   [Upstash Redis-Backed Chat Memory](upstash-redis-backed-chat-memory.md)
-   [Zep Memory](zep-memory.md)

Each memory node type has its own configuration options and is suitable for different scenarios. Refer to the individual documentation for each memory node type for more detailed information.

## Tips and Best Practices

1. Choose the appropriate memory node based on your specific use case and requirements.
2. Consider the trade-off between memory size and performance when configuring your memory nodes.
3. Regularly test your chatbot to ensure it's using the stored memory effectively and accurately.
4. Use memory nodes in combination with other AnswerAgentAI features for more sophisticated conversational AI experiences.

## Handling Multiple Users

AnswerAgentAI provides methods to maintain separate conversation histories for multiple users:

### UI & Embedded Chat

For UI and Embedded Chat implementations, AnswerAgentAI automatically separates conversations for different users by generating a unique `chatId` for each new interaction.

### Prediction API

To separate conversations for multiple users when using the Prediction API:

1. Locate the "Session ID" input parameter in your chosen memory node.

<!-- TODO: Screenshot showing the "Session ID" input parameter in a memory node configuration -->

2. In your POST request to `/api/v1/prediction/{your-chatflowid}`, include a `sessionId` in the `overrideConfig` object:

```json
{
    "question": "Hello!",
    "overrideConfig": {
        "sessionId": "user1"
    }
}
```

### Message API

Use the Message API to retrieve or delete chat messages:

-   GET `/api/v1/chatmessage/{your-chatflowid}`
-   DELETE `/api/v1/chatmessage/{your-chatflowid}`

Query parameters:

| Parameter | Type   | Description                    |
| --------- | ------ | ------------------------------ |
| sessionId | string | Unique identifier for the user |
| sort      | enum   | "ASC" or "DESC"                |
| startDate | string | Start date for message range   |
| endDate   | string | End date for message range     |

## Conversation Management

AnswerAgentAI provides a user interface for visualizing and managing conversations:

<!-- TODO: Screenshot of the AnswerAgentAI conversation management interface -->

This interface allows you to review, analyze, and manage conversation histories for different users or sessions.

## Troubleshooting

1. If the AI isn't recalling information correctly, check that your memory node is properly connected and configured.
2. Ensure you're using the correct `sessionId` when working with multiple users.
3. If you're experiencing performance issues, consider adjusting the memory size or using a different type of memory node.
4. For persistent storage issues, verify the connection settings for database-backed memory nodes (e.g., DynamoDB, MongoDB, Redis).

By effectively using memory nodes in AnswerAgentAI, you can create more engaging, context-aware conversational AI experiences that better simulate human-like interactions.
</file>

<file path="sidekick-studio/chatflows/memory/redis-backed-chat-memory.md">
---
description: Redis-Backed Chat Memory for AnswerAgentAI
---

# Redis-Backed Chat Memory

## Overview

The Redis-Backed Chat Memory is a powerful feature in AnswerAgentAI that allows you to store and retrieve conversation history using a Redis server. This memory node summarizes conversations and provides long-term storage, enabling more context-aware and personalized interactions in your AI applications.

## Key Benefits

-   **Persistent Memory**: Store conversation history securely in a Redis server for long-term retention.
-   **Scalable**: Efficiently handle large volumes of conversation data across multiple sessions.
-   **Customizable**: Configure session timeouts, memory keys, and window sizes to suit your specific needs.

## How to Use

1. Add the Redis-Backed Chat Memory node to your AnswerAgentAI workflow canvas.
2. Configure the node settings:
    - Connect your Redis credential (optional)
    - Set a Session ID (optional)
    - Configure Session Timeouts (optional)
    - Specify a Memory Key
    - Set the Window Size (optional)
3. Connect the memory node to other nodes in your workflow that require conversation history.

<!-- TODO: Screenshot of the Redis-Backed Chat Memory node on the AnswerAgentAI canvas -->
<figure><img src="/.gitbook/assets/screenshots/redischatmemory.png" alt="" /><figcaption><p> RedicBacked Chat Memory Node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. **Session Management**: Use unique session IDs for different conversations or users to keep memories separate.
2. **Memory Key**: Choose a descriptive memory key to easily identify the purpose of the stored data.
3. **Window Size**: Adjust the window size to balance between providing sufficient context and managing memory usage.
4. **Security**: Ensure your Redis server is properly secured, especially when storing sensitive conversation data.

## Troubleshooting

1. **Connection Issues**:

    - Verify that your Redis server is running and accessible.
    - Double-check the connection credentials in the AnswerAgentAI settings.

2. **Memory Not Persisting**:

    - Ensure that the Session ID is correctly set and consistent across interactions.
    - Check if the Session Timeout is set appropriately for your use case.

3. **Performance Concerns**:
    - If you experience slow responses, try reducing the Window Size to limit the amount of history being processed.

## Configuration Options

### Session ID

-   **Description**: A unique identifier for the conversation session.
-   **Default**: If not specified, a random ID will be generated.
-   **Usage**: Use consistent session IDs to maintain conversation continuity across interactions.

### Session Timeouts

-   **Description**: The duration (in seconds) after which a session expires.
-   **Default**: Omit this parameter to make sessions never expire.
-   **Usage**: Set an appropriate timeout to manage server resources and clear old conversations.

### Memory Key

-   **Description**: A key used to identify and retrieve the stored chat history.
-   **Default**: 'chat_history'
-   **Usage**: Use descriptive keys to organize different types of conversation data.

### Window Size

-   **Description**: The number of recent messages to include in the conversation history.
-   **Default**: Not set (includes all messages)
-   **Usage**: Adjust this value to balance between context depth and processing efficiency.

<!-- TODO: Screenshot of the configuration panel for the Redis-Backed Chat Memory node -->
<figure><img src="/.gitbook/assets/screenshots/redismemoryconfiguration.png" alt="" /><figcaption><p> Redis-Backed Chat Memory Node Configuration &#x26; Drop UI</p></figcaption></figure>

By leveraging the Redis-Backed Chat Memory in AnswerAgentAI, you can create more intelligent and context-aware conversational AI applications that maintain user interactions over extended periods.
</file>

<file path="sidekick-studio/chatflows/memory/upstash-redis-backed-chat-memory.md">
---
description: Summarizes the conversation and stores the memory in Upstash Redis server
---

# Upstash Redis-Backed Chat Memory

## Overview

The Upstash Redis-Backed Chat Memory is a powerful feature in AnswerAgentAI that allows you to store and retrieve conversation history using Upstash Redis. This memory solution provides persistent storage for chat messages, enabling long-term memory capabilities for your AI applications.

## Key Benefits

-   **Persistent Storage**: Safely store conversation history in Upstash Redis for long-term retention.
-   **Scalable Solution**: Leverage Upstash Redis's scalability to handle large volumes of chat data.
-   **Flexible Configuration**: Customize session management and memory retrieval to suit your specific needs.

## How to Use

1. Add the Upstash Redis-Backed Chat Memory node to your AnswerAgentAI workflow canvas.

<!-- TODO: Screenshot of adding the Upstash Redis-Backed Chat Memory node to the canvas -->
<figure><img src="/.gitbook/assets/screenshots/upstashmemorynode.png" alt="" /><figcaption><p> Upstash Redis-Backed Chat Memory Node Configuration &#x26; Drop UI</p></figcaption></figure>

2. Configure the node with the following parameters:

    - **Upstash Redis REST URL**: Enter your Upstash Redis instance URL (e.g., `https://<your-url>.upstash.io`).
    - **Session Id**: (Optional) Specify a unique identifier for the chat session. If not provided, a random ID will be generated.
    - **Session Timeouts**: (Optional) Set the time-to-live (TTL) for chat sessions in seconds. Omit this to make sessions never expire.
    - **Memory Key**: Set the key used to store chat history (default is `chat_history`).

3. Connect your Upstash Redis credential:
    - Click on the "Connect Credential" option.
    - Select or create an "Upstash Redis Memory API" credential with your Upstash REST token.

<!-- TODO: Screenshot of the credential configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/upstashapcredentials.png" alt="" /><figcaption><p> Upstash Redis Memory API credential &#x26; Drop UI</p></figcaption></figure>

4. Connect the Upstash Redis-Backed Chat Memory node to other nodes in your workflow that require access to chat history.

## Tips and Best Practices

1. **Session Management**: Use unique session IDs for different conversations or users to keep chat histories separate.
2. **Security**: Always use credentials to secure your Upstash Redis connection. Never expose your Upstash REST token in client-side code.
3. **Performance**: Consider setting appropriate session timeouts to manage memory usage and storage costs.
4. **Scalability**: Upstash Redis is designed to handle high loads, making it suitable for applications with many concurrent users.

## Troubleshooting

1. **Connection Issues**:

    - Ensure your Upstash Redis REST URL is correct and accessible.
    - Verify that your Upstash REST token is valid and has the necessary permissions.

2. **Missing Chat History**:

    - Check if the correct session ID is being used across interactions.
    - Verify that the session hasn't expired if you've set a session timeout.

3. **Performance Concerns**:
    - If experiencing slow responses, consider optimizing your Redis usage or upgrading your Upstash plan.

## API Reference

For advanced usage and custom implementations, refer to the Upstash Redis client API documentation:

```javascript
import { Redis } from '@upstash/redis'

// Initialize Redis client
const client = new Redis({
    url: 'https://<your-url>.upstash.io',
    token: 'your-upstash-rest-token'
})

// Example: Storing a message
await client.lpush('sessionId', JSON.stringify(messageObject))

// Example: Retrieving messages
const messages = await client.lrange('sessionId', 0, -1)
```

For more detailed information on available methods and options, consult the [@upstash/redis npm package documentation](https://www.npmjs.com/package/@upstash/redis).
</file>

<file path="sidekick-studio/chatflows/memory/zep-memory.md">
---
description: Summarizes the conversation and stores the memory in Zep server
---

# Zep Memory

## Feature Title: Zep Memory - Open Source

## Overview

Zep Memory is a powerful long-term memory store for LLM applications in AnswerAgentAI. It stores, summarizes, embeds, indexes, and enriches chatbot histories, making them accessible through simple, low-latency APIs. This feature allows your AnswerAgentAI workflows to maintain context and remember previous conversations efficiently.

## Key Benefits

-   Long-term memory storage for chatbots and LLM applications
-   Efficient summarization and indexing of conversation histories
-   Easy integration with AnswerAgentAI workflows

## How to Use (Zep Cloud Node)

1. Add the Zep Memory - Cloud node to your AnswerAgentAI workflow canvas.
2. Configure the node settings:
    - Connect your Zep Memory API credential (optional)
    - Set a Session ID (optional)
    - Choose the Memory Type (perpetual or message_window)
    - Customize prefixes and keys as needed

<!-- TODO: Add a screenshot of the Zep Memory - Cloud node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/zepmemory.png" alt="" /><figcaption><p> Zep Memory Cloud Node Configuration Panel &#x26; Drop UI</p></figcaption></figure>

3. Connect the Zep Memory - Cloud node to your conversation flow.
4. Run your workflow to utilize Zep's cloud-based memory storage.

## Tips and Best Practices

1. Use a consistent Session ID for related conversations to maintain context over time.
2. Choose the appropriate Memory Type based on your use case:
    - "perpetual" for ongoing, long-term conversations
    - "message_window" for conversations with a limited context window
3. Customize the AI and Human prefixes to match your conversation style.
4. Utilize the Memory Key, Input Key, and Output Key to organize your data effectively.

## Troubleshooting

1. **Authentication Issues**: Ensure your Zep Memory API credential is correctly configured in AnswerAgentAI.
2. **Missing Context**: Verify that the Session ID is consistent across related conversations.
3. **Unexpected Behavior**: Double-check the Memory Type setting to ensure it aligns with your intended use case.

### Advanced Configuration

The Zep Memory - Cloud node offers several advanced configuration options:

-   **Session ID**: A unique identifier for the conversation. If not specified, a random ID will be generated.
-   **Memory Type**: Choose between "perpetual" (default) or "message_window" to control how memory is managed.
-   **AI Prefix**: The prefix used to identify AI-generated messages (default: "ai").
-   **Human Prefix**: The prefix used to identify human-generated messages (default: "human").
-   **Memory Key**: The key used to store and retrieve memory data (default: "chat_history").
-   **Input Key**: The key used for input data (default: "input").
-   **Output Key**: The key used for output data (default: "text").

<!-- TODO: Add a screenshot highlighting the advanced configuration options -->
<figure><img src="/.gitbook/assets/screenshots/zepmemoryadvanced.png" alt="" /><figcaption><p> Zep Memory Cloud Node Configuration Panel &#x26; Drop UI</p></figcaption></figure>

By leveraging these options, you can fine-tune the Zep Memory - Cloud node to best suit your specific use case and integration requirements.

## How to Use (Custom Server)

1. Deploy a Zep server (see deployment guides below)
2. In your AnswerAgentAI canvas, add the Zep Memory node to your workflow
3. Configure the Zep Memory node with your Zep server's base URL
4. Connect the Zep Memory node to your conversation flow
5. Save and run your workflow

### Configuring the Zep Memory Node

1. Base URL: Enter the URL of your deployed Zep server (e.g., `http://127.0.0.1:8000`)
2. Session ID: (Optional) Specify a custom session ID or leave blank for a random ID
3. Size: Set the number of recent messages to use as context (default: 10)
4. AI Prefix: Set the prefix for AI messages (default: 'ai')
5. Human Prefix: Set the prefix for human messages (default: 'human')
6. Memory Key: Set the key for storing memory (default: 'chat_history')
7. Input Key: Set the key for input values (default: 'input')
8. Output Key: Set the key for output values (default: 'text')

## Tips and Best Practices

1. Use a consistent Session ID for related conversations to maintain context across multiple interactions
2. Adjust the Size parameter based on your application's needs for balancing context and performance
3. Regularly monitor your Zep server's performance and scale as needed
4. Implement proper security measures, including JWT authentication, for production deployments

## Troubleshooting

1. Connection issues: Ensure your Zep server is running and accessible from your AnswerAgentAI instance
2. Memory not persisting: Verify that the Session ID is consistent across interactions
3. Slow performance: Consider adjusting the Size parameter or scaling your Zep server

## Deployment Guides (Custom Server)

### Deploying Zep to Render

1. Go to the [Zep GitHub repository](https://github.com/getzep/zep#quick-start)
2. Click the "Deploy to Render" button
3. On Render's Blueprint page, click "Create New Resources"
4. Wait for the deployment to complete
5. Copy the deployed URL from the "zep" application in your Render dashboard

<!-- TODO: Screenshot of Render dashboard showing the deployed Zep application -->

### Deploying Zep to Digital Ocean (via Docker)

1. Clone the Zep repository:

    ```
    git clone https://github.com/getzep/zep.git
    cd zep
    ```

2. Create and edit the `.env` file:

    ```
    nano .env
    ```

3. Add your OpenAI API Key to the `.env` file:

    ```
    ZEP_OPENAI_API_KEY=your_api_key_here
    ```

4. Build and run the Docker container:

    ```
    docker compose up -d --build
    ```

5. Allow firewall access to port 8000:

    ```
    sudo ufw allow from any to any port 8000 proto tcp
    ufw status numbered
    ```

    Note: If using Digital Ocean's dashboard firewall, ensure port 8000 is added there as well.

## Zep Authentication

To secure your Zep instance using JWT authentication:

1. Download the `zepcli` utility from the [releases page](https://github.com/getzep/zepcli/releases)

2. Generate a secret and JWT token:

    - On Linux or macOS: `./zepcli -i`
    - On Windows: `zepcli.exe -i`

3. Configure auth environment variables on your Zep server:

    ```
    ZEP_AUTH_REQUIRED=true
    ZEP_AUTH_SECRET=<the secret you generated>
    ```

4. In AnswerAgentAI, create a new credential for Zep:

    - Add a new credential
    - Enter the JWT Token in the API Key field

5. Use the created credential in the Zep Memory node:
    - Select the credential in the "Connect Credential" field of the Zep Memory node

<!-- TODO: Screenshot of the Zep Memory node configuration with the credential selected -->
<figure><img src="/.gitbook/assets/screenshots/zepmemoryapi.png" alt="" /><figcaption><p> Zep Memory Node Credential &#x26; Drop UI</p></figcaption></figure>

By following these steps, you'll have a secure, authenticated connection between your AnswerAgentAI workflow and your Zep memory server.
</file>

<file path="sidekick-studio/chatflows/moderation/openai-moderation.md">
---
description: Check whether content complies with OpenAI usage policies
---

# OpenAI Moderation

## Overview

The OpenAI Moderation node is a powerful tool in AnswerAgentAI that allows you to check whether content complies with OpenAI's usage policies. This feature is essential for maintaining ethical AI practices and ensuring that your generated content adheres to OpenAI's guidelines.

## Key Benefits

-   Ensures compliance with OpenAI's content policies
-   Helps maintain ethical AI practices in your workflows
-   Prevents potential misuse of AI-generated content

## How to Use

1. Add the OpenAI Moderation node to your canvas in the AnswerAgentAI Studio.

<!-- TODO: Screenshot of adding the OpenAI Moderation node to the canvas -->
<figure><img src="/.gitbook/assets/screenshots/openaimoderation.png" alt="" /><figcaption><p> OpenAI Moderation Node &#x26; Drop UI</p></figcaption></figure>

2. Connect your OpenAI API credential:
    - Click on the node to open its settings.
    - In the "Connect Credential" field, select your OpenAI API credential or create a new one if you haven't already.

<!-- TODO: Screenshot of connecting the OpenAI API credential -->
<figure><img src="/.gitbook/assets/screenshots/openaimoderationcredential.png" alt="" /><figcaption><p> OpenAI Credentials &#x26; Drop UI</p></figcaption></figure>

3. (Optional) Customize the error message:
    - In the node settings, locate the "Error Message" field.
    - Enter a custom error message that will be displayed if the content violates OpenAI's moderation policies.
    - If left blank, the default message will be used: "Cannot Process! Input violates OpenAI's content moderation policies."

<!-- TODO: Screenshot of customizing the error message -->
<figure><img src="/.gitbook/assets/screenshots/openaimoderationerrormsg.png" alt="" /><figcaption><p> OpenAI Customize Error Msg &#x26; Drop UI</p></figcaption></figure>

4. Connect the OpenAI Moderation node to other nodes in your workflow:
    - Connect the input of the OpenAI Moderation node to the node that generates or processes the content you want to moderate.
    - Connect the output of the OpenAI Moderation node to the next step in your workflow.

<!-- TODO: Screenshot of connecting the OpenAI Moderation node in a workflow -->
<figure><img src="/.gitbook/assets/screenshots/opoenaimoderationinaworkflow.png" alt="" /><figcaption><p> OpenAI Moderation Node in a Workflow &#x26; Drop UI</p></figcaption></figure>

5. Run your workflow to see the OpenAI Moderation node in action.

## Tips and Best Practices

1. Always use the OpenAI Moderation node before generating or processing content with OpenAI's models to ensure compliance.
2. Customize the error message to provide clear instructions to end-users when content is flagged.
3. Consider adding additional nodes after the OpenAI Moderation node to handle flagged content appropriately (e.g., logging, notifying administrators, or providing alternative responses).
4. Regularly review OpenAI's content policies to stay up-to-date with any changes that may affect your moderation settings.

## Troubleshooting

1. **Issue**: The OpenAI Moderation node is not working.
   **Solution**: Ensure that you have provided a valid OpenAI API key in the credential settings.

2. **Issue**: Content is being flagged incorrectly.
   **Solution**: Review OpenAI's content policies and adjust your input content accordingly. If you believe there's an error in the moderation, you can report it to OpenAI for review.

3. **Issue**: The custom error message is not displaying.
   **Solution**: Make sure you have entered the custom message correctly in the "Error Message" field and that there are no formatting issues.

By using the OpenAI Moderation node in your AnswerAgentAI workflows, you can ensure that your AI-generated content remains compliant with OpenAI's usage policies, promoting responsible and ethical AI practices.
</file>

<file path="sidekick-studio/chatflows/moderation/README.md">
---
description: AnswerAgentAI Moderation Nodes
---

# Moderation Nodes

Moderation nodes play a crucial role in maintaining the integrity and safety of your AnswerAgentAI workflows. They analyze text content to identify and flag potentially problematic material, helping to create a safer and more controlled environment for your applications.

## Key Benefits

-   Enhance content safety by automatically filtering inappropriate or harmful text
-   Protect users and maintain platform integrity
-   Customize moderation levels to suit your specific use case

## Available Moderation Nodes

AnswerAgentAI offers two types of moderation nodes:

-   [OpenAI Moderation](openai-moderation.md)
-   [Simple Prompt Moderation](simple-prompt-moderation.md)
</file>

<file path="sidekick-studio/chatflows/moderation/simple-prompt-moderation.md">
---
description: Check and prevent sensitive input from being sent to the language model
---

# Simple Prompt Moderation

## Overview

The Simple Prompt Moderation feature in AnswerAgentAI allows you to implement a basic content filter for user inputs before they are processed by the language model. This feature helps maintain content safety and prevents potentially harmful or unwanted content from being processed.

## Key Benefits

-   Enhances content safety by filtering out unwanted or sensitive input
-   Customizable deny list to suit your specific moderation needs
-   Optional use of a language model for more advanced similarity detection

## How to Use

1. Add the Simple Prompt Moderation node to your AnswerAgentAI workflow canvas.

<!-- TODO: Screenshot of adding the Simple Prompt Moderation node to the canvas -->
<figure><img src="/.gitbook/assets/screenshots/simplepromptmoderaation.png" alt="" /><figcaption><p> Simple Prompt Moderation Node &#x26; Drop UI</p></figcaption></figure>

2. Configure the node with the following settings:

    a. Deny List: Enter the phrases or words you want to block, one per line.
    b. Chat Model (optional): Select a language model to use for similarity detection.
    c. Error Message (optional): Customize the error message displayed when moderation fails.

<!-- TODO: Screenshot of the Simple Prompt Moderation node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/simplepromptmoderaationconfiguration.png" alt="" /><figcaption><p> Simple Prompt Moderation Node Configuration Panel &#x26; Drop UI</p></figcaption></figure>

3. Connect the Simple Prompt Moderation node between your input source and the language model in your workflow.

<!-- TODO: Screenshot of a workflow with the Simple Prompt Moderation node properly connected -->
<figure><img src="/.gitbook/assets/screenshots/simplepromptmoderaationconfigurationinworkflow.png" alt="" /><figcaption><p> Simple Prompt Moderation Node In a workflow &#x26; Drop UI</p></figcaption></figure>

4. Run your workflow. The Simple Prompt Moderation node will check user inputs against the deny list before allowing them to proceed to the language model.

## Tips and Best Practices

1. Start with a basic deny list and refine it over time based on your specific use case and user behavior.
2. Use clear and specific phrases in your deny list to avoid false positives.
3. If you're using the optional Chat Model for similarity detection, choose a model that balances accuracy and performance for your needs.
4. Regularly review and update your deny list to ensure it remains effective and relevant.
5. Consider using more advanced moderation techniques in combination with this simple approach for comprehensive content filtering.

## Troubleshooting

1. Issue: Legitimate inputs are being blocked
   Solution: Review your deny list and remove or modify overly broad or common phrases that might cause false positives.

2. Issue: Unwanted content is still getting through
   Solution: Add more specific phrases to your deny list or consider using the optional Chat Model for more advanced detection.

3. Issue: Moderation is slowing down the workflow
   Solution: If you're using the Chat Model option, try a smaller or faster model. Alternatively, simplify your deny list to focus on the most critical terms.

4. Issue: Error message is not displaying correctly
   Solution: Check the "Error Message" field in the node configuration and ensure it contains the desired text.

Remember, while the Simple Prompt Moderation feature provides a good starting point for content filtering, it's important to continuously monitor and adjust your moderation strategy to ensure the best balance between safety and usability in your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/output-parsers/advanced-structured-output-parser.md">
---
description: Parse LLM output into structured data using Zod schemas
---

# Advanced Structured Output Parser

## Overview

The Advanced Structured Output Parser is a powerful tool in AnswerAgentAI that allows you to parse the output of a language model into a predefined structure. By providing a Zod schema, you can ensure that the model's output adheres to a specific format, making it easier to work with structured data in your workflows.

## Key Benefits

-   Ensure consistent and structured output from language models
-   Easily validate and transform complex data structures
-   Improve the reliability of your AI-powered applications

## How to Use

1. Add the "Advanced Structured Output Parser" node to your AnswerAgentAI canvas.
2. Configure the node settings:
    - Set the "Autofix" option if you want the parser to attempt fixing errors automatically.
    - Provide a Zod schema in the "Example JSON" field.
3. Connect the parser node to your language model output.
4. Use the parsed output in subsequent nodes or as the final result of your workflow.

<!-- TODO: Screenshot of the Advanced Structured Output Parser node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/advancedstrcutureputputparser.png" alt="" /><figcaption><p> Advanced Structured Output Parser node configuration panel &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Start with a simple schema and gradually increase complexity as needed.
2. Use the "Autofix" option cautiously, as it may sometimes produce unexpected results.
3. Test your schemas thoroughly with various inputs to ensure robustness.
4. Leverage Zod's rich set of validation and transformation methods for advanced use cases.

## Troubleshooting

-   If you encounter a "Error parsing Zod Schema" message, double-check your schema syntax and ensure all required imports are present.
-   If the parser fails to extract the expected data, review your language model's prompt to ensure it's generating output in the correct format.

## Zod Schema Examples

Here are some examples of Zod schemas for different use cases:

### 1. Movie Information

This schema defines a structure for movie information, including a title, release year, genres (limited to 2), and a short description.

```javascript
z.object({
    title: z.string(),
    yearOfRelease: z.number().int(),
    genres: z
        .enum(['Action', 'Comedy', 'Drama', 'Fantasy', 'Horror', 'Mystery', 'Romance', 'Science Fiction', 'Thriller', 'Documentary'])
        .array()
        .max(2),
    shortDescription: z.string().max(500)
})
```

### 2. User Profile

This schema represents a user profile with validation for username length, email format, age range, number of interests, and verification status.

```javascript
z.object({
    username: z.string().min(3).max(20),
    email: z.string().email(),
    age: z.number().int().min(13).max(120),
    interests: z.array(z.string()).min(1).max(5),
    isVerified: z.boolean()
})
```

### 3. Product Catalog

This schema defines a product catalog with multiple products, each having an ID, name, price, category, stock status, and optional tags.

```javascript
z.object({
    products: z
        .array(
            z.object({
                id: z.string().uuid(),
                name: z.string(),
                price: z.number().positive(),
                category: z.enum(['Electronics', 'Clothing', 'Books', 'Home & Garden']),
                inStock: z.boolean(),
                tags: z.array(z.string()).optional()
            })
        )
        .min(1)
        .max(100)
})
```

### 4. Weather Forecast

This schema represents a weather forecast with location, date, temperature in Celsius and Fahrenheit, weather conditions, precipitation probability, and wind speed.

```javascript
z.object({
    location: z.string(),
    date: z.string().regex(/^\d{4}-\d{2}-\d{2}$/),
    temperature: z.object({
        celsius: z.number(),
        fahrenheit: z.number()
    }),
    conditions: z.enum(['Sunny', 'Cloudy', 'Rainy', 'Snowy', 'Windy']),
    precipitation: z.number().min(0).max(100),
    windSpeed: z.number().nonnegative()
})
```

### 5. Blog Post

This schema defines a structure for a blog post, including title, author, publish date, content, tags, and optional comments.

```javascript
z.object({
    title: z.string().max(100),
    author: z.string(),
    publishDate: z.string().datetime(),
    content: z.string().min(100),
    tags: z.array(z.string()).max(10),
    comments: z
        .array(
            z.object({
                user: z.string(),
                text: z.string().max(500),
                timestamp: z.string().datetime()
            })
        )
        .optional()
})
```

By using these Zod schema examples as a starting point, you can create custom schemas tailored to your specific use cases in AnswerAgentAI. Remember to adjust the validation rules and structure to match your exact requirements.
</file>

<file path="sidekick-studio/chatflows/output-parsers/csv-output-parser.md">
---
description: Parse LLM output as a comma-separated list of values
---

# CSV Output Parser

## Overview

The CSV Output Parser is a powerful tool in AnswerAgentAI that transforms the output of a language model into a comma-separated list of values. This parser is particularly useful when you need to generate structured data from LLM responses or normalize output from chat models and LLMs.

## Key Benefits

-   Simplifies data extraction from LLM outputs
-   Enables easy integration with downstream tasks that require structured data
-   Improves consistency in LLM-generated lists

## How to Use

1. Add the CSV Output Parser node to your AnswerAgentAI workflow canvas.
2. Connect the output of your LLM or chat model to the input of the CSV Output Parser.
3. Configure the parser settings:
    - **Autofix**: Enable this option if you want the parser to attempt fixing errors automatically.

<!-- TODO: Add a screenshot of the CSV Output Parser node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/csvoutputparser.png" alt="" /><figcaption><p> CSV Output Parser node configuration panel &#x26; Drop UI</p></figcaption></figure>

4. Connect the output of the CSV Output Parser to your desired downstream nodes or tasks.

## Tips and Best Practices

1. Use clear prompts: When working with LLMs, ensure your prompts explicitly request comma-separated lists to improve parsing accuracy.
2. Validate output: Always validate the parsed output to ensure it meets your requirements, especially when working with critical data.
3. Combine with other parsers: Consider chaining the CSV Output Parser with other parsers for more complex data structures.

## Troubleshooting

1. **Issue**: Parser fails to separate values correctly
   **Solution**: Check if the LLM output is truly comma-separated. You may need to adjust your prompt or enable the Autofix option.

2. **Issue**: Unexpected empty values in the parsed list
   **Solution**: Review your LLM prompt to ensure it's not generating empty list items. You may need to post-process the parsed list to remove empty values.

By leveraging the CSV Output Parser in your AnswerAgentAI workflows, you can efficiently transform unstructured LLM outputs into structured, comma-separated lists, enabling smoother data processing and analysis in your projects.
</file>

<file path="sidekick-studio/chatflows/output-parsers/custom-list-output-parser.md">
---
description: Parse LLM output as a list of values
---

# Custom List Output Parser

## Overview

The Custom List Output Parser is a powerful tool in AnswerAgentAI that allows you to parse the output of a Language Model (LLM) call as a list of values. This feature is particularly useful when you need to extract structured information from the LLM's response in a list format.

## Key Benefits

-   Easily convert unstructured LLM output into a structured list
-   Customize the parsing process to fit your specific needs
-   Improve the reliability of parsed output with autofix functionality

## How to Use

To use the Custom List Output Parser in your AnswerAgentAI workflow:

1. Locate the "Custom List Output Parser" node in the node library.
2. Drag and drop the node onto your canvas.
3. Connect the output of your LLM node to the input of the Custom List Output Parser node.
4. Configure the node parameters:
    - Length: Specify the number of values you want in the output list (optional).
    - Separator: Define the character used to separate values in the list (default is ',').
    - Autofix: Enable this option to attempt automatic error correction if parsing fails.
5. Connect the output of the Custom List Output Parser to your desired destination node.

<!-- TODO: Add a screenshot showing the Custom List Output Parser node on the canvas with its configuration panel open -->
<figure><img src="/.gitbook/assets/screenshots/customlistoutputparser.png" alt="" /><figcaption><p> Custom List Output Parser node configuration panel &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. When setting the "Length" parameter, ensure it matches the expected number of items in your LLM's output for best results.
2. Choose a unique separator that is unlikely to appear within the list items themselves to avoid parsing errors.
3. Enable the "Autofix" option when dealing with potentially inconsistent LLM outputs to improve reliability.
4. Test your parser with various LLM outputs to ensure it handles different scenarios correctly.

## Troubleshooting

Common issues and solutions:

1. Incorrect number of items in the output list:

    - Verify that the "Length" parameter is set correctly.
    - Check if the LLM is consistently producing the expected number of items.

2. Items not separated correctly:

    - Ensure the "Separator" parameter matches the separator used in the LLM output.
    - Consider using a more unique separator if the current one appears within list items.

3. Parsing errors:
    - Enable the "Autofix" option to attempt automatic error correction.
    - Review the LLM prompt to ensure it's generating output in the expected format.
</file>

<file path="sidekick-studio/chatflows/output-parsers/README.md">
---
description: Transform and structure AI-generated output with Output Parser nodes
---

# Output Parser Nodes

## Overview

Output Parser nodes in AnswerAgentAI are powerful tools designed to transform and structure the raw output from AI models into more useful formats for downstream tasks. These nodes are essential when working with Language Models (LLMs) or chat models to generate structured data or normalize their outputs.

## Key Benefits

-   Standardize and structure AI-generated content
-   Improve data consistency for downstream processing
-   Enhance the usability of AI outputs in various applications

### Output Parser Nodes

-   [CSV Output Parser](csv-output-parser.md)
-   [Custom List Output Parser](custom-list-output-parser.md)
-   [Structured Output Parser](structured-output-parser.md)
-   [Advanced Structured Output Parser](advanced-structured-output-parser.md)
</file>

<file path="sidekick-studio/chatflows/output-parsers/structured-output-parser.md">
---
description: Parse LLM output into structured JSON format
---

# Structured Output Parser

## Overview

The Structured Output Parser is a powerful feature in AnswerAgentAI that allows you to parse the output of a Language Model (LLM) call into a predefined JSON structure. This tool is essential for extracting structured information from the LLM's responses, making it easier to process and use the data in your workflows.

## Key Benefits

-   Standardizes LLM outputs into a consistent, structured JSON format
-   Improves data extraction and processing from LLM responses
-   Enables easy integration of LLM outputs with other systems and workflows

## How to Use

1. Add the Structured Output Parser node to your AnswerAgentAI canvas.
2. Configure the node with the following settings:

    a. Autofix (Optional):

    - Toggle this option to enable automatic fixing of parsing errors.
    - When enabled, if the initial parsing fails, the system will make another call to the model to correct any errors.

    b. JSON Structure:

    - Define the structure you want the LLM output to conform to.
    - Add properties to the data grid, specifying:
        - Property name
        - Data type (string, number, or boolean)
        - Description of the property

3. Connect the Structured Output Parser node to your LLM node in the workflow.
4. Run your workflow to see the parsed, structured output.

<!-- TODO: Add a screenshot of the Structured Output Parser node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/structuredoutputparser.png" alt="" /><figcaption><p> Structured Output Parser node configuration panel &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Be specific in your property descriptions to guide the LLM in generating appropriate content.
2. Start with a simple structure and gradually add complexity as needed.
3. Use the Autofix feature when dealing with complex structures or less reliable LLM outputs.
4. Test your parser with various LLM outputs to ensure it handles different scenarios correctly.
5. If you need nested or more structured outputs, consider using the [Advanced Structured Output Parser](advanced-structured-output-parser.md) node.

## Troubleshooting

1. Invalid JSON Structure:

    - Error: "Invalid JSON in StructuredOutputParser"
    - Solution: Double-check your JSON structure in the data grid. Ensure all properties have valid names and types.

2. Parsing Failures:

    - Issue: The parser fails to extract the desired information consistently.
    - Solution: Refine your property descriptions, simplify the structure, or enable the Autofix option.

3. Unexpected Output:
    - Issue: The parsed output doesn't match your expectations.
    - Solution: Review your LLM prompt and ensure it aligns with the structure you've defined in the parser.

## Example Usage

Here's a basic example of how to set up the JSON Structure:

1. Property: answer
   Type: string
   Description: Answer to the user's question

2. Property: source
   Type: string
   Description: Sources used to answer the question, should be websites

This structure will parse the LLM output into an object with 'answer' and 'source' fields, making it easy to use this information in subsequent steps of your workflow.

<!-- TODO: Add a screenshot of a sample workflow using the Structured Output Parser -->
<figure><img src="/.gitbook/assets/screenshots/structuredoutputparserinaworkflow.png" alt="" /><figcaption><p> Structured Output Parser node In a workflow &#x26; Drop UI</p></figcaption></figure>

By using the Structured Output Parser, you can transform raw LLM responses into clean, structured data, enhancing the capabilities and reliability of your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/prompts/chat-prompt-template.md">
---
description: Create customizable chat prompt templates for your AI workflows
---

# Chat Prompt Template

## Overview

The Chat Prompt Template node allows you to create structured prompts for chat-based AI interactions. It combines a system message and a human message to guide the AI's behavior and provide context for the conversation.

## Key Benefits

-   Customize AI behavior with specific system instructions
-   Create reusable templates for consistent chat interactions
-   Easily incorporate dynamic values into your prompts

## How to Use

1. Add the Chat Prompt Template node to your AnswerAgentAI canvas.
2. Configure the node with the following inputs:

    a. System Message:

    - Enter the instructions or context for the AI assistant.
    - Use curly braces {} to denote variables (e.g., \{input_language\}).

    b. Human Message:

    - Enter the template for the user's input.
    - Use curly braces {} to denote variables (e.g., \{text\}).

    c. Format Prompt Values (optional):

    - Provide a JSON object with key-value pairs for the variables used in your prompts.

3. Connect the Chat Prompt Template node to other nodes in your workflow.

<!-- TODO: Add a screenshot of the Chat Prompt Template node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/chatprompttemplate.png" alt="" /><figcaption><p> Chat Prompt Template Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Use clear and specific language in your system message to guide the AI's behavior effectively.
-   Keep your human message template simple and focused on the core input you want to process.
-   When using variables, ensure that the keys in your Format Prompt Values JSON match the variable names in your prompts.
-   Test your prompt template with various inputs to ensure it produces the desired results consistently.

## Troubleshooting

1. Issue: Invalid JSON error in Format Prompt Values
   Solution: Ensure that your JSON is properly formatted. Use double quotes for keys and string values, and avoid trailing commas.

2. Issue: Variables not being replaced in the prompt
   Solution: Double-check that the variable names in your prompts match the keys in your Format Prompt Values JSON exactly.

## Example Usage

Here's an example of how to set up a Chat Prompt Template for a language translation assistant:

System Message:

```
You are a helpful assistant that translates \{input_language\} to \{output_language\}.
```

Human Message:

```
\{text\}
```

Format Prompt Values:

```json
{
    "input_language": "English",
    "output_language": "Spanish",
    "text": "Hello, how are you?"
}
```

This setup will create a prompt that instructs the AI to act as a translator from English to Spanish, with the text "Hello, how are you?" as the input to be translated.

<!-- TODO: Add a screenshot of a completed Chat Prompt Template node connected to other nodes in a workflow -->
<figure><img src="/.gitbook/assets/screenshots/chatpromptnodeinaworkflow.png" alt="" /><figcaption><p> Chat Prompt Template Node In Workflow &#x26; Drop UI</p></figcaption></figure>

By using the Chat Prompt Template node, you can create flexible and powerful prompts that adapt to various use cases in your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/prompts/few-shot-prompt-template.md">
---
description: Build prompt templates with examples using the Few Shot Prompt Template node
---

# Few Shot Prompt Template

## Overview

The Few Shot Prompt Template node allows you to create dynamic prompt templates with examples. This powerful feature enables you to guide the AI's responses by providing sample inputs and outputs, making it especially useful for tasks that require specific formatting or reasoning patterns.

## Key Benefits

-   Improve AI response consistency by providing examples
-   Customize prompts for specific use cases or domains
-   Enhance the quality of AI-generated content by demonstrating desired outputs

## How to Use

1. Add the "Few Shot Prompt Template" node to your AnswerAgentAI canvas.
2. Configure the node with the following inputs:

    a. Examples: Provide a JSON array of example input-output pairs.
    b. Example Prompt: Connect a Prompt Template node to define the structure of each example.
    c. Prefix: Add text that appears before the examples (optional).
    d. Suffix: Add text that appears after the examples, including input variables.
    e. Example Separator: Specify how to separate multiple examples (default is two newlines).
    f. Template Format: Choose between "f-string" (default) or "jinja-2" formatting.

3. Connect the Few Shot Prompt Template node to other nodes in your workflow, such as a language model or chain.

## Tips and Best Practices

-   Provide diverse examples to cover various scenarios your AI might encounter.
-   Keep your examples concise and relevant to the task at hand.
-   Experiment with different prefix and suffix texts to find the most effective prompt structure.
-   Use the Example Separator to control the visual spacing between examples.

## Troubleshooting

-   If you encounter an "Invalid JSON" error, double-check the format of your Examples input.
-   Ensure that the Example Prompt template matches the structure of your provided examples.
-   Verify that all required input variables are included in the Suffix.

## Example Use Cases

### 1. Antonym Generator

```javascript
{
  "examples": [
    { "word": "happy", "antonym": "sad" },
    { "word": "tall", "antonym": "short" },
    { "word": "rich", "antonym": "poor" }
  ],
  "examplePrompt": "Word: {word}\nAntonym: {antonym}",
  "prefix": "Generate antonyms for the following words:",
  "suffix": "Word: {input}\nAntonym:",
  "exampleSeparator": "\n\n"
}
```

This setup creates a prompt template that demonstrates how to generate antonyms, then asks for the antonym of a new input word.

### 2. Customer Support Response Generator

```javascript
{
  "examples": [
    {
      "query": "How do I reset my password?",
      "response": "To reset your password, please follow these steps:\n1. Go to the login page\n2. Click on 'Forgot Password'\n3. Enter your email address\n4. Follow the instructions sent to your email"
    },
    {
      "query": "What are your business hours?",
      "response": "Our customer support is available 24/7. Our physical stores are open Monday to Friday, 9 AM to 6 PM, and Saturday 10 AM to 4 PM. We are closed on Sundays and public holidays."
    }
  ],
  "examplePrompt": "Customer: {query}\n\nSupport Agent: {response}",
  "prefix": "You are a helpful customer support agent. Respond to the customer query based on the following examples:",
  "suffix": "Customer: {input}\n\nSupport Agent:",
  "exampleSeparator": "\n\n---\n\n"
}
```

This setup creates a prompt template that demonstrates how to respond to customer queries in a consistent and helpful manner.

### 3. Code Comment Generator

````javascript
{
  "examples": [
    {
      "code": "def factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)",
      "comment": "This function calculates the factorial of a given number using recursion."
    },
    {
      "code": "for i in range(10):\n    print(i)",
      "comment": "This loop prints numbers from 0 to 9."
    }
  ],
  "examplePrompt": "Code:\n```python\n{code}\n```\n\nComment: {comment}",
  "prefix": "Generate a brief comment explaining the following code snippets:",
  "suffix": "Code:\n```python\n{input}\n```\n\nComment:",
  "exampleSeparator": "\n\n"
}
````

This setup creates a prompt template that demonstrates how to generate concise and informative comments for Python code snippets.

<!-- TODO: Add a screenshot showing the Few Shot Prompt Template node configuration panel with one of the example use cases filled in -->
<figure><img src="/.gitbook/assets/screenshots/fewshotprompt.png" alt="" /><figcaption><p> Few Shot Template Node Configuration &#x26; Drop UI</p></figcaption></figure>

By using these example configurations, you can create powerful Few Shot Prompt Templates tailored to various tasks and domains in AnswerAgentAI.
</file>

<file path="sidekick-studio/chatflows/prompts/prompt-template.md">
---
description: Create and customize templates for language model prompts
---

# Prompt Template

## Overview

The Prompt Template node in AnswerAgentAI allows you to create structured templates for generating prompts for language models. This feature enables you to design reusable prompt structures with placeholders for dynamic content, making it easier to create consistent and customizable prompts for various use cases.

## Key Benefits

-   Standardize prompt structures for consistent outputs
-   Easily incorporate dynamic content into your prompts
-   Improve efficiency by reusing prompt templates across different workflows

## How to Use

1. Add the Prompt Template node to your AnswerAgentAI canvas.
2. Configure the template:
   a. In the "Template" field, enter your prompt structure with placeholders in curly braces, e.g., `{placeholder}`.
   b. (Optional) In the "Format Prompt Values" field, provide JSON data for the placeholders.

3. Connect the Prompt Template node to other nodes in your workflow.

<!-- TODO: Screenshot of the Prompt Template node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/prompt.png" alt="" /><figcaption><p> Prompt Template Node &#x26; Drop UI</p></figcaption></figure>

### Example Template

```
What is a good name for a company that makes {product}?
```

### Example Format Prompt Values (JSON)

```json
{
    "product": "eco-friendly water bottles"
}
```

## Tips and Best Practices

1. Use descriptive placeholder names to make your templates more readable and maintainable.
2. Keep your templates modular and reusable by focusing on specific prompt structures.
3. Use the "Format Prompt Values" field to test your template with sample data before integrating it into your workflow.
4. Consider creating a library of commonly used prompt templates for quick access and consistency across projects.

## Troubleshooting

1. Invalid JSON error: Ensure that the JSON in the "Format Prompt Values" field is correctly formatted. Use a JSON validator if needed.
2. Missing placeholder values: Check that all placeholders in your template have corresponding values in the JSON data or connected nodes.

## Use Cases and Examples

### 1. Product Naming

Template:

```
Generate 5 creative names for a {product_type} that emphasizes its {key_feature}. The target audience is {target_audience}.
```

Format Prompt Values:

```json
{
    "product_type": "smartphone",
    "key_feature": "long battery life",
    "target_audience": "busy professionals"
}
```

### 2. Customer Support Response

Template:

```
You are a customer support representative for {company_name}. Craft a polite and helpful response to the following customer inquiry:

Customer: {customer_inquiry}

Your response should address the customer's concern and provide a solution if possible. If you need more information, ask for it politely.
```

Format Prompt Values:

```json
{
    "company_name": "TechGadgets Inc.",
    "customer_inquiry": "I received my new smartwatch yesterday, but I can't figure out how to pair it with my phone. Can you help?"
}
```

### 3. Content Summarization

Template:

```
Summarize the following {content_type} in {word_count} words or less, focusing on the main points and key takeaways:

{content}
```

Format Prompt Values:

```json
{
    "content_type": "research paper",
    "word_count": 150,
    "content": "... (full text of the research paper) ..."
}
```

## Difference from Chat Prompt Template

The Prompt Template node differs from the Chat Prompt Template in several key aspects:

1. Structure: Prompt Template is designed for single-turn interactions, while Chat Prompt Template is optimized for multi-turn conversations.

2. Use case: Use Prompt Template for generating standalone prompts, such as content creation or single-query tasks. Use Chat Prompt Template for building conversational flows or maintaining context across multiple exchanges.

3. Placeholder handling: Prompt Template uses simple `{placeholder}` syntax, while Chat Prompt Template may have more complex structures for handling user and system messages.

4. Output: Prompt Template generates a single prompt string, whereas Chat Prompt Template typically produces a structured chat history or message array.

Choose the Prompt Template when you need a straightforward, single-turn prompt generation, and opt for the Chat Prompt Template when building more complex, conversational interactions.
</file>

<file path="sidekick-studio/chatflows/prompts/README.md">
---
description: Prompt Templates in AnswerAgentAI
---

# Prompts

Prompt template nodes in AnswerAgentAI help translate user input and parameters into instructions for language models. They guide the model's response, helping it understand the context and generate relevant and coherent language-based output.

## Best Practices for Prompt Engineering

1. Be specific and clear: Provide detailed instructions to guide the model's output.
2. Use examples: Include sample inputs and outputs to demonstrate the desired format and content.
3. Break down complex tasks: Divide multi-step processes into smaller, manageable prompts.
4. Iterate and refine: Test your prompts and adjust them based on the results.
5. Consider the model's context window: Be mindful of the input length limitations.
6. Use consistent formatting: Maintain a uniform structure for similar types of prompts.

## Troubleshooting Prompts

1. Unexpected outputs: Review your prompt for ambiguity or unclear instructions.
2. Inconsistent results: Ensure your prompt provides enough context and constraints.
3. Off-topic responses: Check if your prompt is too open-ended or lacks specific guidance.
4. Repetitive outputs: Add diversity to your examples or include instructions to avoid repetition.
5. Incomplete responses: Verify that your prompt isn't too complex for a single response.

## Using Templates

Templates in AnswerAgentAI allow you to create reusable prompt structures. Here are some tips for effective template use:

1. Identify common patterns: Create templates for frequently used prompt structures.
2. Use variables: Incorporate placeholders for dynamic content in your templates.
3. Balance flexibility and specificity: Design templates that can be adapted for various use cases while maintaining clear instructions.
4. Document your templates: Include comments or descriptions to explain the purpose and usage of each template.
5. Version control: Keep track of template versions and updates to maintain consistency across your projects.

## Prompt Nodes

### Chat Prompt Template

The Chat Prompt Template node is designed for creating structured conversations with language models. It allows you to define a series of messages, including system messages, user inputs, and AI responses.

[Learn more about Chat Prompt Template](chat-prompt-template.md)

### Few Shot Prompt Template

The Few Shot Prompt Template node enables you to provide examples to guide the model's behavior. This is particularly useful when you want the model to follow a specific pattern or format in its responses.

[Learn more about Few Shot Prompt Template](few-shot-prompt-template.md)

### Prompt Template

The Prompt Template node is a versatile tool for creating custom prompts with variable inputs. It allows you to define a template structure and dynamically insert values based on user input or other data sources.

[Learn more about Prompt Template](prompt-template.md)

<!-- TODO: Add a screenshot showing the Prompt Nodes in the AnswerAgentAI canvas -->
<figure><img src="/.gitbook/assets/screenshots/promptnodes.png" alt="" /><figcaption><p> Prompt Nodes &#x26; Drop UI</p></figcaption></figure>

By mastering these prompt template nodes and following the best practices for prompt engineering, you can create more effective and efficient workflows in AnswerAgentAI. Remember to iterate and refine your prompts based on the results you achieve, and don't hesitate to experiment with different approaches to find the most suitable solution for your specific use case.
</file>

<file path="sidekick-studio/chatflows/retrievers/cohere-rerank-retriever.md">
---
description: Cohere Rerank Retriever - Enhance document retrieval with semantic relevance
---

# Cohere Rerank Retriever

## Overview

The Cohere Rerank Retriever is a powerful feature in AnswerAgentAI that enhances document retrieval by ranking documents based on their semantic relevance to a given query. This retriever uses Cohere's advanced reranking models to provide more accurate and contextually appropriate search results.

## Key Benefits

-   Improved search accuracy: Ranks documents based on semantic relevance, not just keyword matching
-   Multilingual support: Offers both English and multilingual reranking models
-   Customizable retrieval: Allows fine-tuning of parameters for optimal results

## How to Use

1. Add the Cohere Rerank Retriever node to your AnswerAgentAI workflow canvas.
2. Connect a Vector Store Retriever to the Cohere Rerank Retriever node.
3. Configure the node settings:
    - Select the Cohere API credential.
    - Choose the reranking model (English or Multilingual).
    - Set additional parameters like Top K and Max Chunks Per Doc if needed.
4. Connect the output to your desired next step in the workflow.

<!-- TODO: Add a screenshot of the Cohere Rerank Retriever node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/cohereretreivernode.png" alt="" /><figcaption><p> Cohere Retreiver Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Start with the default settings and adjust as needed based on your specific use case.
2. Experiment with different Top K values to balance between retrieval speed and accuracy.
3. Use the multilingual model if your content or queries are in languages other than English.
4. Consider the trade-off between Max Chunks Per Doc and processing time  higher values may provide better results but take longer to process.

## Troubleshooting

1. If you're not getting expected results:

    - Ensure your Cohere API credential is correctly set up.
    - Check if the base Vector Store Retriever is properly configured and contains relevant documents.
    - Try adjusting the Top K value to retrieve more or fewer documents.

2. If the retrieval process is slow:

    - Consider reducing the Max Chunks Per Doc value.
    - Optimize your base Vector Store Retriever for faster initial retrieval.

3. For multilingual issues:
    - Make sure you're using the 'rerank-multilingual-v2.0' model for non-English content.
</file>

<file path="sidekick-studio/chatflows/retrievers/embeddings-filter-retriever.md">
---
description: Enhance document retrieval with embeddings-based filtering
---

# Embeddings Filter Retriever

## Overview

The Embeddings Filter Retriever is a powerful tool in AnswerAgentAI that enhances document retrieval by using embeddings to filter out documents unrelated to the query. This retriever combines the efficiency of vector store retrieval with the precision of embeddings-based filtering, resulting in more relevant and focused document retrieval.

## Key Benefits

-   Improved relevance: Filters out documents that are not semantically related to the query
-   Customizable filtering: Adjust similarity thresholds to fine-tune retrieval precision
-   Flexible output options: Retrieve the retriever object, filtered documents, or concatenated text

## How to Use

1. Add the Embeddings Filter Retriever node to your AnswerAgentAI workflow canvas.
2. Connect a Vector Store Retriever to the "Vector Store Retriever" input.
3. Connect an Embeddings model to the "Embeddings" input.
4. (Optional) Specify a query in the "Query" input field. If left empty, the user's question will be used.
5. Adjust the "Similarity Threshold" (default: 0.8) to set the minimum similarity score for document inclusion.
6. (Optional) Set the "K" value to limit the number of returned documents (default: 20).
7. Choose the desired output type: retriever, document, or text.
8. Connect the output to subsequent nodes in your workflow.

<!-- TODO: Add a screenshot of the Embeddings Filter Retriever node with its inputs and outputs labeled -->
<figure><img src="/.gitbook/assets/screenshots/embeddingsfilter.png" alt="" /><figcaption><p> Embeddings Filter Retreiver Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different similarity thresholds to find the optimal balance between precision and recall for your use case.
2. When using the "document" or "text" output, consider adding a prompt template node to format the retrieved information for your language model.
3. If you're unsure about the query ahead of time, leave the "Query" field empty to use the dynamic user input.
4. Use the "K" parameter to limit the number of documents when you need a fixed-size output, and use the "Similarity Threshold" when you want to ensure a minimum relevance level.

## Troubleshooting

1. If you're not getting any results, try lowering the similarity threshold or increasing the "K" value.
2. Ensure that your Vector Store Retriever and Embeddings model are compatible and properly configured.
3. If you encounter an error stating "Must specify one of 'k' or 'similarity_threshold'", make sure you've set at least one of these parameters.

<!-- TODO: Add a screenshot showing the error message and where to set the required parameters -->
</file>

<file path="sidekick-studio/chatflows/retrievers/hyde-retriever.md">
---
description: Use HyDE retriever to retrieve relevant documents from a vector store
---

# HyDE Retriever

## Overview

The HyDE (Hypothetical Document Embeddings) Retriever is a powerful tool in AnswerAgentAI that enhances document retrieval from a vector store. It uses a language model to generate a hypothetical answer to a query, which is then used to find relevant documents. This approach can significantly improve the quality and relevance of retrieved documents, especially for complex or nuanced queries.

## Key Benefits

-   Improved retrieval quality for complex queries
-   Leverages the power of language models to enhance search
-   Flexible configuration options for various use cases

## How to Use

1. Add the HyDE Retriever node to your AnswerAgentAI canvas.
2. Connect a Language Model node to the "Language Model" input.
3. Connect a Vector Store node to the "Vector Store" input.
4. (Optional) Provide a specific query in the "Query" field. If left empty, the user's question will be used.
5. Choose a predefined prompt or create a custom one:
    - Select a prompt from the "Select Defined Prompt" dropdown, or
    - Enter a custom prompt in the "Custom Prompt" field (this will override the selected predefined prompt).
6. (Optional) Adjust the "Top K" value to set the number of results to retrieve (default is 4).
7. Choose the desired output type:
    - HyDE Retriever: Returns the retriever object for further processing
    - Document: Returns an array of retrieved documents
    - Text: Returns a concatenated string of the retrieved documents' content

<!-- TODO: Add a screenshot showing the HyDE Retriever node with its inputs and outputs connected -->
<figure><img src="/.gitbook/assets/screenshots/hyderetreiver.png" alt="" /><figcaption><p> Hyde Retreiver Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different prompts to find the best one for your use case. The predefined prompts cover various scenarios, but a custom prompt might work better for specific domains.
2. Adjust the "Top K" value based on your needs. A higher value will return more results but may include less relevant documents.
3. Use the "Document" output when you need to access individual document metadata or want to process documents separately.
4. Use the "Text" output when you need a simple concatenated string of all retrieved content.

## Troubleshooting

-   If the retriever is not returning expected results, try the following:

    1. Check if the Language Model and Vector Store are properly connected and configured.
    2. Experiment with different prompts or adjust the existing one.
    3. Increase the "Top K" value to retrieve more documents.
    4. Verify that the vector store contains relevant documents for your queries.

-   If you encounter performance issues:
    1. Reduce the "Top K" value to retrieve fewer documents.
    2. Use a smaller or more efficient Language Model.
    3. Optimize your Vector Store for faster retrieval.
</file>

<file path="sidekick-studio/chatflows/retrievers/llm-filter-retriever.md">
---
description: LLM Filter Retriever - Enhance document retrieval with contextual compression
---

# LLM Filter Retriever

## Overview

The LLM Filter Retriever is an advanced retrieval node that enhances the document retrieval process by applying contextual compression. It iterates over initially returned documents and extracts only the content that is relevant to the query, providing more focused and accurate results.

## Key Benefits

-   Improves retrieval accuracy by filtering out irrelevant content
-   Reduces noise in retrieved documents, focusing on query-relevant information
-   Enhances the quality of input for downstream tasks in your AnswerAgentAI workflow

## How to Use

1. Add the LLM Filter Retriever node to your AnswerAgentAI canvas.
2. Connect a Vector Store Retriever to the "Vector Store Retriever" input.
3. Connect a Language Model to the "Language Model" input.
4. (Optional) Provide a specific query in the "Query" input field.
5. Choose the desired output type: retriever, document, or text.

<!-- TODO: Add a screenshot showing the LLM Filter Retriever node connected to a Vector Store Retriever and a Language Model -->
<figure><img src="/.gitbook/assets/screenshots/llmfilterretreiver.png" alt="" /><figcaption><p> LLM Filter Retreiver Node &#x26; Drop UI</p></figcaption></figure>

## Input Parameters

1. **Vector Store Retriever**: Connect a Vector Store Retriever node to provide the base retrieval mechanism.
2. **Language Model**: Connect a Language Model node (e.g., GPT-3, GPT-4) to power the contextual compression.
3. **Query** (optional): Enter a specific query for document retrieval. If not provided, the user's question will be used.

## Output Options

1. **LLM Filter Retriever**: Returns the retriever object for use in subsequent nodes.
2. **Document**: Provides an array of document objects containing metadata and filtered page content.
3. **Text**: Returns a concatenated string of the filtered page content from all retrieved documents.

## Tips and Best Practices

1. Experiment with different Language Models to find the best balance between performance and accuracy.
2. Use specific queries when possible to improve the relevance of retrieved content.
3. Consider the trade-off between processing time and result quality when using this node in your workflow.

## Troubleshooting

1. **Error: "There must be a LLM model connected to LLM Filter Retriever"**

    - Ensure that you have connected a Language Model node to the LLM Filter Retriever.

2. **Retrieved content is not relevant to the query**

    - Try using a more powerful Language Model or refine your Vector Store Retriever's settings.
    - Ensure your document collection in the Vector Store is relevant and up-to-date.

3. **Slow performance**
    - The LLM Filter Retriever may increase processing time due to the additional filtering step. Consider using it only when necessary for high-precision tasks.

Remember that the effectiveness of the LLM Filter Retriever depends on the quality of both the Vector Store Retriever and the Language Model used. Experiment with different combinations to achieve the best results for your specific use case.
</file>

<file path="sidekick-studio/chatflows/retrievers/prompt-retriever.md">
---
description: Store and retrieve prompt templates for specialized tasks
---

# Prompt Retriever

## Overview

The Prompt Retriever node allows you to store prompt templates with a name and description, which can be later queried and used by the MultiPromptChain. This node is particularly useful for organizing and managing multiple specialized prompts for different tasks or domains.

## Key Benefits

-   Easily organize and manage multiple prompt templates
-   Streamline the process of using specialized prompts for different tasks
-   Improve the efficiency and accuracy of your AI workflows

## How to Use

1. Add the Prompt Retriever node to your canvas in the AnswerAgentAI Studio.
2. Configure the node by filling in the following fields:

    a. Prompt Name: Enter a unique name for your prompt template (e.g., "physics-qa").

    b. Prompt Description: Provide a brief description of what the prompt does and when it should be used (e.g., "Good for answering questions about physics").

    c. Prompt System Message: Enter the system message that sets the context and behavior for the AI model (e.g., "You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don't know the answer to a question you admit that you don't know.").

3. Connect the Prompt Retriever node to other nodes in your workflow, such as the MultiPromptChain node.

<!-- TODO: Add a screenshot showing the Prompt Retriever node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/promptretreiver.png" alt="" /><figcaption><p> Prompt Retreiver  Node &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Use descriptive and specific names for your prompt templates to easily identify them later.
2. Write clear and concise descriptions that explain the purpose and ideal use case for each prompt.
3. Craft system messages that effectively guide the AI model's behavior and expertise for the specific task.
4. Create multiple Prompt Retriever nodes for different domains or tasks to build a comprehensive library of specialized prompts.
5. Regularly review and update your prompt templates to improve their effectiveness over time.

## Troubleshooting

1. If your prompt is not being retrieved correctly:

    - Double-check that the Prompt Name is correctly spelled and matches the name used in the MultiPromptChain node.
    - Ensure that the Prompt Retriever node is properly connected to the MultiPromptChain node in your workflow.

2. If the AI responses are not as expected:

    - Review and refine your Prompt System Message to provide clearer instructions or context to the AI model.
    - Consider adjusting the Prompt Description to better match the intended use case.

3. If you're experiencing performance issues:
    - Try to keep your system messages concise while still providing necessary context.
    - Avoid creating too many Prompt Retriever nodes in a single workflow, as this may impact performance.

By using the Prompt Retriever node effectively, you can create a powerful and flexible system for managing specialized prompts in your AnswerAgentAI workflows, enhancing the capabilities and accuracy of your AI-powered applications.
</file>

<file path="sidekick-studio/chatflows/retrievers/README.md">
---
description: Understanding and Using Retriever Nodes in AnswerAgentAI
---

# Retriever Nodes

## Overview

Retriever Nodes are powerful components in AnswerAgentAI that allow you to fetch relevant documents based on unstructured queries. These nodes are designed to enhance your workflow by providing efficient and accurate document retrieval capabilities.

## Key Benefits

-   Improved information retrieval: Quickly find relevant documents from large datasets
-   Flexibility: Various retriever types to suit different use cases
-   Enhanced AI interactions: Provide context for more accurate AI responses

## Types of Retriever Nodes

AnswerAgentAI offers several types of Retriever Nodes, each with its unique capabilities:

-   [Cohere Rerank Retriever](cohere-rerank-retriever.md)
-   [Embeddings Filter Retriever](embeddings-filter-retriever.md)
-   [HyDE Retriever](hyde-retriever.md)
-   [LLM Filter Retriever](llm-filter-retriever.md)
-   [Prompt Retriever](prompt-retriever.md)
-   [Reciprocal Rank Fusion Retriever](reciprocal-rank-fusion-retriever.md)
-   [Similarity Score Threshold Retriever](similarity-score-threshold-retriever.md)
-   [Vector Store Retriever](vector-store-retriever.md)
-   [Voyage AI Rerank Retriever](voyage-ai-retriever.md)

## Tips and Best Practices

1. Choose the appropriate Retriever Node based on your specific use case and data characteristics.
2. Experiment with different retriever configurations to optimize performance.
3. Use multiple Retriever Nodes in combination for more comprehensive results.
4. Regularly update your document index to ensure fresh and accurate retrievals.
5. Monitor retrieval performance and adjust settings as needed.

## Troubleshooting

Common issues and solutions:

1. **Slow retrieval**:

    - Optimize your document index
    - Consider using a more efficient Retriever Node
    - Reduce the number of documents in your dataset

2. **Irrelevant results**:

    - Refine your query formulation
    - Adjust similarity thresholds
    - Try a different Retriever Node type
      `

3. **Error messages**:
    - Check your node connections
    - Verify API keys and permissions
    - Consult the AnswerAgentAI documentation for specific error codes

<!-- TODO: Add a screenshot showing the configuration panel of a Retriever Node with highlighted troubleshooting options -->
<figure><img src="/.gitbook/assets/screenshots/retreivernodes.png" alt="" /><figcaption><p> Retreiver Node &#x26; Drop UI</p></figcaption></figure>

By mastering the use of Retriever Nodes, you can significantly enhance your AnswerAgentAI workflows, enabling more accurate and efficient information retrieval for your AI-powered applications.
</file>

<file path="sidekick-studio/chatflows/retrievers/reciprocal-rank-fusion-retriever.md">
---
description: Reciprocal Rank Fusion Retriever for improved search results
---

# Reciprocal Rank Fusion Retriever

## Overview

The Reciprocal Rank Fusion (RRF) Retriever is an advanced retrieval method that enhances search results by generating multiple queries and re-ranking the results. This feature improves the relevance and diversity of retrieved documents, especially for complex or ambiguous queries.

## Key Benefits

-   Improved search accuracy by considering multiple query variations
-   Enhanced result diversity through re-ranking of documents
-   Better handling of complex or ambiguous user queries

## How to Use

1. Add the "Reciprocal Rank Fusion Retriever" node to your AnswerAgentAI workflow canvas.

2. Connect the required inputs:

    - Vector Store Retriever: Connect a vector store retriever node.
    - Language Model: Connect a language model node (e.g., GPT-3.5, GPT-4).

3. Configure the node parameters:

    - Query (optional): Specify a custom query or leave blank to use the user's question.
    - Query Count: Set the number of synthetic queries to generate (default is 4).
    - Top K: Specify the number of top results to fetch (default is the base retriever's Top K).
    - Constant: Set the constant value for the RRF algorithm (default is 60).

4. Connect the output to subsequent nodes in your workflow.

<!-- TODO: Add a screenshot showing the RRF Retriever node with its inputs and outputs connected -->

<figure><img src="/.gitbook/assets/screenshots/rfrr.png" alt="" /><figcaption><p> RRF Retreiver  Node &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Experiment with different Query Count values to find the optimal balance between result quality and processing time.

2. Adjust the Top K value based on your specific use case. A higher value may provide more comprehensive results but might increase processing time.

3. Fine-tune the Constant value to control the balance between high-ranked and lower-ranked items in the results.

4. Use this retriever for queries that might benefit from multiple perspectives or interpretations.

## Troubleshooting

1. If results seem less relevant than expected:

    - Increase the Query Count to generate more variations of the original query.
    - Adjust the Top K value to retrieve more initial results before re-ranking.

2. If processing time is too long:

    - Reduce the Query Count or Top K values.
    - Ensure your vector store and language model are optimized for performance.

3. If you receive an error about missing inputs:
    - Double-check that both the Vector Store Retriever and Language Model inputs are properly connected.
</file>

<file path="sidekick-studio/chatflows/retrievers/similarity-score-threshold-retriever.md">
---
description: Retrieve documents based on similarity score threshold
---

# Similarity Score Threshold Retriever

## Overview

The Similarity Score Threshold Retriever is a powerful tool in AnswerAgentAI that allows you to retrieve documents from a vector store based on a minimum similarity percentage. This retriever ensures that only the most relevant documents are returned, improving the quality of information used in your workflows.

## Key Benefits

-   Retrieve highly relevant documents by setting a minimum similarity score
-   Customize the retrieval process with adjustable parameters
-   Improve the accuracy of your AnswerAgentAI workflows by filtering out less relevant information

## How to Use

1. Add the Similarity Score Threshold Retriever node to your AnswerAgentAI canvas.
2. Connect a Vector Store node to the "Vector Store" input of the retriever.
3. Configure the following parameters:
    - Query (optional): Specify a custom query for document retrieval. If left empty, the user's question will be used.
    - Minimum Similarity Score (%): Set the minimum similarity percentage for retrieved documents.
    - Max K: Define the maximum number of results to fetch.
    - K Increment: Specify how much to increase K by each time when fetching results.
4. Connect the output of the retriever to other nodes in your workflow as needed.

<!-- TODO: Add a screenshot of the Similarity Score Threshold Retriever node with its inputs and outputs labeled -->
<figure><img src="/.gitbook/assets/screenshots/similarityscore.png" alt="" /><figcaption><p> Similarity Score Threshold Retreiver  Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different minimum similarity scores to find the right balance between relevance and coverage for your specific use case.
2. Use the "Query" input to provide context-specific searches when you want to retrieve documents for a particular topic or question.
3. Adjust the "Max K" and "K Increment" values to fine-tune the number of documents retrieved and the retrieval process's efficiency.
4. Consider using the "Document" or "Text" outputs for further processing or analysis in your workflow.

## Troubleshooting

1. If you're not getting any results, try lowering the minimum similarity score or increasing the Max K value.
2. Ensure that your Vector Store is properly configured and contains relevant documents for your queries.
3. If the retriever is slow, try reducing the Max K value or increasing the K Increment to balance between thoroughness and speed.

This retriever is based on the `ScoreThresholdRetriever` class from LangChain, adapted for use in AnswerAgentAI. For more detailed information on the underlying implementation, refer to the LangChain documentation.
</file>

<file path="sidekick-studio/chatflows/retrievers/vector-store-retriever.md">
---
description: Store and retrieve vector data for efficient information retrieval
---

# Vector Store Retriever

## Overview

The Vector Store Retriever is a powerful node in AnswerAgentAI that allows you to store and retrieve vector data efficiently. This feature is particularly useful for organizing and querying large amounts of information, making it an essential tool for building advanced question-answering systems and other AI-powered applications.

## Key Benefits

-   Efficient storage and retrieval of vector data
-   Seamless integration with MultiRetrievalQAChain for complex question-answering tasks
-   Customizable naming and description for easy organization of multiple retrievers

## How to Use

1. Add the Vector Store Retriever node to your AnswerAgentAI canvas.

2. Connect a Vector Store node to the "Vector Store" input of the Vector Store Retriever.

3. Configure the following parameters:

    - **Vector Store**: This should be automatically populated from the connected Vector Store node.
    - **Retriever Name**: Enter a unique name for your retriever (e.g., "netflix_movies").
    - **Retriever Description**: Provide a brief description of when to use this retriever (e.g., "Good for answering questions about Netflix movies").

4. Connect the output of the Vector Store Retriever to other nodes in your workflow, such as a MultiRetrievalQAChain node.

<!-- TODO: Add a screenshot showing the Vector Store Retriever node connected to a Vector Store node and a MultiRetrievalQAChain node -->
<figure><img src="/.gitbook/assets/screenshots/vectorstoreretreiver.png" alt="" /><figcaption><p> Vector Store Retreiver  Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose descriptive names for your retrievers to easily identify their purpose in complex workflows.

2. Write clear and concise descriptions for each retriever, focusing on its specific use case or the type of information it contains.

3. When using multiple Vector Store Retrievers in a single workflow, ensure that each one is tailored to a specific domain or topic for better organization and more accurate results.

4. Regularly update and maintain your vector stores to ensure the retriever always has access to the most current and relevant information.

## Troubleshooting

1. **Issue**: The Vector Store Retriever is not returning any results.
   **Solution**: Ensure that the connected Vector Store has been properly populated with data. Check the Vector Store node's configuration and verify that the data ingestion process was successful.

2. **Issue**: The retriever is returning irrelevant results.
   **Solution**: Review the data in your Vector Store and consider refining your data preprocessing steps. You may also want to experiment with different vector embedding techniques or adjust the similarity threshold in your querying process.

3. **Issue**: The Vector Store Retriever node is not connecting to other nodes in the workflow.
   **Solution**: Verify that the node connections are correct and that the output of the Vector Store Retriever is compatible with the input requirements of the connected nodes. If you're using a MultiRetrievalQAChain, ensure that it's configured to accept the Vector Store Retriever as an input.

By leveraging the Vector Store Retriever node in AnswerAgentAI, you can create powerful and efficient information retrieval systems that form the backbone of advanced AI applications. Whether you're building a question-answering system, a recommendation engine, or any other application that requires fast and accurate information lookup, the Vector Store Retriever is an invaluable tool in your AnswerAgentAI toolkit.
</file>

<file path="sidekick-studio/chatflows/retrievers/voyage-ai-retriever.md">
---
description: Voyage AI Rerank Retriever - Enhance document retrieval with semantic relevance
---

# Voyage AI Rerank Retriever

## Overview

The Voyage AI Rerank Retriever is a powerful feature in AnswerAgentAI that enhances document retrieval by semantically ranking documents based on their relevance to a given query. This retriever uses Voyage AI's reranking model to sort documents from most to least semantically relevant, improving the quality of retrieved information.

## Key Benefits

-   Improves search accuracy by semantically ranking documents
-   Enhances the relevance of retrieved information for user queries
-   Integrates seamlessly with existing vector store retrievers

## How to Use

1. Add the Voyage AI Rerank Retriever node to your AnswerAgentAI workflow canvas.
2. Connect a Vector Store Retriever to the Voyage AI Rerank Retriever node.
3. Configure the node settings:
    - Select the model (currently only 'rerank-lite-1' is available)
    - Optionally, specify a query for document retrieval
    - Set the Top K value to determine the number of top results to fetch
4. Connect your Voyage AI API credentials.
5. Choose the desired output type: retriever, document, or text.

<!-- TODO: Add a screenshot of the Voyage AI Rerank Retriever node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/voyageretreiver.png" alt="" /><figcaption><p> Voyage AI Rerank Retreiver  Node &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different Top K values to find the optimal balance between relevance and quantity of retrieved documents.
2. Use the query input when you want to retrieve documents based on a specific question rather than the user's input.
3. Consider using the 'document' or 'text' output options when you need to directly access the retrieved information in subsequent nodes.

## Troubleshooting

1. If you encounter authentication errors, double-check that your Voyage AI API credentials are correctly configured in the AnswerAgentAI credential manager.
2. Ensure that the connected Vector Store Retriever is properly set up and contains indexed documents.
3. If the retrieved documents seem irrelevant, try adjusting the Top K value or refining your query input.
</file>

<file path="sidekick-studio/chatflows/text-splitters/character-text-splitter.md">
---
description: Character Text Splitter Node for AnswerAgentAI
---

# Character Text Splitter

## Overview

The Character Text Splitter is a powerful tool in AnswerAgentAI that allows you to split long pieces of text into smaller, more manageable chunks. This splitter focuses on dividing text based on a specific character or sequence of characters, making it ideal for processing large documents or datasets.

## Key Benefits

-   Efficiently breaks down large texts into smaller, processable chunks
-   Customizable separator for flexible text splitting
-   Allows control over chunk size and overlap for optimal processing

## How to Use

1. Add the Character Text Splitter node to your AnswerAgentAI workflow canvas.
2. Configure the node parameters:

    - Chunk Size: Set the number of characters for each chunk (default: 1000)
    - Chunk Overlap: Specify the number of characters to overlap between chunks (default: 200)
    - Custom Separator: Optionally, define a custom separator to override the default ("\n\n")

3. Connect the Character Text Splitter node to your text input source.
4. Run your workflow to split the input text into chunks.

<!-- TODO: Add a screenshot of the Character Text Splitter node on the AnswerAgentAI canvas -->
<figure><img src="/.gitbook/assets/screenshots/charactertextsplitter.png" alt="" /><figcaption><p> Character Text Splitter &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different chunk sizes to find the optimal balance between context preservation and processing efficiency.
2. Use chunk overlap to maintain context between chunks, especially when dealing with continuous text.
3. When working with structured text (e.g., code or formatted documents), consider using a custom separator that respects the document's structure.
4. For general text processing, the default separator ("\n\n") works well as it often represents paragraph breaks.

## Troubleshooting

1. If your text isn't splitting as expected, double-check your custom separator (if used) to ensure it matches the character sequence in your text.
2. If chunks are too large or small, adjust the Chunk Size parameter accordingly.
3. If you're losing important context between chunks, try increasing the Chunk Overlap value.

This will create a new Character Text Splitter with custom parameters and use it to split the `longText` into chunks.

Remember that the Character Text Splitter is just one of many text splitting options in AnswerAgentAI. Depending on your specific use case, you might want to explore other splitters like the Token Text Splitter or the Recursive Character Text Splitter for more advanced splitting strategies.
</file>

<file path="sidekick-studio/chatflows/text-splitters/code-text-splitter.md">
---
description: Split code documents based on language-specific syntax
---

# Code Text Splitter

## Overview

The Code Text Splitter is a powerful feature in AnswerAgentAI that allows you to split code documents into smaller, manageable chunks based on language-specific syntax. This tool is particularly useful when working with large codebases or when you need to process code in a more granular manner.

## Key Benefits

-   Language-specific splitting: Accurately splits code based on the syntax of the chosen programming language.
-   Customizable chunk size: Allows you to control the size of each code chunk for optimal processing.
-   Improved code analysis: Enables more efficient analysis and processing of large codebases.

## How to Use

1. In the AnswerAgentAI Studio, locate the "Text Splitters" category in the node palette.
2. Drag and drop the "Code Text Splitter" node onto your canvas.
3. Connect the Code Text Splitter node to your input source (e.g., a document loader or another node that outputs text).
4. Configure the Code Text Splitter node:
   a. Select the appropriate programming language from the "Language" dropdown menu.
   b. (Optional) Adjust the "Chunk Size" to set the number of characters in each chunk (default is 1000).
   c. (Optional) Set the "Chunk Overlap" to determine the number of characters that overlap between chunks (default is 200).
5. Connect the output of the Code Text Splitter to the next node in your workflow.

<!-- TODO: Add a screenshot showing the Code Text Splitter node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/codetextsplitter.png" alt="" /><figcaption><p> Code Text Splitter &#x26; Drop UI</p></figcaption></figure>
## Supported Languages

The Code Text Splitter supports the following programming languages:

-   C++ (cpp)
-   Go (go)
-   Java (java)
-   JavaScript (js)
-   PHP (php)
-   Protocol Buffers (proto)
-   Python (python)
-   reStructuredText (rst)
-   Ruby (ruby)
-   Rust (rust)
-   Scala (scala)
-   Swift (swift)
-   Markdown (markdown)
-   LaTeX (latex)
-   HTML (html)
-   Solidity (sol)

## Tips and Best Practices

1. Choose the correct language: Ensure you select the appropriate programming language for your code to get the most accurate splitting results.
2. Experiment with chunk sizes: Adjust the chunk size based on your specific use case. Smaller chunks may be better for detailed analysis, while larger chunks can provide more context.
3. Consider chunk overlap: Use chunk overlap to maintain context between chunks, especially for languages with complex syntax or long function definitions.
4. Combine with other nodes: Use the Code Text Splitter in combination with other nodes, such as embeddings or vector stores, to create powerful code analysis workflows.

## Troubleshooting

1. Incorrect splitting: If the code isn't split correctly, double-check that you've selected the right programming language.
2. Performance issues: If processing is slow, try increasing the chunk size to reduce the total number of chunks.
3. Loss of context: If important context is lost between chunks, increase the chunk overlap value.

Remember that the effectiveness of the Code Text Splitter can vary depending on the complexity and structure of your code. Experiment with different settings to find the optimal configuration for your specific use case.
</file>

<file path="sidekick-studio/chatflows/text-splitters/html-to-markdown-text-splitter.md">
---
description: Convert HTML to Markdown and split text based on Markdown headers
---

# HtmlToMarkdown Text Splitter

## Overview

The HtmlToMarkdown Text Splitter is a powerful tool in AnswerAgentAI that converts HTML content to Markdown format and then splits the resulting text into smaller, manageable chunks based on Markdown headers. This feature is particularly useful when dealing with large HTML documents or web content that needs to be processed or analyzed in smaller segments.

## Key Benefits

-   Seamlessly converts HTML to Markdown format
-   Intelligently splits text based on Markdown structure
-   Allows for customizable chunk sizes and overlap

## How to Use

1. Locate the HtmlToMarkdown Text Splitter in the Text Splitters category of the AnswerAgentAI Studio.
2. Drag and drop the HtmlToMarkdown Text Splitter node onto your workflow canvas.
3. Connect the node to your HTML input source.
4. Configure the node settings:
    - Set the Chunk Size (optional)
    - Set the Chunk Overlap (optional)
5. Connect the output of the HtmlToMarkdown Text Splitter to subsequent nodes in your workflow.

<!-- TODO: Add a screenshot showing the HtmlToMarkdown Text Splitter node on the canvas with its settings panel open -->
<figure><img src="/.gitbook/assets/screenshots/htmltomarkdown.png" alt="" /><figcaption><p> Code Text Splitter &#x26; Drop UI</p></figcaption></figure>
## Configuration Options

### Chunk Size

-   Description: Number of characters in each chunk
-   Default: 1000
-   Type: Number
-   Optional: Yes

### Chunk Overlap

-   Description: Number of characters to overlap between chunks
-   Default: 200
-   Type: Number
-   Optional: Yes

## Tips and Best Practices

1. Experiment with different chunk sizes to find the optimal balance between context preservation and processing efficiency for your specific use case.
2. Use chunk overlap to ensure that important information isn't lost between chunks, especially when dealing with complex HTML structures.
3. Consider the structure of your HTML content when setting chunk size and overlap. Larger chunk sizes may be more appropriate for content with fewer natural breaks.
4. Remember that the quality of the Markdown conversion depends on the structure of your input HTML. Well-structured HTML will generally result in better Markdown output.

## Troubleshooting

1. If the output chunks are too large or small, adjust the Chunk Size parameter.
2. If you notice important information being cut off between chunks, increase the Chunk Overlap.
3. For HTML content with complex nested structures, you may need to preprocess the HTML or adjust your workflow to handle potential inconsistencies in the Markdown output.

<!-- TODO: Add a screenshot showing an example of the HtmlToMarkdown Text Splitter output, displaying how the HTML has been converted to Markdown and split into chunks -->

By using the HtmlToMarkdown Text Splitter, you can efficiently process HTML content in your AnswerAgentAI workflows, making it easier to analyze, summarize, or perform other operations on web-based text data.
</file>

<file path="sidekick-studio/chatflows/text-splitters/markdown-text-splitter.md">
---
description: Split content based on Markdown headers
---

# Markdown Text Splitter

## Overview

The Markdown Text Splitter is a powerful tool in AnswerAgentAI that allows you to split your content into smaller, more manageable documents based on Markdown headers. This feature is particularly useful when working with large Markdown files or when you need to process Markdown content in chunks.

## Key Benefits

-   Efficiently splits Markdown content into logical sections
-   Preserves the structure and hierarchy of your Markdown documents
-   Customizable chunk size and overlap for fine-tuned control

## How to Use

1. Locate the "Markdown Text Splitter" node in the Text Splitters category of the AnswerAgentAI Studio.
2. Drag and drop the node onto your workflow canvas.
3. Connect the node to your input source (e.g., a Markdown file loader or text input).
4. Configure the node parameters:
    - Chunk Size: Set the number of characters for each chunk (default: 1000)
    - Chunk Overlap: Set the number of characters to overlap between chunks (default: 200)
5. Connect the output of the Markdown Text Splitter to your desired destination node (e.g., a text processing or analysis node).

<!-- TODO: Add a screenshot of the Markdown Text Splitter node on the canvas with its input and output connections -->
<figure><img src="/.gitbook/assets/screenshots/markdowntextsplitter.png" alt="" /><figcaption><p> Markdown Text Splitter &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Experiment with different chunk sizes to find the optimal balance between processing efficiency and maintaining context.
2. Use a larger chunk overlap when dealing with complex Markdown structures to ensure important information isn't split between chunks.
3. Consider the nature of your Markdown content when setting parameters. For example, technical documentation might benefit from larger chunk sizes compared to blog posts.
4. Combine the Markdown Text Splitter with other text processing nodes to create powerful workflows for analyzing and transforming Markdown content.

## Troubleshooting

1. If your output chunks seem too large or small, adjust the Chunk Size parameter accordingly.
2. If you notice important context being lost between chunks, try increasing the Chunk Overlap parameter.
3. Ensure that your input is valid Markdown content. The splitter works best with properly formatted Markdown files.

<!-- TODO: Add a screenshot showing the node's configuration panel with Chunk Size and Chunk Overlap parameters highlighted -->

By using the Markdown Text Splitter in AnswerAgentAI, you can efficiently process large Markdown documents while preserving their structure and context. This node is an essential tool for anyone working with Markdown content in their workflows.
</file>

<file path="sidekick-studio/chatflows/text-splitters/README.md">
---
description: Text Splitter Nodes in AnswerAgentAI
---

# Text Splitter Nodes

## Overview

Text Splitter Nodes in AnswerAgentAI are essential tools for processing and managing large volumes of text data. These nodes allow you to break down long pieces of text into smaller, more manageable chunks while preserving semantic relationships. This process is crucial for various natural language processing tasks and for optimizing the performance of language models.

## Key Benefits

-   Improved processing efficiency for large text documents
-   Enhanced semantic coherence in text analysis tasks
-   Versatile options for different types of text and use cases

## Available Text Splitter Nodes

AnswerAgentAI offers several Text Splitter Nodes, each designed for specific use cases. Here's a brief explanation of each Text Splitter Node and when to use them:

1. [Character Text Splitter](character-text-splitter.md):

    - Splits text based on a specified number of characters
    - Use when you need a simple, length-based split without considering content structure

2. [Code Text Splitter](code-text-splitter.md):

    - Specialized for splitting programming code
    - Use when working with source code files or code snippets to maintain code structure

3. [Html-To-Markdown Text Splitter](html-to-markdown-text-splitter.md):

    - Converts HTML to Markdown before splitting
    - Use when processing HTML content and you prefer working with Markdown format

4. [Markdown Text Splitter](markdown-text-splitter.md):

    - Splits Markdown text while preserving its structure
    - Use when working with Markdown documents to maintain formatting and hierarchy

5. [Recursive Character Text Splitter](recursive-character-text-splitter.md):

    - Splits text recursively based on multiple delimiters
    - Use for complex documents with varying structures or when you need fine-grained control over splitting

6. [Token Text Splitter](token-text-splitter.md):
    - Splits text based on the number of tokens
    - Use when working with language models that have token limits, ensuring consistent input sizes

Each of these nodes offers unique capabilities for different text processing scenarios. Choose the appropriate splitter based on your specific text format and processing requirements.
</file>

<file path="sidekick-studio/chatflows/text-splitters/recursive-character-text-splitter.md">
---
description: Recursive Character Text Splitter for Advanced Document Chunking
---

# Recursive Character Text Splitter

## Overview

The Recursive Character Text Splitter is an advanced text splitting tool in AnswerAgentAI that allows you to split documents recursively using different characters or custom separators. This splitter offers more sophisticated chunking capabilities compared to the simpler Character Text Splitter, giving you greater control over how your text is divided.

## Key Benefits

-   Recursive splitting for more semantically coherent chunks
-   Customizable separators for tailored text division
-   Improved context preservation in complex documents

## How to Use

1. In the AnswerAgentAI Studio, add the "Recursive Character Text Splitter" node to your canvas.
2. Configure the following parameters:

    - Chunk Size: Set the maximum number of characters per chunk (default: 1000)
    - Chunk Overlap: Specify the number of characters to overlap between chunks (default: 200)
    - Custom Separators: (Optional) Define an array of custom separators

3. Connect the splitter to your document input and subsequent processing nodes.

<!-- TODO: Add a screenshot of the Recursive Character Text Splitter node configuration in the AnswerAgentAI Studio -->
<figure><img src="/.gitbook/assets/screenshots/recursivetextsplitter.png" alt="" /><figcaption><p> Recursive Text Splitter &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Experiment with different chunk sizes and overlaps to find the optimal balance for your specific use case.
2. Use custom separators when dealing with structured text or documents with clear section demarcations.
3. Consider the nature of your text when choosing separators (e.g., newlines for paragraphs, special characters for code blocks).

## Recursive Splitting vs. Character Text Splitter

The Recursive Character Text Splitter differs from the regular Character Text Splitter in several key ways:

1. **Recursive Approach**: This splitter uses a hierarchical approach to text division. It starts with the first separator (default: "\n\n"), then moves to the next ("\n"), and finally to a space (" "). This method helps maintain the semantic structure of the document better than a simple character count split.

2. **Custom Separators**: You can define your own array of separators, allowing for highly customized text division based on your document's structure.

3. **Context Preservation**: By using meaningful separators, this splitter is more likely to keep related information together, preserving context within chunks.

4. **Flexibility**: The recursive approach adapts better to varying document structures, making it more versatile for different types of text.

## Using Custom Separators for Ultimate Control

The Custom Separators feature gives you precise control over how your text is chunked:

1. **Tailored Chunking**: Define separators that match your document's structure (e.g., ["##", "###"] for Markdown headers).

2. **Preserve Semantic Units**: Keep logical sections together by using appropriate separators (e.g., ["\n\n", "\n", " "] for paragraphs and sentences).

3. **Handle Special Formats**: Easily chunk code blocks, CSV data, or any structured text by specifying relevant delimiters.

4. **Optimize for Analysis**: Create chunks that align with your analytical needs, improving the effectiveness of subsequent processing steps.

Example custom separator input:

```json
["##", "###", "\n\n", "\n", " "]
```

This configuration would split the text first by "##", then "###", followed by double newlines, single newlines, and finally spaces.

By leveraging the Recursive Character Text Splitter with custom separators, you gain ultimate control over how your documents are processed, ensuring that the resulting chunks are optimally suited for your specific use case in AnswerAgentAI.
</file>

<file path="sidekick-studio/chatflows/text-splitters/token-text-splitter.md">
---
description: Split text into chunks using token-based methods
---

# Token Text Splitter

## Overview

The Token Text Splitter is a powerful tool in AnswerAgentAI that allows you to split long pieces of text into smaller, more manageable chunks. This splitter works by first converting the text into BPE (Byte Pair Encoding) tokens, then splitting these tokens into chunks, and finally converting the tokens within each chunk back into text.

## Key Benefits

-   Preserves semantic meaning by splitting text based on tokens rather than arbitrary character counts
-   Offers flexibility with various encoding options to suit different types of text
-   Allows customization of chunk size and overlap for optimal text processing

## How to Use

1. Add the Token Text Splitter node to your AnswerAgentAI workflow canvas.
2. Configure the node with the following parameters:

    a. Encoding Name:

    - Select the appropriate encoding from the dropdown menu (gpt2, r50k_base, p50k_base, p50k_edit, or cl100k_base).
    - The default encoding is 'gpt2'.

    b. Chunk Size:

    - Enter the desired number of characters for each chunk.
    - The default value is 1000 characters.

    c. Chunk Overlap:

    - Specify the number of characters to overlap between chunks.
    - The default value is 200 characters.

3. Connect the Token Text Splitter node to your text input source and any subsequent nodes that will process the split text.

<!-- TODO: Add a screenshot showing the Token Text Splitter node on the canvas with its configuration panel open -->
<figure><img src="/.gitbook/assets/screenshots/tokentextsplitter.png" alt="" /><figcaption><p> Token Text Splitter &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Choose the appropriate encoding based on your text content and the model you're using in your workflow.
2. Experiment with different chunk sizes and overlaps to find the optimal balance between context preservation and processing efficiency.
3. Consider the requirements of the nodes that will process the split text when determining your chunk size.
4. For most use cases, the default 'gpt2' encoding works well, but specialized tasks might benefit from other encodings.

## Troubleshooting

1. If your text is not splitting as expected, double-check the chunk size and overlap values to ensure they are appropriate for your content.
2. If you encounter performance issues, try reducing the chunk size or using a different encoding that may be more efficient for your specific text.
3. Ensure that the encoding you choose is compatible with the models or processes you'll be using later in your workflow.

By using the Token Text Splitter, you can efficiently prepare large volumes of text for further processing in your AnswerAgentAI workflows, ensuring that your AI models receive optimally sized input for the best possible performance.
</file>

<file path="sidekick-studio/chatflows/tools/bravesearch-api.md">
---
description: Access Brave search results in real-time with the BraveSearch API Tool
---

# BraveSearch API Tool

## Overview

The BraveSearch API Tool allows you to integrate Brave's search capabilities into your AnswerAgentAI workflows. This tool provides real-time access to Brave search results, enabling you to retrieve up-to-date information on various topics directly within your projects.

## Key Benefits

-   Access to current and relevant search results from Brave
-   Easy integration with AnswerAgentAI workflows
-   Real-time information retrieval for enhanced decision-making

## How to Use

1. Add the BraveSearch API Tool to your canvas in the AnswerAgentAI Studio.

<!-- TODO: Screenshot of adding BraveSearch API Tool to the canvas -->
<figure><img src="/.gitbook/assets/screenshots/braveapi.png" alt="" /><figcaption><p>BraveSearch API Tool  &#x26; Drop UI</p></figcaption></figure>

2. Connect your Brave Search API credential:
    - Click on the BraveSearch API Tool node
    - In the right sidebar, click on "Add New Credential"
    - Select "BraveSearch API" from the dropdown
    - Enter your Brave Search API key
    - Click "Save" to store your credential

<!-- TODO: Screenshot of adding BraveSearch API credential -->
<figure><img src="/.gitbook/assets/screenshots/bravesearchapi.png" alt="" /><figcaption><p>BraveSearch API Credentials  &#x26; Drop UI</p></figcaption></figure>

3. The BraveSearch API Tool is now ready to use in your workflow. You can connect it to other nodes that require search functionality or real-time information.

4. To use the tool in your workflow, simply pass a search query as input to the BraveSearch API Tool node. The tool will return search results in JSON format.

## Tips and Best Practices

1. Use specific and targeted search queries to get the most relevant results.
2. Consider combining the BraveSearch API Tool with other nodes to process and analyze the search results further.
3. Be mindful of API usage limits and implement appropriate error handling in your workflows.

## Troubleshooting

1. **API Key Issues**: If you encounter authentication errors, double-check that you've entered the correct Brave Search API key in your credential settings.

2. **No Results**: If your search query returns no results, try broadening your search terms or check if the topic is too recent or obscure.

3. **Rate Limiting**: If you experience rate limiting issues, consider implementing a delay between requests or optimizing your workflow to reduce the number of API calls.

Remember that the BraveSearch API Tool is particularly useful for retrieving current information and can be a valuable asset in workflows that require up-to-date data from the web.
</file>

<file path="sidekick-studio/chatflows/tools/calculator.md">
---
description: Perform calculations on responses using the Calculator tool
---

# Calculator Tool

## Overview

The Calculator tool in AnswerAgentAI allows you to perform mathematical calculations within your workflows. This powerful feature enables you to process numerical data, solve equations, and incorporate mathematical operations into your AI-powered applications.

## Key Benefits

-   Perform complex calculations without leaving the AnswerAgentAI environment
-   Enhance your AI workflows with precise mathematical operations
-   Easily integrate numerical processing into your chatbots and AI agents

## How to Use

1. Locate the Calculator tool in the Tools section of the node library.
2. Drag and drop the Calculator node onto your canvas.
3. Connect the Calculator node to other nodes in your workflow where mathematical operations are needed.
4. The Calculator will automatically process numerical inputs and perform calculations as required.

<!-- TODO: Add a screenshot of the Calculator node on the canvas, connected to other nodes -->
<figure><img src="/.gitbook/assets/screenshots/calculator node.png" alt="" /><figcaption><p>Calculator node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Use the Calculator tool in combination with other nodes to create more complex, data-driven workflows.
-   When designing chatbots or AI agents, incorporate the Calculator tool to handle user queries that involve mathematical operations.
-   Remember that the Calculator tool can handle a wide range of operations, including basic arithmetic, trigonometric functions, and more advanced mathematical concepts.

## Troubleshooting

-   If you're not getting the expected results, double-check the input format and ensure that the mathematical expressions are correctly formatted.
-   For complex calculations, consider breaking them down into smaller steps using multiple Calculator nodes for better clarity and easier debugging.

The Calculator tool in AnswerAgentAI is a versatile addition to your toolkit, allowing you to seamlessly integrate mathematical operations into your AI workflows. Whether you're building a financial analysis bot or a scientific calculator application, this tool provides the computational power you need to deliver accurate and efficient results.
</file>

<file path="sidekick-studio/chatflows/tools/chain-tool.md">
---
description: Use a chain as an allowed tool for agents
---

# Chain Tool

## Overview

The Chain Tool node allows you to use a chain as an allowed tool for agents in AnswerAgentAI. This feature enables you to incorporate complex chains of operations as tools that agents can utilize, enhancing their capabilities and allowing for more sophisticated workflows.

## Key Benefits

-   Integrate complex chains as tools for agents
-   Customize tool behavior with flexible configuration options
-   Enhance agent capabilities with specialized chains

## How to Use

1. Locate the Chain Tool node in the Tools category of the AnswerAgentAI Studio.
2. Drag and drop the Chain Tool node onto your canvas.
3. Configure the node by providing the following information:
    - Chain Name: Enter a unique name for your chain tool (e.g., "state-of-union-qa").
    - Chain Description: Provide a brief description of what the chain tool does and when it should be used.
    - Return Direct (optional): Toggle this option if you want the tool to return results directly.
    - Base Chain: Connect a compatible Base Chain node to serve as the foundation for your chain tool.

<!-- TODO: Add a screenshot showing the Chain Tool node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/.png" alt="" /><figcaption><p>Chain Tool node  &#x26; Drop UI</p></figcaption></figure>

4. Connect the Chain Tool node to your agent node or other relevant nodes in your workflow.
5. Save and deploy your workflow to make the chain tool available for use by agents.

## Tips and Best Practices

-   Choose descriptive names and provide clear descriptions for your chain tools to help agents understand when and how to use them effectively.
-   Consider using the "Return Direct" option for chains that produce final results, allowing agents to use the output directly without additional processing.
-   Experiment with different types of chains as tools to expand the capabilities of your agents and create more versatile workflows.
-   Regularly review and update your chain tools to ensure they remain relevant and effective for your agents' tasks.

## Troubleshooting

-   If your chain tool is not being recognized by the agent, double-check that the Chain Name is correctly specified and matches the name used in your agent's configuration.
-   Ensure that the Base Chain connected to the Chain Tool node is compatible and properly configured.
-   If the chain tool is not producing the expected results, review the Chain Description to make sure it accurately represents the tool's functionality.

By utilizing the Chain Tool node in AnswerAgentAI, you can create powerful and flexible agents capable of leveraging complex chains as tools, opening up new possibilities for automation and problem-solving in your workflows.
</file>

<file path="sidekick-studio/chatflows/tools/chatflow-tool.md">
---
description: Use another chatflow as a tool in your workflow
---

# Chatflow Tool

## Overview

The Chatflow Tool allows you to use another chatflow as a tool within your current workflow. This feature enables you to leverage existing chatflows and incorporate their functionality into your current project, enhancing modularity and reusability.

## Key Benefits

-   Reuse existing chatflows as tools in new workflows
-   Enhance modularity and reduce redundancy in your projects
-   Easily integrate complex functionalities from other chatflows

## How to Use

1. Drag and drop the Chatflow Tool node onto your canvas.
2. Configure the node with the following settings:

    a. Select Chatflow: Choose the chatflow you want to use as a tool from the dropdown menu.

    b. Tool Name: Enter a name for this tool (e.g., "State of the Union QA").

    c. Tool Description: Provide a description of what the tool does. This helps the AI determine when to use this tool.

    d. Use Question from Chat: Toggle this option if you want to use the question from the chat as input to the selected chatflow.

    e. Custom Input: If not using the question from chat, you can specify a custom input here. Leave it empty to let the AI decide the input.

3. Optionally, connect a credential if you need to use API authentication for the selected chatflow.

4. Connect the Chatflow Tool node to other nodes in your workflow as needed.

<!-- TODO: Add a screenshot showing a configured Chatflow Tool node on the canvas -->

<figure><img src="/.gitbook/assets/screenshots/.png" alt="" /><figcaption><p>Chatflow Tool node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose descriptive names and provide detailed descriptions for your Chatflow Tools to help the AI understand when to use them.
2. Avoid using a chatflow as a tool within itself to prevent infinite loops.
3. Consider using the "Use Question from Chat" option when you want the tool to dynamically respond to user input.
4. If you need consistent input for the tool, use the "Custom Input" field to provide a specific prompt or question.

## Troubleshooting

1. If the Chatflow Tool is not executing:

    - Ensure that the selected chatflow exists and is accessible.
    - Check if the required credentials are properly configured.

2. If you receive an error about calling the same chatflow:

    - Verify that you're not trying to use the current chatflow as a tool within itself.

3. If the tool's output is unexpected:
    - Review the description and input settings to make sure they align with the selected chatflow's purpose.
    - Test the selected chatflow independently to ensure it's functioning as expected.

Remember that the Chatflow Tool allows you to leverage the power of existing chatflows within new workflows, promoting code reuse and modular design in your AnswerAgentAI projects.
</file>

<file path="sidekick-studio/chatflows/tools/custom-tool.md">
---
description: Create custom tools to extend AnswerAgentAI's capabilities
---

# Custom Tool

Custom Tools allow you to add custom JavaScript functions to interact with APIs that are not natively supported by AnswerAgentAI. This feature enables you to extend the functionality of your workflows and integrate with a wide range of external services.

## Overview

The Custom Tool node allows you to write JavaScript code that can be executed as part of your workflow. This is particularly useful when you need to interact with APIs or services that don't have a pre-built node in AnswerAgentAI.

## Key Benefits

-   Extend AnswerAgentAI's functionality with custom integrations
-   Interact with any API or service using JavaScript
-   Flexibility to implement complex logic and data processing

## How to Use

1. Add a Custom Tool node to your canvas.
2. Configure the tool with a name, description, and input schema (if required).
3. Write your JavaScript function in the provided code editor.
4. Connect the Custom Tool node to other nodes in your workflow.

## Example: Fetching Data from an API

Here's a simple example of how to create a Custom Tool that fetches data from a public API:

```javascript
const fetch = require('node-fetch')

const url = 'https://api.example.com/data'

try {
    const response = await fetch(url)
    const data = await response.json()
    return JSON.stringify(data)
} catch (error) {
    console.error('Error fetching data:', error)
    return JSON.stringify({ error: 'Failed to fetch data' })
}
```

This example uses the `node-fetch` library, which is available by default in Custom Tools. It fetches data from a hypothetical API and returns the result as a JSON string.

## Tips and Best Practices

1. Use `try-catch` blocks to handle errors gracefully.
2. Return data as a string to ensure compatibility with other nodes.
3. Use `console.log()` for debugging, but remember to remove or comment out these lines in production.
4. When working with APIs that require authentication, consider using environment variables to store sensitive information like API keys.

## Troubleshooting

-   If your Custom Tool is not executing, check that it's properly connected in the workflow.
-   Ensure that any external libraries you're using are supported by AnswerAgentAI. You can find a list of supported libraries in the AnswerAgentAI documentation.
-   If you're getting unexpected results, use `console.log()` statements to debug your code and check the AnswerAgentAI logs for output.

For more detailed information on building custom tools, including how to work with input schemas and advanced use cases, please refer to the [Interacting with APIs documentation](../../../developers/use-cases/interacting-with-api.md).

<!-- TODO: Add a screenshot showing the Custom Tool node configuration interface -->
<figure><img src="/.gitbook/assets/screenshots/.png" alt="" /><figcaption><p>Custom Tool node  &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/tools/dalle-image.md">
---
description: Create images using Dall-E OpenAI model
---

# Dall-E Post Tool

## Overview

The Dall-E Post Tool allows you to generate images using OpenAI's Dall-E AI model directly within your AnswerAgentAI workflows. This powerful feature enables you to create unique, AI-generated images based on text prompts, enhancing your creative capabilities and visual content production.

## Key Benefits

-   Generate custom images on-demand using natural language prompts
-   Seamlessly integrate AI-powered image creation into your workflows
-   Access cutting-edge image generation technology without leaving the AnswerAgentAI platform

## How to Use

1. Add the Dall-E Post Tool to your canvas in the AnswerAgentAI Studio.
2. Connect your OpenAI API credential to the tool.
3. Configure the tool's inputs:
    - **Prompt**: Enter the text description of the image you want to generate.
    - **Model**: Choose the model. Select **dall-e-3** for a direct URL response or **gpt-image-1** to upload the generated image to your S3 storage and get its URL.
4. Connect the tool to other nodes in your workflow as needed.
5. Run your workflow to generate the image based on your prompt.

<!-- TODO: Add a screenshot of the Dall-E Post Tool node on the canvas with its configuration panel open -->
<figure><img src="/.gitbook/assets/screenshots/dallepost.png" alt="" /><figcaption><p> Dall-E Post Tool node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Be specific and detailed in your prompts for better results.
2. Experiment with different phrasings to fine-tune your generated images.
3. Consider using the output of this tool as input for other nodes that handle image processing or analysis.
4. Remember that image generation may take a few seconds, so account for this in your workflow timing.

## Troubleshooting

1. **Error: "Invalid API key"**

    - Ensure that you have correctly entered your OpenAI API key in the credential settings.
    - Verify that your OpenAI account has access to the Dall-E API.

2. **Image generation fails or returns an error**

    - Check your prompt for any prohibited content or violations of OpenAI's usage policies.
    - Ensure you have sufficient API credits in your OpenAI account.

3. **Unexpected or low-quality image results**
    - Try refining your prompt with more specific details or adjusting the language.
    - Experiment with different model versions if available.

Remember that the quality and relevance of the generated images depend largely on the clarity and specificity of your prompts. Practice and experimentation will help you get the best results from the Dall-E Post Tool in your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/tools/exa-search.md">
---
description: Exa Search Tool for AnswerAgentAI
---

# Exa Search

## Overview

The Exa Search tool is a powerful search engine wrapper designed specifically for use by Language Models (LLMs) in AnswerAgentAI. It allows you to perform advanced searches and retrieve highly relevant results from the web.

## Key Benefits

-   Optimized for LLM queries, providing more accurate and relevant search results
-   Flexible search options, including keyword, neural, and magic search types
-   Ability to filter results by domains, dates, and categories for more targeted searches

## How to Use

1. Add the Exa Search node to your AnswerAgentAI canvas.
2. Configure the node settings:
    - Tool Description: Customize the description of what the tool does (optional).
    - Number of Results: Specify how many search results to return (default is 10).
    - Search Type: Choose between keyword, neural, or magic search.
    - Use Auto Prompt: Enable to convert your query to an Exa-optimized query automatically.
    - Category: Select a specific data category to focus your search (optional).
    - Include/Exclude Domains: Specify domains to include or exclude from the search results.
    - Date Filters: Set crawl and published date ranges to refine your search.
3. Connect the Exa Search node to your workflow.
4. Run your workflow, and the Exa Search tool will provide search results based on your configuration.

<!-- TODO: Add a screenshot of the Exa Search node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/exasearch.png" alt="" /><figcaption><p> Exa Search node  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use the "magic" search type when you're unsure whether keyword or neural search would be more effective.
2. Leverage the category filter to focus on specific types of content, such as company information, research papers, or news articles.
3. Utilize the domain inclusion/exclusion feature to narrow down your search to trusted sources or exclude irrelevant websites.
4. Experiment with different date ranges to find the most up-to-date or historical information as needed.

## Troubleshooting

1. If you're not getting enough results, try increasing the "Number of Results" setting. Note that the maximum is 10 for basic plans, but can be higher for custom plans.
2. If the search results are not relevant, try adjusting the search type or enabling the "Use Auto Prompt" option for better query optimization.
3. Ensure that your Exa Search API key is correctly set up in the AnswerAgentAI credentials manager.

<!-- TODO: Add a screenshot showing where to input the Exa Search API key in the credentials manager -->
<figure><img src="/.gitbook/assets/screenshots/exaapi.png" alt="" /><figcaption><p> Exa Search API  &#x26; Drop UI</p></figcaption></figure>
Remember that the Exa Search tool is designed to work seamlessly with LLMs, making it an excellent choice for AI-driven search tasks in your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/tools/google-calendar.md">
---
description: Manage Google Calendar events with AnswerAgentAI tools
---

# Google Calendar Tools

## Overview

The Google Calendar Tools suite enables you to create, update, and delete calendar events directly from your AnswerAgentAI workflows. These powerful integrations allow you to build AI assistants that can manage schedules, book appointments, and automate calendar management tasks.

## Available Tools

| Tool                      | Purpose                            | Key Features                                                |
| ------------------------- | ---------------------------------- | ----------------------------------------------------------- |
| **Create Calendar Event** | Add new events to Google Calendar  | Conflict detection, attendee management, multiple calendars |
| **Update Calendar Event** | Modify existing calendar events    | Partial updates, attendee notifications, schedule changes   |
| **Delete Calendar Event** | Remove events from Google Calendar | Safe deletion with confirmation, attendee notifications     |

## Prerequisites

Before using Google Calendar Tools, ensure you have:

1. **Google OAuth Configured** - Follow the [Google OAuth Setup Guide](../../../developers/authorization/google-oauth.md)
2. **Required Scopes** - Your OAuth application must include:
    - `https://www.googleapis.com/auth/calendar`
    - `https://www.googleapis.com/auth/calendar.events`

## Create Calendar Event Tool

### Overview

The Create Calendar Event tool allows you to add new events to Google Calendar with comprehensive options for scheduling, attendees, and event details.

### How to Use

1. **Add the Tool**

    - Locate "Create Google Calendar Event" in the Tools section
    - Drag the node onto your canvas
    - Connect your Google OAuth credential

2. **Select Calendar**

    - Choose from available calendars in your Google account
    - Defaults to "Primary" calendar if none selected
    - The tool will load all accessible calendars

3. **Event Input Format**
   When the tool is called, provide event details in this format:
    ```
    Title: [event title]
    Start: [YYYY-MM-DD HH:mm]
    End: [YYYY-MM-DD HH:mm]
    Description: (optional) [event description]
    Location: (optional) [event location]
    Attendees: (optional) [comma-separated email addresses]
    Calendar: (optional) [calendar ID, defaults to selected calendar]
    ```

### Input Examples

#### Basic Event

```
Title: Team Meeting
Start: 2024-01-15 14:00
End: 2024-01-15 15:00
```

#### Detailed Event with Attendees

```
Title: Product Launch Planning
Start: 2024-01-20 10:00
End: 2024-01-20 11:30
Description: Discuss Q1 product launch strategy and timeline
Location: Conference Room A
Attendees: john@company.com, sarah@company.com, mike@company.com
```

#### Event in Specific Calendar

```
Title: Client Call
Start: 2024-01-18 09:00
End: 2024-01-18 09:30
Calendar: client-meetings@company.com
Description: Monthly check-in with ABC Corp
```

### Features

-   **Conflict Detection**: Warns if events overlap with existing calendar entries
-   **Attendee Management**: Automatically sends invitations to specified attendees
-   **Multiple Calendars**: Support for shared and personal calendars
-   **Error Handling**: Graceful handling of scheduling conflicts and API limits

## Update Calendar Event Tool

### Overview

The Update Calendar Event tool enables modification of existing calendar events, including time changes, attendee updates, and detail modifications.

### How to Use

1. **Add the Tool**

    - Locate "Update Google Calendar Event" in the Tools section
    - Connect your Google OAuth credential
    - Select the target calendar

2. **Update Input Format**
    ```
    Event ID: [event id] (required)
    Title: (optional) [new event title]
    Start: (optional) [YYYY-MM-DD HH:mm]
    End: (optional) [YYYY-MM-DD HH:mm]
    Description: (optional) [new description]
    Location: (optional) [new location]
    Attendees: (optional) [comma-separated email addresses]
    Calendar: (optional) [calendar ID]
    ```

### Update Examples

#### Reschedule Event

```
Event ID: abc123def456
Start: 2024-01-15 16:00
End: 2024-01-15 17:00
```

#### Update Attendees Only

```
Event ID: xyz789uvw012
Attendees: john@company.com, sarah@company.com, new-member@company.com
```

#### Complete Event Update

```
Event ID: def456ghi789
Title: Updated Meeting Title
Start: 2024-01-16 14:00
End: 2024-01-16 15:30
Description: Updated agenda and objectives
Location: Virtual Meeting Room
Attendees: team@company.com
```

### Features

-   **Partial Updates**: Only modify specified fields, leave others unchanged
-   **Attendee Notifications**: Automatically notify attendees of changes
-   **Validation**: Ensures event exists before attempting updates
-   **Conflict Checking**: Warns about scheduling conflicts when changing times

## Delete Calendar Event Tool

### Overview

The Delete Calendar Event tool safely removes events from Google Calendar with proper notifications and confirmation options.

### How to Use

1. **Add the Tool**

    - Locate "Delete Google Calendar Event" in the Tools section
    - Connect your Google OAuth credential
    - Select the target calendar

2. **Delete Input Format**
    ```
    Event ID: [event id] (required)
    Calendar: (optional) [calendar ID, defaults to primary]
    Confirm: (optional) [true/false, defaults to false]
    ```

### Delete Examples

#### Simple Deletion

```
Event ID: abc123def456
```

#### Delete from Specific Calendar

```
Event ID: xyz789uvw012
Calendar: team-events@company.com
```

#### Confirmed Deletion

```
Event ID: def456ghi789
Confirm: true
```

### Features

-   **Safe Deletion**: Verifies event exists before deletion
-   **Attendee Notifications**: Automatically notifies attendees of cancellation
-   **Confirmation Option**: Optional confirmation step for added safety
-   **Error Handling**: Graceful handling of missing events or permission issues

## Common Use Cases

### AI Scheduling Assistant

```yaml
Workflow: 1. User requests meeting scheduling
    2. AI checks availability using calendar API
    3. Creates event with Create Calendar Event tool
    4. Confirms details with user

Tools Used: Create Calendar Event
Benefits: Automated scheduling, conflict detection
```

### Event Management Bot

```yaml
Workflow: 1. User requests event changes
    2. AI identifies event using natural language
    3. Updates event using Update Calendar Event tool
    4. Confirms changes made

Tools Used: Update Calendar Event, Delete Calendar Event
Benefits: Natural language event management
```

### Meeting Reminder System

```yaml
Workflow: 1. Scheduled task checks upcoming events
    2. AI analyzes event details
    3. Updates events with reminders
    4. Sends notifications to attendees

Tools Used: Update Calendar Event
Benefits: Automated meeting preparation
```

## Input Processing Details

### Time Format Requirements

-   **Format**: YYYY-MM-DD HH:mm
-   **Timezone**: Uses your Google Calendar default timezone
-   **Examples**:
    -   `2024-01-15 14:30` (2:30 PM)
    -   `2024-12-25 09:00` (9:00 AM)

### Attendee Email Format

-   **Single**: `john@company.com`
-   **Multiple**: `john@company.com, sarah@company.com, mike@company.com`
-   **Validation**: Tool validates email format
-   **Invitations**: Sent automatically when attendees are specified

### Event ID Requirements

-   **Source**: Obtained from Google Calendar or previous tool operations
-   **Format**: Google Calendar unique identifier
-   **Example**: `abc123def456ghi789`
-   **Finding IDs**: Use calendar list operations or event creation responses

## Best Practices

### Error Prevention

1. **Time Validation**

    - Ensure end time is after start time
    - Use 24-hour format for clarity
    - Check timezone settings in Google Calendar

2. **Attendee Management**

    - Validate email addresses before sending
    - Consider attendee privacy and permissions
    - Use distribution lists for large meetings

3. **Calendar Selection**
    - Verify calendar access permissions
    - Use appropriate calendar for event type
    - Check calendar sharing settings

### Performance Optimization

1. **Batch Operations**

    - Group related calendar operations
    - Avoid rapid sequential API calls
    - Monitor API quota usage

2. **Conflict Handling**

    - Check for conflicts before creating events
    - Provide alternative time suggestions
    - Handle rate limiting gracefully

3. **User Experience**
    - Provide clear confirmation messages
    - Handle errors with helpful explanations
    - Offer retry options for failed operations

## Troubleshooting

### Common Issues

1. **"Calendar not found"**

    - **Solution:** Verify calendar ID is correct
    - **Check:** Ensure calendar is accessible to the authenticated user
    - **Try:** Use "primary" for default calendar

2. **"Event not found"**

    - **Solution:** Verify event ID is correct and current
    - **Check:** Event hasn't been deleted by another process
    - **Note:** Event IDs are case-sensitive

3. **"Insufficient permissions"**

    - **Solution:** Re-authorize with calendar scopes
    - **Check:** OAuth application includes calendar permissions
    - **Verify:** User has edit access to the target calendar

4. **"Invalid time format"**

    - **Solution:** Use YYYY-MM-DD HH:mm format
    - **Check:** Ensure times are valid (e.g., not 25:00)
    - **Note:** Use 24-hour format only

5. **"Rate limit exceeded"**
    - **Solution:** Reduce frequency of calendar operations
    - **Wait:** Respect API quotas and retry after delays
    - **Optimize:** Batch operations when possible

### Integration Issues

1. **Calendar Sync Problems**

    - **Check:** Google Calendar is accessible in web interface
    - **Verify:** Sync settings in Google Calendar
    - **Try:** Refresh calendar data in Google Calendar

2. **Timezone Confusion**

    - **Solution:** Use consistent timezone settings
    - **Check:** Default timezone in Google Calendar
    - **Note:** Tool uses account's default timezone

3. **Attendee Notification Issues**
    - **Verify:** Email addresses are correct
    - **Check:** Spam/junk folders for invitations
    - **Note:** Some organizations block external calendar invites

## Integration Examples

### Customer Service Booking

```
Flow: Customer request  AI processes  Creates appointment  Confirms with customer
Tools: Create Calendar Event
Features: Automatic scheduling, conflict avoidance
```

### Meeting Management Assistant

```
Flow: Meeting change request  AI identifies event  Updates details  Notifies participants
Tools: Update Calendar Event, Delete Calendar Event
Features: Natural language processing, attendee management
```

### Automated Calendar Cleanup

```
Flow: Scheduled cleanup  AI reviews events  Deletes outdated events  Reports changes
Tools: Delete Calendar Event
Features: Automated maintenance, bulk operations
```

## Security and Privacy

### Data Protection

-   **Minimal Access**: Only request necessary calendar permissions
-   **Event Privacy**: Respect calendar privacy settings
-   **Attendee Data**: Handle attendee information according to privacy policies

### Access Control

-   **Credential Security**: Protect OAuth credentials
-   **Permission Scope**: Use minimum required scopes
-   **User Consent**: Ensure users understand calendar access

## API Reference

The Google Calendar tools use the Calendar API v3:

-   **Events.insert** - Create new events
-   **Events.update** - Modify existing events
-   **Events.delete** - Remove events
-   **Events.get** - Retrieve event details
-   **CalendarList.list** - Get available calendars

## Next Steps

After setting up Google Calendar tools:

1. **Test Basic Operations** - Create, update, and delete test events
2. **Build Workflows** - Integrate with chat models for natural language processing
3. **Add Validation** - Implement input validation and error handling
4. **Scale Usage** - Monitor API quotas and optimize for production use

---

**Related Documentation:**

-   [Google OAuth Setup](../../../developers/authorization/google-oauth.md)
-   [Custom Tools](./custom-tool.md)
-   [Chain Tools](./chain-tool.md)
</file>

<file path="sidekick-studio/chatflows/tools/google-custom-search.md">
---
description: Google Custom Search - Access Google search results in real-time
---

# Google Custom Search

## Overview

The Google Custom Search tool allows you to integrate Google's powerful search capabilities into your AnswerAgentAI workflows. This tool provides real-time access to Google search results, enabling your agents to retrieve up-to-date information from the web.

## Key Benefits

-   Access to Google's vast index of web content
-   Real-time search results for the most current information
-   Customizable search parameters to refine results

## How to Use

1. Add the Google Custom Search node to your canvas in the AnswerAgentAI Studio.
2. Connect your Google Custom Search API credentials:
    - Click on the node to open its settings.
    - Under the "Connect Credential" section, select or add your Google Custom Search API credentials.

<!-- TODO: Add a screenshot of the Google Custom Search node settings, highlighting the credential selection -->
<figure><img src="/.gitbook/assets/screenshots/googlecustomsearchpng" alt="" /><figcaption><p> Google Custom Search node   &#x26; Drop UI</p></figcaption></figure>

3. Connect the Google Custom Search node to other nodes in your workflow where you want to use web search capabilities.

## Tips and Best Practices

-   Use specific search queries to get more relevant results.
-   Combine the Google Custom Search tool with text processing nodes to extract the most relevant information from search results.
-   Be mindful of API usage limits and costs associated with the Google Custom Search API.

## Troubleshooting

-   If you're not getting any results, double-check your API credentials and ensure they are correctly set up in AnswerAgentAI.
-   Verify that your Google Custom Search Engine (CSE) is properly configured in the Google Programmable Search Engine control panel.
-   If you're experiencing rate limiting issues, consider implementing a delay between searches or upgrading your API plan.

<!-- TODO: Add a screenshot showing where to find API usage information in the Google Cloud Console -->

Remember that the Google Custom Search tool provides a powerful way to incorporate web search capabilities into your AnswerAgentAI workflows, allowing your agents to access a wealth of information from the internet in real-time.
</file>

<file path="sidekick-studio/chatflows/tools/openapi-toolkit.md">
---
description: Load and use OpenAPI specifications as tools in AnswerAgentAI workflows
---

# OpenAPI Toolkit

## Overview

The OpenAPI Toolkit node allows you to load an OpenAPI specification and use it as a set of tools in your AnswerAgentAI workflows. This feature enables your agents to interact with APIs defined by OpenAPI (formerly known as Swagger) specifications, expanding their capabilities to include external services and data sources.

## Key Benefits

-   Easily integrate external APIs into your AnswerAgentAI workflows
-   Automatically generate tools based on API endpoints and operations
-   Enhance your agents' abilities with access to a wide range of web services

## How to Use

1. Add the OpenAPI Toolkit node to your canvas in the AnswerAgentAI Studio.
2. Connect a Language Model node to the OpenAPI Toolkit node.
3. Upload your OpenAPI specification YAML file.
4. (Optional) Connect an OpenAPI Auth credential if your API requires authentication.
5. Configure any additional settings as needed.
6. Connect the OpenAPI Toolkit node to other nodes in your workflow to utilize the generated tools.

<!-- TODO: Add a screenshot of the OpenAPI Toolkit node configuration in the AnswerAgentAI Studio -->
<figure><img src="/.gitbook/assets/screenshots/openapitoolkit.png" alt="" /><figcaption><p> OpenAPI Toolkit node configuration   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Ensure your OpenAPI specification is valid and up-to-date for the best results.
-   Use descriptive names for your API operations to make it easier for agents to understand and use the tools.
-   If your API requires authentication, create and use an OpenAPI Auth credential to securely store your API tokens.
-   Test the generated tools individually before incorporating them into more complex workflows.

## Troubleshooting

1. **Failed to load OpenAPI spec**:

    - Make sure your YAML file is properly formatted and contains a valid OpenAPI specification.
    - Check that the file is not corrupted and can be read correctly.

2. **Authentication errors**:

    - Verify that you've provided the correct API token in the OpenAPI Auth credential.
    - Ensure that the token has the necessary permissions to access the API endpoints.

3. **Tools not working as expected**:
    - Double-check the OpenAPI specification to ensure it accurately describes the API endpoints and operations.
    - Verify that the API server is accessible and responding correctly to requests.

If you continue to experience issues, consult the AnswerAgentAI documentation or reach out to support for further assistance.
</file>

<file path="sidekick-studio/chatflows/tools/python-interpreter.md">
---
description: Execute Python code in a sandbox environment
---

# Python Interpreter

## Overview

The Python Interpreter tool in AnswerAgentAI allows you to execute Python code within a secure sandbox environment. This powerful feature enables you to run Python scripts directly within your workflows, expanding the capabilities of your AI agents.

## Key Benefits

-   Execute Python code safely within AnswerAgentAI workflows
-   Access a wide range of Python libraries and functions
-   Integrate complex computations and data processing into your AI agents

## How to Use

1. Add the Python Interpreter node to your canvas in the AnswerAgentAI Studio.
2. Configure the tool settings:
    - Tool Name: Enter a custom name for the tool (default: "python_interpreter")
    - Tool Description: Provide a description of the tool's functionality (a default description is provided)
3. Connect the Python Interpreter node to other nodes in your workflow as needed.
4. In nodes or agents that use this tool, provide Python code as input when calling the tool.

<!-- TODO: Screenshot of the Python Interpreter node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/pythoninterpreter.png" alt="" /><figcaption><p> Python Interpreter node configuration   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Always print your outputs in the Python code, as the environment resets after each execution.
2. Use only packages available in Pyodide, the Python distribution used by this tool.
3. Write clear, self-contained scripts that can be executed independently.
4. Handle potential errors in your Python code to provide informative feedback.

## Troubleshooting

1. **ImportError**: If you encounter an ImportError, make sure you're only using packages available in Pyodide.
2. **Execution Timeout**: For long-running scripts, consider breaking them into smaller, more manageable parts.
3. **Unexpected Results**: Remember that the environment resets after each execution. Store necessary data or state within your script.

## Example Usage

Here's a simple example of how to use the Python Interpreter tool:

```python
import math

def calculate_circle_area(radius):
    return math.pi * radius ** 2

radius = 5
area = calculate_circle_area(radius)
print(f"The area of a circle with radius {radius} is {area:.2f}")
```

This script will calculate and print the area of a circle with a radius of 5 units.

Remember, the Python Interpreter tool is a powerful feature that allows you to extend the capabilities of your AnswerAgentAI workflows with custom Python code. Use it wisely to enhance your AI agents' functionality and problem-solving abilities.
</file>

<file path="sidekick-studio/chatflows/tools/read-file.md">
---
description: Read files from disk using the Read File tool
---

# Read File Tool (Local Only)

## Overview

The Read File tool is a powerful feature in AnswerAgentAI that allows you to read files directly from your computer's disk. This tool is particularly useful when you need to access and process local file contents within your workflows.

## Key Benefits

-   Easy access to local file contents
-   Seamless integration with other AnswerAgentAI tools and workflows
-   Flexible file path configuration

## How to Use

1. Add the Read File tool to your canvas in the AnswerAgentAI Studio.
2. Configure the tool's settings:
    - (Optional) Set the "Base Path" if you want to specify a default directory for file operations.
3. Connect the Read File tool to other nodes in your workflow.
4. When executing the workflow, provide the file path as input to the Read File tool.

<!-- TODO: Add a screenshot of the Read File tool node on the canvas -->
<figure><img src="/.gitbook/assets/screenshots/readfile.png" alt="" /><figcaption><p> Read File tool node   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Always use forward slashes (/) in file paths, even on Windows systems, to ensure compatibility.
-   If you frequently access files from the same directory, set the "Base Path" to avoid repeating the full path for each file.
-   Ensure that the AnswerAgentAI application has the necessary permissions to access the files you want to read.
-   Use relative paths when possible to make your workflows more portable across different systems.

## Troubleshooting

1. **File not found error**:

    - Double-check the file path and ensure it's correct.
    - Verify that the file exists in the specified location.
    - Check if the "Base Path" is set correctly (if used).

2. **Permission denied error**:

    - Ensure that the AnswerAgentAI application has read permissions for the file and its parent directories.
    - Try running AnswerAgentAI with elevated privileges if necessary.

3. **Unexpected file contents**:
    - Verify that the file is not open or locked by another application.
    - Check if the file is in the expected format (e.g., text file vs. binary file).

Remember that the Read File tool is designed to work with text-based files. For binary files or more complex file operations, you may need to use specialized tools or custom scripts within your AnswerAgentAI workflow.
</file>

<file path="sidekick-studio/chatflows/tools/README.md">
---
description: LangChain Tool Nodes
---

# Tools

---

Tools are functions that agents can use to interact with the world. These tools can be generic utilities (e.g. search), other chains, or even other agents.

### Tool Nodes

-   [BraveSearch API](bravesearch-api.md)
-   [Calculator](calculator.md)
-   [Chain Tool](chain-tool.md)
-   [Chatflow Tool](chatflow-tool.md)
-   [Custom Tool](custom-tool.md)
-   [Exa Search](exa-search.md)
-   [Google Calendar](google-calendar.md)
-   [Google Custom Search](google-custom-search.md)
-   [OpenAPI Toolkit](openapi-toolkit.md)
-   [Python Interpreter](python-interpreter.md)
-   [Read File](read-file.md)
-   [Request Get](request-get.md)
-   [Request Post](request-post.md)
-   [Retriever Tool](retriever-tool.md)
-   [Serp API](serp-api.md)
-   [Serper](serper.md)
-   [Web Browser](web-browser.md)
-   [Write File](write-file.md)
</file>

<file path="sidekick-studio/chatflows/tools/request-get.md">
---
description: Execute HTTP GET requests with the Requests Get tool
---

# Requests Get

## Overview

The Requests Get tool allows you to execute HTTP GET requests within your AnswerAgentAI workflows. This powerful feature enables your agents to interact with external APIs and retrieve data from specified URLs.

## Key Benefits

-   Seamlessly integrate external data sources into your workflows
-   Enhance your agents' capabilities by allowing them to fetch real-time information
-   Customize requests with headers for more complex API interactions

## How to Use

1. Locate the "Requests Get" node in the Tools section of the node palette.
2. Drag and drop the node onto your canvas to add it to your workflow.
3. Configure the node by setting the following parameters:
    - URL (optional): The exact URL to which the agent will make the GET request.
    - Description (optional): A prompt to guide the agent on when to use this tool.
    - Headers (optional): Any additional headers required for the GET request.

<!-- TODO: Add a screenshot showing the Requests Get node on the canvas with its configuration panel open -->
<figure><img src="/.gitbook/assets/screenshots/requestget.png" alt="" /><figcaption><p> Requests Get node   &#x26; Drop UI</p></figcaption></figure>

4. Connect the Requests Get node to other nodes in your workflow as needed.

## Tips and Best Practices

-   If you don't specify a URL, the agent will attempt to determine it from the AIPlugin if provided.
-   Use the Description field to provide context for when the agent should use this tool. This helps in creating more intelligent and context-aware workflows.
-   When working with APIs that require authentication, use the Headers field to include necessary authorization tokens or API keys.
-   Test your GET requests thoroughly to ensure they're returning the expected data before deploying your workflow.

## Troubleshooting

-   If you're not receiving the expected data, double-check the URL and ensure it's accessible and returning the correct information.
-   For APIs requiring authentication, verify that you've included the correct headers and that your authentication credentials are valid.
-   If the agent isn't using the Requests Get tool when expected, review and refine the Description field to provide clearer instructions.

Remember that the Requests Get tool is a powerful way to extend your AnswerAgentAI workflows by incorporating external data. Use it wisely to create more dynamic and informative agent interactions.
</file>

<file path="sidekick-studio/chatflows/tools/request-post.md">
---
description: Execute HTTP POST requests with the Requests Post tool
---

# Requests Post

## Overview

The Requests Post tool allows you to execute HTTP POST requests within your AnswerAgentAI workflows. This powerful feature enables your agents to interact with external APIs and services, sending data and receiving responses.

## Key Benefits

-   Interact with external APIs and web services
-   Send complex data structures in POST requests
-   Customize headers and request parameters

## How to Use

1. Locate the "Requests Post" node in the Tools section of the node palette.
2. Drag and drop the node onto your canvas.
3. Configure the node by filling in the following parameters:
    - URL (optional): The exact URL for the POST request.
    - Body (optional): The JSON body for the POST request.
    - Description (optional): A prompt to guide the agent on when to use this tool.
    - Headers (optional): Custom headers for the request.

<!-- TODO: Add a screenshot of the Requests Post node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/requestpost.png" alt="" /><figcaption><p> Requests Post node   &#x26; Drop UI</p></figcaption></figure>
4. Connect the Requests Post node to other nodes in your workflow as needed.

## Tips and Best Practices

1. Use the Description field to provide clear instructions to the agent on when and how to use this tool.
2. When working with specific APIs, always refer to their documentation for the correct URL, body structure, and required headers.
3. Use the JSON format for the Body and Headers fields to ensure proper formatting of complex data structures.
4. If you're using an AI plugin, you can leave the URL and Body fields empty, and the agent will attempt to determine these automatically.

## Troubleshooting

1. If you encounter "Invalid URL" errors, double-check that the URL is correctly formatted and includes the protocol (e.g., https://).
2. For "Invalid JSON" errors in the Body or Headers fields, verify that your JSON is properly formatted without any syntax errors.
3. If the request fails, check the response status code and message for clues about what went wrong. Common issues include authentication errors (401) or incorrect request format (400).

Remember that the Requests Post tool is powerful but should be used responsibly. Ensure that you have permission to access the APIs you're interacting with and be mindful of rate limits and data usage.
</file>

<file path="sidekick-studio/chatflows/tools/retriever-tool.md">
---
description: Use a retriever as an allowed tool for agents
---

# Retriever Tool

## Overview

The Retriever Tool node allows you to incorporate a retriever as a tool for agents in AnswerAgentAI workflows. This tool enables agents to search and retrieve relevant documents based on a given query, enhancing their ability to access and utilize information.

## Key Benefits

-   Empowers agents with the ability to search and retrieve relevant information
-   Enhances the knowledge base of agents by providing access to stored documents
-   Allows for flexible configuration of retriever behavior within agent workflows

## How to Use

1. Add the Retriever Tool node to your AnswerAgentAI canvas.
2. Configure the following parameters:

    - Retriever Name: Provide a unique name for the retriever tool (e.g., "search_state_of_union").
    - Retriever Description: Explain when the agent should use this tool to retrieve documents.
    - Retriever: Select the BaseRetriever instance to be used by this tool.
    - Return Source Documents: Choose whether to include source document information in the output.

3. Connect the Retriever Tool node to your agent node or other relevant nodes in your workflow.

<!-- TODO: Add a screenshot showing the Retriever Tool node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/retreivalnode.png" alt="" /><figcaption><p> Retriever Tool node    &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose a clear and descriptive name for your retriever tool to make it easily identifiable for the agent.
2. Provide a detailed description of when the agent should use this tool to ensure proper utilization.
3. Consider the trade-offs of returning source documents. While it provides more context, it may increase the response size.
4. Ensure that the selected retriever is properly configured and has access to the relevant document collection.

## Troubleshooting

1. If the agent is not using the retriever tool:

    - Double-check that the tool is properly connected to the agent node.
    - Verify that the tool's description clearly indicates when it should be used.

2. If the retrieved information is not relevant:

    - Review and refine the underlying retriever's configuration.
    - Consider updating the document collection or improving the retrieval method.

3. If you encounter performance issues:
    - Evaluate the size of your document collection and consider optimizing it.
    - Adjust the number of retrieved documents if supported by your retriever.
</file>

<file path="sidekick-studio/chatflows/tools/serp-api.md">
---
description: SERP API Tool Node Documentation
---

# SERP API Tool Node

## Overview

The SERP API Tool Node allows you to integrate Google search results into your AnswerAgentAI workflows. It provides access to real-time Google search data, enabling your agents to retrieve up-to-date information from the web.

## Key Benefits

-   Access to real-time Google search results
-   Retrieve various types of search data (web results, images, news, etc.)
-   Customize search parameters for specific use cases

## How to Use

1. Add the SERP API Tool Node to your AnswerAgentAI workflow canvas.
2. Connect your SERP API credential to the node.
3. Configure the search parameters as needed for your use case.
4. Connect the SERP API Tool Node to other nodes in your workflow to process the search results.

## Parameters

The SERP API Tool Node accepts the following parameters:

1. `q` (required): The search query string.

2. `location`: The location from which you want the search to originate.

3. `device`: The device to use for results. Options: `desktop` (default), `tablet`, or `mobile`.

4. `google_domain`: The Google domain to use (e.g., `google.com`, `google.co.uk`).

5. `gl`: The country code for the search (e.g., `us` for United States, `uk` for United Kingdom).

6. `hl`: The language code for the search results (e.g., `en` for English, `es` for Spanish).

7. `num`: The number of results to return (default is 10).

8. `start`: The result offset for pagination (e.g., 0 for first page, 10 for second page).

9. `safe`: Safe search setting. Options: `active` or `off` (default).

10. `tbm`: The type of search to perform. Options include:

    - (empty): Regular Google Search
    - `isch`: Google Images
    - `lcl`: Google Local
    - `vid`: Google Videos
    - `nws`: Google News
    - `shop`: Google Shopping

11. `tbs`: Advanced search parameters (e.g., for date ranges, file types).

12. `no_cache`: Set to `true` to bypass cached results (default is `false`).

13. `lr`: Limit results to specific languages.

14. `filter`: Enable (`1`) or disable (`0`) duplicate content filter.

15. `as_sitesearch`: Limit results to a specific website.

16. `as_qdr`: Filter results by date (e.g., `d` for past 24 hours, `w` for past week).

17. `as_rights`: Filter results by usage rights.

18. `sort`: Sort results (e.g., by date for news searches).

## Tips and Best Practices

1. Use specific and targeted search queries for better results.
2. Combine the SERP API Tool with other nodes to process and analyze the search results.
3. Be mindful of your API usage to avoid exceeding rate limits.
4. Use the `location` parameter to get region-specific results when relevant.
5. Experiment with different search types (`tbm` parameter) for various use cases.

## Troubleshooting

1. If you're not getting results, check that your SERP API credential is correctly configured.
2. Ensure that your search query is properly formatted and encoded.
3. If you're hitting rate limits, consider implementing caching or reducing the frequency of requests.

<!-- TODO: Add a screenshot of the SERP API Tool Node configuration in the AnswerAgentAI canvas -->
<figure><img src="/.gitbook/assets/screenshots/serperapi.png" alt="" /><figcaption><p> SERP API Tool  node    &#x26; Drop UI</p></figcaption></figure>

By configuring these parameters, you can customize the SERP API Tool Node to retrieve the most relevant search results for your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/tools/serper.md">
---
description: Serper - Google Search API Integration
---

# Serper Tools

## Overview

The Serper Tools node in AnswerAgentAI integrates the Serper.dev Google Search API, allowing workflows to perform powerful web searches directly within the AnswerAgentAI Studio. This tool enables agents to access up-to-date information from the internet, enhancing their ability to provide relevant and current responses.

## Key Benefits

-   Access to real-time web search results within AnswerAgentAI workflows
-   Enhance agent capabilities with current and relevant information
-   Seamless integration with Google Search functionality

## How to Use

1. Add the Serper Tools node to your canvas in the AnswerAgentAI Studio.
2. Connect your Serper API credential:
    - Click on the node to open its settings.
    - Under the "Connect Credential" section, select or add your Serper API credentials.

<!-- TODO: Screenshot of the Serper Tools node settings, highlighting the credential connection -->
<figure><img src="/.gitbook/assets/screenshots/serperapi.png" alt="" /><figcaption><p> Serper Tools Node    &#x26; Drop UI</p></figcaption></figure>

3. Configure any additional parameters if required (none are specified in the current version).
4. Connect the Serper Tools node to other nodes in your workflow where web search functionality is needed.

## Tips and Best Practices

-   Use Serper Tools when your agents need access to current information that may not be present in their training data.
-   Combine Serper Tools with other nodes to create more informed and up-to-date responses.
-   Be mindful of API usage and costs associated with the Serper.dev service.

## Troubleshooting

-   If the node fails to initialize, ensure that your Serper API credential is correctly set up and contains a valid API key.
-   Check your Serper.dev account for any API usage limits or restrictions if you encounter unexpected behavior.

<!-- TODO: Screenshot showing where to check API usage in the Serper.dev dashboard -->

Remember that the Serper Tools node acts as a wrapper around the Serper.dev Google Search API. Familiarize yourself with the Serper.dev documentation for more detailed information on the underlying API capabilities and limitations.
</file>

<file path="sidekick-studio/chatflows/tools/web-browser.md">
---
description: Web Browser Tool for AnswerAgentAI
---

# Web Browser Tool

## Overview

The Web Browser tool in AnswerAgentAI gives agents the ability to visit websites and extract information. This powerful feature allows your workflows to interact with web content, enabling a wide range of web-based tasks and information retrieval.

## Key Benefits

-   Enables agents to access and process information from the web
-   Enhances the capabilities of your workflows by incorporating real-time web data
-   Allows for dynamic information gathering based on user queries or workflow requirements

## How to Use

1. Locate the Web Browser tool in the Tools section of the node palette.
2. Drag and drop the Web Browser node onto your canvas.
3. Connect the necessary inputs to the Web Browser node:
    - Language Model: Connect a language model node to enable text processing.
    - Embeddings: Connect an embeddings node for text representation.
4. Configure any additional settings if required (refer to the node's settings panel).
5. Connect the Web Browser node's output to subsequent nodes in your workflow.

<!-- TODO: Add a screenshot showing the Web Browser node on the canvas with its inputs connected -->
<figure><img src="/.gitbook/assets/screenshots/webbrowser.png" alt="" /><figcaption><p> Web Browser node   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Ensure that your language model and embeddings are appropriate for web content processing.
-   Be mindful of web scraping etiquette and respect websites' robots.txt files and terms of service.
-   Consider implementing error handling in your workflow to manage potential issues with web access or content retrieval.
-   Use the Web Browser tool in combination with other nodes to filter, process, or analyze the retrieved web content.

## Troubleshooting

-   If the Web Browser tool fails to retrieve content, check your internet connection and verify that the target website is accessible.
-   Ensure that your language model and embeddings are correctly configured and compatible with the Web Browser tool.
-   If you encounter performance issues, consider optimizing your workflow to minimize unnecessary web requests.

By incorporating the Web Browser tool into your AnswerAgentAI workflows, you can create more dynamic and information-rich applications that leverage the vast resources available on the web.
</file>

<file path="sidekick-studio/chatflows/tools/write-file.md">
---
description: Write files to disk using the Write File tool
---

# Write File Tool (Local Only)

## Overview

The Write File tool allows you to write text content to files on your computer's disk. This powerful feature enables AnswerAgentAI to create and modify files as part of its workflows, making it useful for tasks such as generating reports, saving data, or creating log files.

## Key Benefits

-   Easily save generated content or data to files on your local system
-   Integrate file writing capabilities into your AnswerAgentAI workflows
-   Flexible file path options for organizing your saved files

## How to Use

1. Add the Write File tool to your canvas in the AnswerAgentAI Studio.
2. Configure the tool by setting the optional Base Path parameter.
3. Use the tool in your workflow by providing a file path and the text content to write.

### Configuration

<!-- TODO: Screenshot of the Write File tool configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/writefile.png" alt="" /><figcaption><p> Write File node   &#x26; Drop UI</p></figcaption></figure>
- **Base Path** (optional): Set a base directory for all file operations. If not specified, the tool will use the default working directory.

### Using in Workflows

To use the Write File tool in your workflows, you need to provide two pieces of information:

1. `file_path`: The name or path of the file you want to write to.
2. `text`: The content you want to write to the file.

## Tips and Best Practices

1. Use descriptive file names to easily identify the purpose of each file.
2. Organize your files by setting an appropriate Base Path for different projects or categories.
3. Be cautious when overwriting existing files, as the Write File tool will replace the entire content of the file.
4. Consider using dynamic file names or paths based on dates or other variables for better organization.

## Troubleshooting

1. **Permission Issues**: Ensure that AnswerAgentAI has the necessary permissions to write to the specified directory.

    - Solution: Check the file system permissions and adjust them if needed.

2. **Invalid File Path**: If you encounter errors related to invalid file paths, double-check the provided file path.

    - Solution: Make sure the file path is correct and the directories exist. Create any missing directories before writing the file.

3. **Disk Space**: If you're unable to write files, check if there's enough free disk space.
    - Solution: Free up disk space or choose a different location with more available space.

Remember that the Write File tool is a powerful feature that interacts with your local file system. Always be cautious when using it, especially in automated workflows, to avoid unintended file modifications or overwrites.
</file>

<file path="sidekick-studio/chatflows/tools-mcp/bravesearch-mcp.md">
---
sidebar_position: 8
title: BraveSearch MCP
description: Use BraveSearch MCP to perform internet searches using Brave Search
---

# BraveSearch MCP for Answer Agent

This documentation outlines how to use the Brave Search Model Context Protocol (MCP) integration with Answer Agent. The MCP allows Answer Agent to perform web searches using Brave Search API, providing up-to-date information from the internet through natural language.

## Setting Up Credentials

To use the BraveSearch MCP, you'll need to configure the following credentials:

### Obtaining Credentials

1. **Brave Search API Key**:
    - Go to [https://brave.com/search/api/](https://brave.com/search/api/)
    - Sign up for an API key
    - Complete the registration process
    - Copy your API key from the developer dashboard

### Configuration

You can provide these credentials in one of two ways:

1. **During conversation**: Answer Agent will prompt you for your credentials if not already configured.

2. **Via configuration file**: You can set up a permanent configuration by creating a file with:
    ```
    BRAVE_SEARCH_API_KEY=your_api_key_here
    ```

>  **Warning**: Your API key has usage limits based on your subscription plan. Monitor your usage to avoid unexpected charges.

## Available Tools

_Full documentation for BraveSearch MCP tools is coming soon._

## Common Workflows

_Sample workflows for BraveSearch MCP are coming soon._
</file>

<file path="sidekick-studio/chatflows/tools-mcp/confluence-mcp.md">
---
sidebar_position: 5
title: Confluence MCP
description: Use Confluence MCP to query and update Confluence pages
---

# Confluence MCP Documentation

## Introduction

Confluence MCP (Model Context Protocol) provides a standardized interface for AI assistants to interact with Confluence content. This integration enables Answer Agent to seamlessly search, create, update, and manage Confluence pages, comments, and attachments.

## Setting Up Credentials

To use the Confluence MCP, you'll need to set up the following credentials:

### Required Credentials

| Credential              | Description                                                                 |
| ----------------------- | --------------------------------------------------------------------------- |
| `CONFLUENCE_API_TOKEN`  | Your Atlassian API token                                                    |
| `CONFLUENCE_BASE_URL`   | Your Confluence instance URL (e.g., https://your-domain.atlassian.net/wiki) |
| `CONFLUENCE_USER_EMAIL` | The email associated with your Atlassian account                            |

### How to Get Your API Token

1. Log in to your Atlassian account at [https://id.atlassian.com/manage-profile/security/api-tokens](https://id.atlassian.com/manage-profile/security/api-tokens)
2. Click "Create API token"
3. Enter a label for your token (e.g., "Answer Agent Integration")
4. Click "Create"
5. Copy the token (you won't be able to see it again)

## Available Tools

### get_page

Retrieves a specific Confluence page by its ID.

**Schema:**

```json
{
    "pageId": {
        "type": "string",
        "description": "ID of the Confluence page to retrieve",
        "required": true
    },
    "format": {
        "type": "string",
        "enum": ["text", "markdown"],
        "description": "Format to return the content in (default: text)",
        "required": false
    },
    "includeMarkup": {
        "type": "boolean",
        "description": "Whether to include the original Confluence Storage Format (XHTML) markup in the response (default: false). Useful when you want to update the page later in order to preserve formatting.",
        "required": false
    }
}
```

### search_pages

Searches for Confluence pages using CQL (Confluence Query Language).

**Schema:**

```json
{
    "query": {
        "type": "string",
        "description": "CQL search query",
        "required": true
    },
    "limit": {
        "type": "number",
        "description": "Maximum number of results to return (default: 10)",
        "required": false
    },
    "format": {
        "type": "string",
        "enum": ["text", "markdown"],
        "description": "Format to return the content in (default: text)",
        "required": false
    },
    "includeMarkup": {
        "type": "boolean",
        "description": "Whether to include the original Confluence Storage Format (XHTML) markup in the response (default: false)",
        "required": false
    }
}
```

### get_spaces

Lists all available Confluence spaces.

**Schema:**

```json
{
    "limit": {
        "type": "number",
        "description": "Maximum number of spaces to return (default: 50)",
        "required": false
    }
}
```

### create_page

Creates a new Confluence page.

**Schema:**

```json
{
    "spaceKey": {
        "type": "string",
        "description": "Key of the space where the page will be created",
        "required": true
    },
    "title": {
        "type": "string",
        "description": "Title of the new page",
        "required": true
    },
    "content": {
        "type": "string",
        "description": "Content of the page in Confluence Storage Format (XHTML)",
        "required": true
    },
    "parentId": {
        "type": "string",
        "description": "Optional ID of the parent page",
        "required": false
    }
}
```

### update_page

Updates an existing Confluence page.

**Schema:**

```json
{
    "pageId": {
        "type": "string",
        "description": "ID of the page to update",
        "required": true
    },
    "title": {
        "type": "string",
        "description": "New title of the page",
        "required": true
    },
    "content": {
        "type": "string",
        "description": "New content in Confluence Storage Format (XHTML). CRITICAL: Content MUST be valid XHTML. Providing plain text or Markdown will result in the markup being displayed literally, not rendered as rich text.",
        "required": true
    },
    "version": {
        "type": "number",
        "description": "Current version number of the page",
        "required": true
    }
}
```

### get_comments

Retrieves comments for a specific Confluence page.

**Schema:**

```json
{
    "pageId": {
        "type": "string",
        "description": "ID of the page to retrieve comments for",
        "required": true
    },
    "format": {
        "type": "string",
        "enum": ["text", "markdown"],
        "description": "Format to return comment content in (default: text)",
        "required": false
    },
    "limit": {
        "type": "number",
        "description": "Maximum number of comments to return (default: 25)",
        "required": false
    }
}
```

### add_comment

Adds a comment to a Confluence page.

**Schema:**

```json
{
    "pageId": {
        "type": "string",
        "description": "ID of the page to add the comment to",
        "required": true
    },
    "content": {
        "type": "string",
        "description": "Comment content in Confluence Storage Format (XHTML)",
        "required": true
    },
    "parentId": {
        "type": "string",
        "description": "Optional ID of the parent comment for threading",
        "required": false
    }
}
```

### get_attachments

Retrieves attachments for a specific Confluence page.

**Schema:**

```json
{
    "pageId": {
        "type": "string",
        "description": "ID of the page to retrieve attachments for",
        "required": true
    },
    "limit": {
        "type": "number",
        "description": "Maximum number of attachments to return (default: 25)",
        "required": false
    }
}
```

### add_attachment

Adds an attachment to a Confluence page.

**Schema:**

```json
{
    "pageId": {
        "type": "string",
        "description": "ID of the page to attach the file to",
        "required": true
    },
    "filename": {
        "type": "string",
        "description": "Desired filename for the attachment",
        "required": true
    },
    "fileContentBase64": {
        "type": "string",
        "description": "Base64 encoded content of the file",
        "required": true
    },
    "comment": {
        "type": "string",
        "description": "Optional comment for the attachment version",
        "required": false
    }
}
```

## CQL (Confluence Query Language) Reference

When using the `search_pages` tool, you can leverage CQL to create powerful queries:

```
# Basic syntax
field operator value

# Examples
space = "Engineering"
title ~ "Meeting Notes"
label = documentation
created >= 2023-01-01
```

Common CQL fields:

-   `space`: Space key
-   `title`: Page title
-   `label`: Labels attached to content
-   `text`: Full-text search across all content
-   `created`/`lastmodified`: Date fields
-   `creator`/`contributor`: People who created or edited content

## Use Cases

### For Developers

1. **Documentation Management**

    - Search for technical documentation across multiple spaces
    - Update API documentation programmatically when new endpoints are added
    - Pull code snippets from Confluence to include in developer resources

2. **Project Tracking**

    - Create sprint planning pages automatically
    - Update project status pages with real-time information
    - Attach build reports and metrics to project pages

3. **Team Collaboration**
    - Add comments to design documents during code reviews
    - Create and update architecture decision records
    - Maintain a knowledge base of technical solutions

### For Publishers

1. **Content Creation**

    - Create new articles and knowledge base entries
    - Update existing content with fresh information
    - Organize content hierarchically across spaces

2. **Content Discovery**

    - Search for existing content to avoid duplication
    - Find related articles to link between pages
    - Identify gaps in documentation

3. **Content Management**

    - Track comments and discussions on published articles
    - Manage attachments and supplementary materials
    - Monitor content versions and update histories

4. **Workflow Integration**
    - Create templates for consistent content creation
    - Set up automated publishing processes
    - Track content review cycles

## Best Practices

1. **Working with XHTML Content**

    - Always use the Confluence Storage Format (XHTML) when creating or updating pages
    - Use `includeMarkup: true` when retrieving pages you plan to update
    - Test complex formatting in Confluence first before programmatic creation

2. **Optimizing Searches**

    - Use specific CQL queries to reduce result sets
    - Include space restrictions where possible (`space = "KEY"`)
    - Use label searches for categorized content

3. **Managing Page Versions**

    - Always include the correct current version when updating pages
    - Retrieve the current page before updating to ensure version accuracy
    - Handle version conflicts gracefully

4. **Handling Attachments**
    - Keep attachment file sizes reasonable
    - Use descriptive filenames for better searchability
    - Add comments to attachments to describe their purpose
</file>

<file path="sidekick-studio/chatflows/tools-mcp/contentful-mcp.md">
---
sidebar_position: 2
title: Contentful MCP
description: Use Contentful MCP to manage content in Contentful CMS
---

# Contentful MCP for Answer Agent

This documentation outlines how to use the Contentful Model Context Protocol (MCP) integration with Answer Agent. The MCP allows Answer Agent to interact with Contentful's Content Management API, enabling content creation, management, and publishing through natural language.

## Setting Up Credentials

To use the Contentful MCP, you'll need to configure the following credentials:

### Obtaining Credentials

1. **Contentful Management Token**:

    - Log in to your Contentful account at [https://app.contentful.com/](https://app.contentful.com/)
    - Go to Settings  API Keys  Content management tokens
    - Create a new Personal Access Token with a descriptive name
    - Copy the token immediately (it will only be shown once)

2. **Space ID**:

    - This is found in the URL when you're in your Contentful space: `https://app.contentful.com/spaces/{SPACE_ID}/...`
    - Or go to Settings  General Settings where you'll see your Space ID

3. **Environment ID**:
    - Default is `master` if you haven't created custom environments
    - For custom environments, you can find them in Settings  Environments

### Configuration

You can provide these credentials in one of two ways:

1. **During conversation**: Answer Agent will prompt you for your credentials if not already configured.

2. **Via configuration file**: You can set up a permanent configuration by creating a file with:
    ```
    CONTENTFUL_MANAGEMENT_ACCESS_TOKEN=your_token_here
    SPACE_ID=your_space_id
    ENVIRONMENT_ID=your_environment_id (defaults to 'master')
    ```

>  **Warning**: The management token has full access to create, modify, and delete content. Consider creating a dedicated token with appropriate permissions for this integration.

## Available Tools

The Contentful MCP provides tools across four main categories:

### Entry Management Tools

These tools allow you to work with content entries in Contentful.

#### search_entries

Search for entries using query parameters.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space (optional if configured)",
    "environmentId": "ID of environment (defaults to 'master')",
    "query": {
        "content_type": "Filter by content type ID",
        "select": "Fields to include in response",
        "limit": "Maximum entries to return (max: 3)",
        "skip": "Number of entries to skip (for pagination)",
        "order": "Field to order results by",
        "query": "Full-text search query"
    }
}
```

**Use Cases**:

-   Developers: Query entries for integration testing or debugging
-   Publishers: Find specific content by title, author, or status

#### create_entry

Create a new entry in Contentful.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "contentTypeId": "ID of the content type for this entry",
    "fields": {
        "fieldName": {
            "en-US": "Field value in English"
        }
    }
}
```

**Use Cases**:

-   Developers: Seed content programmatically
-   Publishers: Create draft entries from templates or structured data

#### get_entry

Retrieve details of a specific entry.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "entryId": "ID of the entry to retrieve"
}
```

**Use Cases**:

-   Developers: Inspect entry structure and relationships
-   Publishers: View complete entry details including metadata

#### update_entry

Update an existing entry.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "entryId": "ID of the entry to update",
    "fields": {
        "fieldName": {
            "en-US": "Updated field value"
        }
    }
}
```

**Use Cases**:

-   Developers: Fix incorrect data or update placeholders
-   Publishers: Edit content or update metadata fields

#### delete_entry

Delete an entry from Contentful.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "entryId": "ID of the entry to delete"
}
```

**Use Cases**:

-   Developers: Remove test entries
-   Publishers: Delete outdated or duplicate content

#### publish_entry

Publish an entry, making it available in the delivery API.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "entryId": "ID of the entry to publish"
}
```

**Use Cases**:

-   Developers: Promote test content to production
-   Publishers: Make new or updated content available to the public

#### unpublish_entry

Unpublish an entry, removing it from the delivery API.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "entryId": "ID of the entry to unpublish"
}
```

**Use Cases**:

-   Developers: Demote content during testing
-   Publishers: Take down content temporarily while maintaining draft version

### Asset Management Tools

These tools help you manage media files and binary assets in Contentful.

#### list_assets

List assets in a space with pagination.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "limit": "Maximum assets to return (max: 3)",
    "skip": "Number of assets to skip (for pagination)"
}
```

**Use Cases**:

-   Developers: Audit asset usage and organization
-   Publishers: Browse available media assets

#### upload_asset

Upload a new asset to Contentful.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "title": "Title of the asset",
    "description": "Description of the asset (optional)",
    "file": {
        "upload": "URL of the file to upload",
        "fileName": "Name of the file",
        "contentType": "MIME type of the file"
    }
}
```

**Use Cases**:

-   Developers: Programmatically upload assets during content migration
-   Publishers: Add new images, documents, or media files

#### get_asset

Retrieve details of a specific asset.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "assetId": "ID of the asset to retrieve"
}
```

**Use Cases**:

-   Developers: Get asset URLs and metadata for integration
-   Publishers: Check asset details and versions

#### update_asset

Update an asset's metadata or file.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "assetId": "ID of the asset to update",
    "title": "Updated title (optional)",
    "description": "Updated description (optional)",
    "file": {
        "url": "URL of the new file (optional)",
        "fileName": "Name of the new file (required if updating file)",
        "contentType": "MIME type of the new file (required if updating file)"
    }
}
```

**Use Cases**:

-   Developers: Update asset metadata programmatically
-   Publishers: Replace outdated assets or fix metadata

#### delete_asset

Delete an asset from Contentful.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "assetId": "ID of the asset to delete"
}
```

**Use Cases**:

-   Developers: Clean up unused test assets
-   Publishers: Remove outdated or unnecessary media

#### publish_asset

Publish an asset, making it available in the delivery API.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "assetId": "ID of the asset to publish"
}
```

**Use Cases**:

-   Developers: Make assets available to frontend applications
-   Publishers: Publish images and files alongside related content

#### unpublish_asset

Unpublish an asset, removing it from the delivery API.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "assetId": "ID of the asset to unpublish"
}
```

**Use Cases**:

-   Developers: Remove asset from production during testing
-   Publishers: Take down assets that should no longer be publicly accessible

### Content Type Management Tools

These tools allow you to manage the structure and schema of your content.

#### list_content_types

List all content types in a space.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "limit": "Maximum content types to return (max: 10)",
    "skip": "Number of content types to skip (for pagination)"
}
```

**Use Cases**:

-   Developers: Explore content model for integration planning
-   Publishers: Review available content types before creating entries

#### get_content_type

Get detailed information about a specific content type.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "contentTypeId": "ID of the content type to retrieve"
}
```

**Use Cases**:

-   Developers: Inspect field definitions and validations
-   Publishers: Check required fields and content structure

#### get_editor_interface

Get the editor interface configuration for a content type.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "contentTypeId": "ID of the content type"
}
```

**Use Cases**:

-   Developers: Understand UI configuration for content editing
-   Publishers: Learn how fields are presented in the Contentful editor

#### update_editor_interface

Update the editor interface configuration for a content type.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "contentTypeId": "ID of the content type",
    "editorInterface": {
        "controls": [
            {
                "fieldId": "fieldName",
                "widgetId": "widgetType",
                "widgetNamespace": "namespace"
            }
        ]
    }
}
```

**Use Cases**:

-   Developers: Customize the editing experience for content creators
-   Publishers: Configure specialized editors for specific field types

#### create_content_type

Create a new content type in Contentful.

**Schema**:

```json
{
  "spaceId": "ID of your Contentful space",
  "environmentId": "ID of environment",
  "name": "Name of the content type",
  "fields": [
    {
      "id": "fieldId",
      "name": "Field Name",
      "type": "Field Type (Symbol, Text, Integer, etc.)",
      "required": true/false,
      "localized": true/false
    }
  ],
  "description": "Description of the content type",
  "displayField": "Field ID to use as display field"
}
```

**Use Cases**:

-   Developers: Set up content models programmatically
-   Publishers: Create new content structures for specific projects

#### update_content_type

Update an existing content type.

**Schema**:

```json
{
  "spaceId": "ID of your Contentful space",
  "environmentId": "ID of environment",
  "contentTypeId": "ID of the content type to update",
  "name": "Updated name",
  "fields": [
    {
      "id": "fieldId",
      "name": "Updated Field Name",
      "type": "Field Type",
      "required": true/false
    }
  ],
  "description": "Updated description",
  "displayField": "Updated display field"
}
```

**Use Cases**:

-   Developers: Evolve content models as requirements change
-   Publishers: Add or modify fields to accommodate new content needs

#### delete_content_type

Delete a content type from Contentful.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "contentTypeId": "ID of the content type to delete"
}
```

**Use Cases**:

-   Developers: Remove experimental or deprecated content types
-   Publishers: Clean up unused content models

#### publish_content_type

Publish a content type, making its changes available.

**Schema**:

```json
{
    "spaceId": "ID of your Contentful space",
    "environmentId": "ID of environment",
    "contentTypeId": "ID of the content type to publish"
}
```

**Use Cases**:

-   Developers: Apply content model changes after updates
-   Publishers: Make new fields or validations available for content creation

### Space & Environment Management Tools

These tools help you manage spaces and environments in Contentful.

> Note: These tools are only available if the Space ID and Environment ID are not pre-configured.

#### list_spaces

List all available Contentful spaces.

**Schema**:

```json
{}
```

**Use Cases**:

-   Developers: Discover available spaces for integration
-   Publishers: Navigate between multiple content spaces

#### get_space

Get details of a specific space.

**Schema**:

```json
{
    "spaceId": "ID of the space to retrieve"
}
```

**Use Cases**:

-   Developers: Get space configuration and metadata
-   Publishers: Check space settings and organization

#### list_environments

List all environments in a space.

**Schema**:

```json
{
    "spaceId": "ID of the space to list environments from"
}
```

**Use Cases**:

-   Developers: Identify available environments for deployment
-   Publishers: Check available environments for content staging

#### create_environment

Create a new environment in a space.

**Schema**:

```json
{
    "spaceId": "ID of the space",
    "environmentId": "ID for the new environment",
    "name": "Name of the new environment"
}
```

**Use Cases**:

-   Developers: Set up new environments for testing or staging
-   Publishers: Create separate environments for content preparation

#### delete_environment

Delete an environment from a space.

**Schema**:

```json
{
    "spaceId": "ID of the space",
    "environmentId": "ID of the environment to delete"
}
```

**Use Cases**:

-   Developers: Clean up temporary testing environments
-   Publishers: Remove obsolete staging environments

## Best Practices

1. **Start with exploration**: Use `list_content_types` to understand the content model before creating entries.

2. **Content type first**: Always check a content type's structure with `get_content_type` before creating entries.

3. **Pagination awareness**: List operations return a maximum of 3-10 items per request. Use the `skip` parameter to navigate through paginated results.

4. **Always provide complete fields**: When updating entries, include all fields, not just the ones you're changing.

5. **Environment isolation**: Consider creating a dedicated environment for Answer Agent to work in before applying changes to production.

6. **Space-specific organization**: When working with multiple spaces, make it explicit which space you're targeting in your requests.

7. **Asset management**: Remember to publish assets after uploading them to make them available in the delivery API.

## Common Workflows

### Creating and Publishing Content

1. Find the content type ID using `list_content_types`
2. Get the content type details with `get_content_type`
3. Create an entry with `create_entry`
4. Publish the entry with `publish_entry`

### Adding Media to Content

1. Upload an asset with `upload_asset`
2. Publish the asset with `publish_asset`
3. Update an entry to reference the asset with `update_entry`
4. Publish the updated entry with `publish_entry`

### Content Model Updates

1. Get the current content type with `get_content_type`
2. Update the content type with `update_content_type`
3. Publish the content type with `publish_content_type`
</file>

<file path="sidekick-studio/chatflows/tools-mcp/custom-mcp.md">
---
sidebar_position: 9
title: Custom MCP
description: Create custom integrations for proprietary systems
---

# Custom MCP for Answer Agent

This documentation outlines how to create and use custom Model Context Protocol (MCP) integrations with Answer Agent. Custom MCPs allow you to build connections to proprietary systems, internal tools, or any API not covered by the built-in MCPs.

## What is a Custom MCP?

A Custom MCP is a server that implements the Model Context Protocol standard, exposing tools that Answer Agent can use to interact with your target system. By creating a custom MCP, you can:

1. **Connect to internal systems** that aren't accessible through public APIs
2. **Add functionality** not available in the standard MCPs
3. **Customize interactions** specific to your organization's needs
4. **Integrate with legacy systems** using your own abstraction layer

## Creating a Custom MCP

To create a custom MCP, you'll need to:

1. **Implement the MCP server protocol** - Either from scratch or by extending an existing MCP server
2. **Define your tools** - The operations your MCP will expose to Answer Agent
3. **Handle authentication** - Secure your integration with appropriate authentication
4. **Deploy your server** - Make it accessible to Answer Agent

### MCP Server Implementation

The MCP protocol defines a standard way for AI assistants to discover and call tools. Your server needs to implement:

-   Tool listing: Providing metadata about available tools
-   Tool calling: Executing operations based on parameters
-   Error handling: Providing meaningful errors for debugging

### Example Implementation

_Detailed implementation examples and code snippets are coming soon._

## Configuring Custom MCP

To connect your Custom MCP to Answer Agent, you'll need:

1. **Server URL or Path**: The location where your MCP server is running
2. **Authentication Details**: Any credentials needed to access your MCP

## Available Tools

The tools available in your Custom MCP depend entirely on your implementation. Common patterns include:

-   **CRUD Operations**: Create, read, update, delete for your system's resources
-   **Search Functionality**: Finding resources based on criteria
-   **Process Triggering**: Starting workflows or processes in your system
-   **Data Transformation**: Converting data between formats

## Best Practices

1. **Security First**: Implement proper authentication and authorization
2. **Clear Documentation**: Document your tools for easier use
3. **Robust Error Handling**: Provide meaningful error messages
4. **Rate Limiting**: Protect your systems from overload
5. **Logging**: Track usage for debugging and improvement

## Example Use Cases

_Detailed use cases and examples are coming soon._
</file>

<file path="sidekick-studio/chatflows/tools-mcp/github-mcp.md">
---
sidebar_position: 6
title: GitHub MCP
description: Use GitHub MCP to manage repositories, issues, and pull requests
---

# GitHub MCP for Answer Agent

## Introduction

GitHub MCP (Model Context Protocol) Server is a powerful integration that provides AI systems with the ability to interact with GitHub repositories and data. It enables features like file operations, repository management, search functionality, issue tracking, and pull request management through a clean API interface.

> **Used MCP**: Development for this project is on the Github repo [https://github.com/github/github-mcp-server](https://github.com/github/github-mcp-server) repo.

## Setup Instructions

Before using the GitHub MCP tools, you need to set up your credentials:

### Obtaining a Personal Access Token

1. **Generate a GitHub Personal Access Token**:
    - Go to [GitHub Settings > Developer settings > Personal access tokens](https://github.com/settings/tokens)
    - Click "Generate new token" (classic)
    - Select which repositories you'd like this token to have access to (Public, All, or Select)
    - Create a token with the appropriate scopes:
        - For full access: `repo` scope ("Full control of private repositories")
        - For public repositories only: `public_repo` scope
    - Give your token a descriptive name
    - Click "Generate token"
    - Copy the token immediately (it will only be shown once)

### Configuring Answer Agent

Configure your credentials in Answer Agent by setting this environment variable:

```
GITHUB_TOKEN=your_token_here
```

## Key Features

-   **Automatic Branch Creation**: When creating/updating files or pushing changes, branches are automatically created if they don't exist
-   **Comprehensive Error Handling**: Clear error messages for common issues
-   **Git History Preservation**: Operations maintain proper Git history without force pushing
-   **Batch Operations**: Support for both single-file and multi-file operations
-   **Advanced Search**: Support for searching code, issues/PRs, and users

## Available Tools

### create_or_update_file

Create or update a single file in a repository.

**Schema**:

```typescript
{
  owner: string, // Repository owner (username or organization)
  repo: string, // Repository name
  path: string, // Path where to create/update the file
  content: string, // Content of the file
  message: string, // Commit message
  branch: string, // Branch to create/update the file in
  sha?: string // SHA of file being replaced (for updates)
}
```

**Use cases**:

-   Add new documentation files
-   Update configuration files
-   Create new source code files

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "path": "docs/README.md",
    "content": "# Hello World\n\nThis is a sample repository.",
    "message": "Add README documentation",
    "branch": "main"
}
```

### push_files

Push multiple files in a single commit.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  branch: string, // Branch to push to
  files: { // Files to push
    path: string,
    content: string
  }[],
  message: string // Commit message
}
```

**Use cases**:

-   Create multiple related files at once
-   Update several configuration files together
-   Make changes across multiple project files

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "branch": "feature/new-module",
    "files": [
        {
            "path": "src/module.js",
            "content": "export function hello() { return 'Hello world'; }"
        },
        {
            "path": "test/module.test.js",
            "content": "import { hello } from '../src/module';\n\ntest('hello returns greeting', () => {\n  expect(hello()).toBe('Hello world');\n});"
        }
    ],
    "message": "Add new module with tests"
}
```

### search_repositories

Search for GitHub repositories.

**Schema**:

```typescript
{
  query: string, // Search query
  page?: number, // Page number for pagination
  perPage?: number // Results per page (max 100)
}
```

**Use cases**:

-   Find repositories by topic
-   Search for repositories with specific languages
-   Discover popular repositories in a domain

**Example**:

```json
{
    "query": "machine learning language:python stars:>1000",
    "page": 1,
    "perPage": 30
}
```

### create_repository

Create a new GitHub repository.

**Schema**:

```typescript
{
  name: string, // Repository name
  description?: string, // Repository description
  private?: boolean, // Whether repo should be private
  autoInit?: boolean // Initialize with README
}
```

**Use cases**:

-   Start a new project
-   Create a repository for documentation
-   Set up a template repository

**Example**:

```json
{
    "name": "awesome-project",
    "description": "A repository for my awesome project",
    "private": false,
    "autoInit": true
}
```

### get_file_contents

Get contents of a file or directory.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  path: string, // Path to file/directory
  branch?: string // Branch to get contents from
}
```

**Use cases**:

-   Read configuration files
-   Check current content before updates
-   List directory contents

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "path": "src",
    "branch": "main"
}
```

### create_issue

Create a new issue.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  title: string, // Issue title
  body?: string, // Issue description
  assignees?: string[], // Usernames to assign
  labels?: string[], // Labels to add
  milestone?: number // Milestone number
}
```

**Use cases**:

-   Report bugs
-   Request new features
-   Create tasks for contributors

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "title": "Fix rendering bug in mobile view",
    "body": "When viewing on mobile devices smaller than 320px, the navigation menu overflows the screen.",
    "assignees": ["developer1"],
    "labels": ["bug", "priority:high"]
}
```

### create_pull_request

Create a new pull request.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  title: string, // PR title
  body?: string, // PR description
  head: string, // Branch containing changes
  base: string, // Branch to merge into
  draft?: boolean, // Create as draft PR
  maintainer_can_modify?: boolean // Allow maintainer edits
}
```

**Use cases**:

-   Submit code changes for review
-   Propose documentation updates
-   Merge feature branches

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "title": "Add user authentication module",
    "body": "This PR implements user authentication with JWT tokens.",
    "head": "feature/user-auth",
    "base": "main",
    "draft": false
}
```

### fork_repository

Fork a repository.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  organization?: string // Organization to fork to
}
```

**Use cases**:

-   Fork repositories to contribute
-   Clone repositories for customization
-   Copy templates for new projects

**Example**:

```json
{
    "owner": "facebook",
    "repo": "react",
    "organization": "my-organization"
}
```

### create_branch

Create a new branch.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  branch: string, // Name for new branch
  from_branch?: string // Source branch (defaults to repo default)
}
```

**Use cases**:

-   Create feature branches
-   Set up release branches
-   Create branches for testing

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "branch": "feature/user-profile",
    "from_branch": "main"
}
```

### list_issues

List and filter repository issues.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  state?: string, // Filter by state ('open', 'closed', 'all')
  labels?: string[], // Filter by labels
  sort?: string, // Sort by ('created', 'updated', 'comments')
  direction?: string, // Sort direction ('asc', 'desc')
  since?: string, // Filter by date (ISO 8601 timestamp)
  page?: number, // Page number
  per_page?: number // Results per page
}
```

**Use cases**:

-   Review open issues
-   Find recently updated issues
-   Get issues with specific labels

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "state": "open",
    "labels": ["bug"],
    "sort": "updated",
    "direction": "desc",
    "page": 1,
    "per_page": 30
}
```

### update_issue

Update an existing issue.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  issue_number: number, // Issue number to update
  title?: string, // New title
  body?: string, // New description
  state?: string, // New state ('open' or 'closed')
  labels?: string[], // New labels
  assignees?: string[], // New assignees
  milestone?: number // New milestone number
}
```

**Use cases**:

-   Change issue status
-   Reassign issues
-   Update issue descriptions with new information

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "issue_number": 42,
    "state": "closed",
    "labels": ["fixed"]
}
```

### add_issue_comment

Add a comment to an issue.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  issue_number: number, // Issue number to comment on
  body: string // Comment text
}
```

**Use cases**:

-   Add progress updates
-   Provide additional information
-   Ask questions about the issue

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "issue_number": 42,
    "body": "I've investigated this issue and found that it's related to the recent database migration."
}
```

### search_code

Search for code across GitHub repositories.

**Schema**:

```typescript
{
  q: string, // Search query using GitHub code search syntax
  sort?: string, // Sort field ('indexed' only)
  order?: string, // Sort order ('asc' or 'desc')
  per_page?: number, // Results per page (max 100)
  page?: number // Page number
}
```

**Use cases**:

-   Find code implementations
-   Search for usage examples
-   Look for specific patterns in code

**Example**:

```json
{
    "q": "language:javascript express app.use repo:expressjs/express",
    "per_page": 30
}
```

### search_issues

Search for issues and pull requests.

**Schema**:

```typescript
{
  q: string, // Search query using GitHub issues search syntax
  sort?: string, // Sort field (comments, reactions, created, etc.)
  order?: string, // Sort order ('asc' or 'desc')
  per_page?: number, // Results per page (max 100)
  page?: number // Page number
}
```

**Use cases**:

-   Find issues across repositories
-   Search for PRs with specific criteria
-   Track issues mentioned in discussions

**Example**:

```json
{
    "q": "is:open is:issue label:bug language:python",
    "sort": "created",
    "order": "desc",
    "per_page": 50
}
```

### search_users

Search for GitHub users.

**Schema**:

```typescript
{
  q: string, // Search query using GitHub users search syntax
  sort?: string, // Sort field (followers, repositories, joined)
  order?: string, // Sort order ('asc' or 'desc')
  per_page?: number, // Results per page (max 100)
  page?: number // Page number
}
```

**Use cases**:

-   Find potential collaborators
-   Search for users by location or company
-   Discover users with specific expertise

**Example**:

```json
{
    "q": "language:rust followers:>500",
    "sort": "followers",
    "order": "desc",
    "per_page": 20
}
```

### list_commits

Gets commits of a branch in a repository.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  page?: string, // Page number
  per_page?: string, // Number of records per page
  sha?: string // Branch name
}
```

**Use cases**:

-   Review recent changes
-   Check commit history
-   Identify contributors

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "sha": "feature-branch",
    "per_page": "10"
}
```

### get_issue

Gets the contents of an issue within a repository.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  issue_number: number // Issue number to retrieve
}
```

**Use cases**:

-   Get detailed issue information
-   Read issue discussions
-   Check issue status

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "issue_number": 42
}
```

### get_pull_request

Get details of a specific pull request.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  pull_number: number // Pull request number
}
```

**Use cases**:

-   Review PR changes
-   Check PR status
-   View PR discussions

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "pull_number": 101
}
```

### list_pull_requests

List and filter repository pull requests.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  state?: string, // Filter by state ('open', 'closed', 'all')
  head?: string, // Filter by head user/org and branch
  base?: string, // Filter by base branch
  sort?: string, // Sort by ('created', 'updated', 'popularity', 'long-running')
  direction?: string, // Sort direction ('asc', 'desc')
  per_page?: number, // Results per page (max 100)
  page?: number // Page number
}
```

**Use cases**:

-   Check open PRs for review
-   Monitor PRs to specific branches
-   Find oldest open PRs

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "state": "open",
    "base": "main",
    "sort": "updated",
    "direction": "desc"
}
```

### create_pull_request_review

Create a review on a pull request.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  pull_number: number, // Pull request number
  body: string, // Review comment text
  event: string, // Review action ('APPROVE', 'REQUEST_CHANGES', 'COMMENT')
  commit_id?: string, // SHA of commit to review
  comments?: { // Line-specific comments
    path: string, // File path
    position: number, // Line position in diff
    body: string // Comment text
  }[]
}
```

**Use cases**:

-   Approve pull requests
-   Request changes to PRs
-   Comment on specific code lines

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "pull_number": 101,
    "body": "The code looks good, but there are some performance concerns.",
    "event": "COMMENT",
    "comments": [
        {
            "path": "src/app.js",
            "position": 5,
            "body": "Consider using memoization here to improve performance."
        }
    ]
}
```

### merge_pull_request

Merge a pull request.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  pull_number: number, // Pull request number
  commit_title?: string, // Title for merge commit
  commit_message?: string, // Extra detail for merge commit
  merge_method?: string // Merge method ('merge', 'squash', 'rebase')
}
```

**Use cases**:

-   Complete code reviews
-   Integrate approved changes
-   Finalize feature development

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "pull_number": 101,
    "commit_title": "Merge: Add user authentication feature",
    "merge_method": "squash"
}
```

### get_pull_request_files

Get the list of files changed in a pull request.

**Schema**:

```typescript
{
  owner: string, // Repository owner
  repo: string, // Repository name
  pull_number: number // Pull request number
}
```

**Use cases**:

-   Review specific files in a PR
-   Check file changes before merging
-   Focus code review on critical files

**Example**:

```json
{
    "owner": "octocat",
    "repo": "hello-world",
    "pull_number": 101
}
```

## Search Query Syntax

GitHub MCP supports GitHub's advanced search syntax across different types of searches:

### Code Search

-   `language:javascript`: Search by programming language
-   `repo:owner/name`: Search in specific repository
-   `path:app/src`: Search in specific path
-   `extension:js`: Search by file extension

**Example**: `q: "import express" language:typescript path:src/`

### Issues Search

-   `is:issue` or `is:pr`: Filter by type
-   `is:open` or `is:closed`: Filter by state
-   `label:bug`: Search by label
-   `author:username`: Search by author

**Example**: `q: "memory leak" is:issue is:open label:bug`

### Users Search

-   `type:user` or `type:org`: Filter by account type
-   `followers:>1000`: Filter by followers
-   `location:London`: Search by location

**Example**: `q: "fullstack developer" location:London followers:>100`

For detailed search syntax, see [GitHub's searching documentation](https://docs.github.com/en/search-github/searching-on-github).

## Use Cases for Developers

### Repository Management

Use the GitHub MCP for common repository operations:

1. **Code Management**:

    - Create new repositories with `create_repository`
    - Push code changes with `push_files`
    - Create branches with `create_branch` for feature development

2. **Issue Tracking**:

    - Create issues with `create_issue` for bugs or features
    - Update issues with `update_issue` as work progresses
    - Add comments with `add_issue_comment` to provide updates

3. **Pull Request Workflow**:

    - Create PRs with `create_pull_request` for code reviews
    - Review PRs with `create_pull_request_review`
    - Merge approved changes with `merge_pull_request`

4. **Code Search and Discovery**:
    - Find code examples with `search_code`
    - Discover repositories with `search_repositories`
    - Find related issues with `search_issues`

### Integration Possibilities

-   Connect with CI/CD systems to automatically update PRs with build status
-   Link issues and PRs for automatic tracking of work
-   Create reports on repository activity and contributor statistics

## Advanced Features

The GitHub MCP server includes these powerful capabilities:

-   **Automatic Branch Creation**: Creates branches on-the-fly for operations that require them
-   **Batch Operations**: Supports multi-file operations in a single API call
-   **Cross-Repository Operations**: Works across multiple repositories for complex workflows
-   **Advanced Search**: Provides powerful search capabilities across code, issues, and users
-   **Complete PR Lifecycle**: Supports the entire PR workflow from creation to merge

## Limitations

-   API rate limits apply based on GitHub's rate limiting policies
-   Some operations require appropriate permissions in the personal access token
-   Search results are limited to GitHub's search API constraints
-   Large file operations may be subject to GitHub's file size limitations
</file>

<file path="sidekick-studio/chatflows/tools-mcp/jira-mcp.md">
---
sidebar_position: 3
title: Jira MCP
description: Use Jira MCP to work with Jira issues and projects
---

# JIRA MCP for Answer Agent

## Introduction

JIRA MCP (Model Context Protocol) Server is a powerful integration that provides AI systems with the ability to interact with JIRA data. It enables features like issue searching, epic management, comment handling, and relationship tracking through a clean API interface.

## Setup Instructions

Before using the JIRA MCP tools, you need to set up your credentials:

### Getting JIRA API Credentials

1. **Generate an API Token**:

    - Log in to your Atlassian account at [https://id.atlassian.com/manage-profile/security/api-tokens](https://id.atlassian.com/manage-profile/security/api-tokens)
    - Click "Create API token"
    - Give your token a meaningful name (e.g., "Answer Agent Integration")
    - Copy the token value (you won't be able to see it again)

2. **Find Your JIRA Instance URL**:

    - This is the base URL you use to access JIRA (e.g., `https://your-company.atlassian.net`)

3. **Identify Your Email Address**:
    - Use the email associated with your Atlassian account

### Configuring Answer Agent

Configure your credentials in Answer Agent by setting these environment variables:

```
JIRA_API_TOKEN=your_api_token_here
JIRA_BASE_URL=your_jira_instance_url
JIRA_USER_EMAIL=your_email_address
```

## Available Tools

### search_issues

Search for JIRA issues using JQL (JIRA Query Language).

**Schema**:

```typescript
{
    searchString: string // JQL search string
}
```

**Use cases**:

-   Find all issues assigned to a specific user
-   Search for all open bugs in a project
-   Filter issues by priority, status, or creation date

**Example**:

```
project = "ENG" AND status = "In Progress" AND assignee = currentUser()
```

### get_epic_children

Retrieve all child issues within an epic, including their comments and relationships.

**Schema**:

```typescript
{
    epicKey: string // The key of the epic issue
}
```

**Use cases**:

-   Get all stories and tasks within a feature epic
-   Review progress of all work items in a release epic
-   Analyze comment history across an entire feature set

**Example**:

```
ENG-123
```

### get_issue

Retrieve detailed information about a specific JIRA issue including its comments, relationships, and links.

**Schema**:

```typescript
{
    issueId: string // The ID or key of the JIRA issue
}
```

**Use cases**:

-   Deep dive into a specific bug's details
-   Review the discussion history on a feature request
-   Check parent/child relationships for a work item

**Example**:

```
ENG-456
```

### create_issue

Create a new JIRA issue with specified parameters.

**Schema**:

```typescript
{
  projectKey: string, // The project key where the issue will be created
  issueType: string, // The type of issue (e.g., "Bug", "Story", "Task")
  summary: string, // The issue summary/title
  description?: string, // Optional issue description
  fields?: { // Optional additional fields
    [key: string]: any
  }
}
```

**Use cases**:

-   Create bug reports from support interactions
-   Generate new feature requests from user feedback
-   Create tasks for identified improvements

**Example**:

```json
{
    "projectKey": "ENG",
    "issueType": "Bug",
    "summary": "Application crashes when uploading large files",
    "description": "When attempting to upload files larger than 50MB, the application crashes with no error message."
}
```

### update_issue

Update the fields of an existing JIRA issue.

**Schema**:

```typescript
{
  issueKey: string, // The key of the issue to update
  fields: { // Fields to update
    [key: string]: any
  }
}
```

**Use cases**:

-   Update issue priority based on new information
-   Assign issues to appropriate team members
-   Add or update custom fields with new data

**Example**:

```json
{
    "issueKey": "ENG-789",
    "fields": {
        "priority": { "name": "High" },
        "assignee": { "name": "john.doe@example.com" }
    }
}
```

### get_transitions

Retrieve available status transitions for a JIRA issue.

**Schema**:

```typescript
{
    issueKey: string // The key of the issue to get transitions for
}
```

**Use cases**:

-   Determine valid status changes for an issue
-   Check if an issue can be moved to "Done"
-   Find the specific transition ID needed for automation

**Example**:

```
ENG-101
```

### transition_issue

Change the status of a JIRA issue by performing a transition.

**Schema**:

```typescript
{
  issueKey: string, // The key of the issue to transition
  transitionId: string, // The ID of the transition to perform
  comment?: string // Optional comment to add with the transition
}
```

**Use cases**:

-   Move issues from "In Progress" to "Done"
-   Send issues back to "To Do" with feedback
-   Mark issues as "Ready for Review" with comments

**Example**:

```json
{
    "issueKey": "ENG-101",
    "transitionId": "21",
    "comment": "This work has been completed and is ready for review."
}
```

### add_attachment

Add a file attachment to a JIRA issue.

**Schema**:

```typescript
{
  issueKey: string, // The key of the issue to add attachment to
  fileContent: string, // Base64 encoded content of the file
  filename: string // Name of the file to be attached
}
```

**Use cases**:

-   Attach screenshots of bugs
-   Add design documents to feature requests
-   Upload log files for troubleshooting

**Example**:

```json
{
    "issueKey": "ENG-202",
    "fileContent": "base64_encoded_file_content_here",
    "filename": "error_screenshot.png"
}
```

### add_comment

Add a comment to a JIRA issue.

**Schema**:

```typescript
{
  issueIdOrKey: string, // The ID or key of the issue to add the comment to
  body: string // The content of the comment (plain text)
}
```

**Use cases**:

-   Add progress updates to tracked work
-   Respond to questions on issues
-   Document implementation details or decisions

**Example**:

```json
{
    "issueIdOrKey": "ENG-303",
    "body": "I've investigated this issue and found that it's related to the recent database migration. We should revert the change to the connection pooling configuration."
}
```

## Use Cases for Developers

### Workflow Automation

Use the JIRA MCP to automate common development workflows:

1. **Issue Triage**:

    - Search for new bugs with `search_issues`
    - Update priority and assign them with `update_issue`
    - Add comments with initial analysis using `add_comment`

2. **Sprint Management**:

    - Get epic children with `get_epic_children` to track sprint progress
    - Transition completed issues with `transition_issue`
    - Create subtasks for complex stories with `create_issue`

3. **Code Review Process**:

    - Search for issues ready for review with `search_issues`
    - Add review comments with `add_comment`
    - Attach code snippets or screenshots with `add_attachment`

4. **Release Management**:
    - Track all issues in a release epic with `get_epic_children`
    - Update statuses in bulk as features are completed
    - Document release notes by aggregating issue data

### Integration Possibilities

-   Connect CI/CD pipelines to update issue status on build/deploy events
-   Link code commits to JIRA issues automatically
-   Generate daily/weekly status reports from issue data

## Use Cases for Publishers

### Content Management

1. **Editorial Workflow**:

    - Create content tasks with `create_issue`
    - Track editorial progress with custom JIRA workflows
    - Search for content ready for publication

2. **Review Cycles**:

    - Add feedback as comments with `add_comment`
    - Attach revised content with `add_attachment`
    - Transition content through editorial states

3. **Content Planning**:
    - Use epics for content themes or campaigns
    - Get all related content pieces with `get_epic_children`
    - Track dependencies between content items

### Analytics and Reporting

-   Create automated reports on content performance
-   Track publication metrics through custom fields
-   Generate executive summaries of content production

## Advanced Features

The JIRA MCP server includes these powerful capabilities:

-   **Relationship Tracking**: Automatically detects and tracks issue mentions and links
-   **ADF Conversion**: Transforms complex Atlassian Document Format into clean plain text
-   **Optimized Payloads**: Returns only the necessary data to maximize context efficiency
-   **Parent/Child Tracking**: Maintains issue hierarchy information
-   **Concurrent API Requests**: Fetches related data in parallel for performance

## Limitations

-   Search results are limited to 50 issues per request
-   Epic children are limited to 100 issues per request
-   Rate limiting may apply based on your JIRA instance configuration
</file>

<file path="sidekick-studio/chatflows/tools-mcp/postgresql-mcp.md">
---
sidebar_position: 7
title: PostgreSQL MCP
description: Use PostgreSQL MCP to execute SQL queries against PostgreSQL databases
---

# PostgreSQL MCP for Answer Agent

This documentation outlines how to use the PostgreSQL Model Context Protocol (MCP) integration with Answer Agent. The MCP allows Answer Agent to interact with PostgreSQL databases, enabling query execution, data manipulation, and schema operations through natural language.

## Setting Up Credentials

To use the PostgreSQL MCP, you'll need to configure the following credentials:

### Obtaining Credentials

1. **Database Connection Details**:
    - PostgreSQL host (e.g., `localhost` or database server address)
    - Port (default: 5432)
    - Database name
    - Username
    - Password
    - SSL configuration (if needed)

### Configuration

You can provide these credentials in one of two ways:

1. **During conversation**: Answer Agent will prompt you for your credentials if not already configured.

2. **Via configuration file**: You can set up a permanent configuration by creating a file with:
    ```
    POSTGRES_HOST=your_host
    POSTGRES_PORT=5432
    POSTGRES_DB=your_database
    POSTGRES_USER=your_username
    POSTGRES_PASSWORD=your_password
    POSTGRES_SSL=true_or_false
    ```

>  **Warning**: These credentials provide direct access to your database. Use a dedicated user with appropriate permissions for this integration, and never use a superuser account.

## Available Tools

The PostgreSQL MCP provides the following tools:

### query

Executes read-only SQL queries against the connected database.

-   **Input**: `sql` (string) - The SQL query to execute
-   **Security**: All queries are executed within a READ ONLY transaction

Example:

```sql
SELECT * FROM users WHERE active = true LIMIT 10;
```

## Resources

The PostgreSQL MCP server automatically provides schema information for each table in the database:

-   **Table Schemas** (`postgres://<host>/<table>/schema`)
    -   JSON schema information for each table
    -   Includes column names and data types
    -   Automatically discovered from database metadata

## Common Workflows

1. **Exploring Database Structure**:

    - Query the available tables and their schemas
    - Examine column types and relationships

2. **Data Analysis**:

    - Execute complex queries for data exploration
    - Join tables to create reports
    - Filter data based on specific conditions

3. **Troubleshooting**:
    - Identify data inconsistencies
    - Validate data integrity across tables

> **Note**: The PostgreSQL MCP provides read-only access to your database. For operations that modify data, you'll need to use a different solution.
</file>

<file path="sidekick-studio/chatflows/tools-mcp/README.md">
---
title: Tools - MCP
description: Model Context Protocol integrations for Answer Agent
---

# Tools - MCP

## What is MCP?

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard allows models like OpenAI, Anthropic, Google, and Answer Agent to interact with external systems through standardized interfaces. MCP servers act as intermediaries that translate natural language requests into API calls, allowing models to:

1. **Query and manipulate data** in various systems
2. **Create and manage content** in CMS platforms
3. **Interact with productivity tools** like Jira and Slack
4. **Perform searches** across different data sources
5. **Execute database operations** securely

MCP servers follow a standardized protocol for exposing tools and handling requests, making it easy to add new integrations to Answer Agent.

## Available MCP Integrations

Answer Agent includes the following MCP integrations:

-   [Contentful](./contentful-mcp.md) - Manage content in Contentful CMS
-   [Salesforce](./salesforce-mcp.md) - Interact with Salesforce CRM and database
-   [Jira](./jira-mcp.md) - Work with Jira issues and projects
-   [Slack](./slack-mcp.md) - Send messages and interact with Slack
-   [Confluence](./confluence-mcp.md) - Query and update Confluence pages
-   [GitHub](./github-mcp.md) - Manage repositories, issues, and pull requests
-   [PostgreSQL](./postgresql-mcp.md) - Execute SQL queries against PostgreSQL databases
-   [BraveSearch](./bravesearch-mcp.md) - Perform internet searches using Brave Search
-   [Custom MCP](./custom-mcp.md) - Create custom integrations for proprietary systems

Each MCP integration provides specialized tools for working with its respective system. Click on an integration to learn more about its capabilities and how to use it.

## Common MCP Features

Most MCP integrations share these common features:

-   **Authentication** - Secure connection to external systems
-   **Content Creation** - Creating new records or documents
-   **Content Management** - Updating and deleting existing content
-   **Querying** - Searching and retrieving data
-   **Specialized Operations** - System-specific actions like publishing or workflow changes

## Getting Started

To use an MCP integration, you'll need:

1. **Credentials** for the external system
2. **Configuration details** like server URLs or project IDs
3. **Appropriate permissions** for the operations you want to perform

Refer to each integration's documentation for specific setup instructions.
</file>

<file path="sidekick-studio/chatflows/tools-mcp/salesforce-mcp.md">
---
sidebar_position: 2
title: Salesforce MCP
description: Use Salesforce MCP to interact with Salesforce CRM and database
---

# Salesforce MCP Server Documentation

## Introduction

The Salesforce MCP (Model Context Protocol) Server enables Answer Agent to interact with Salesforce data and metadata through natural language. This integration allows you to query, modify, and manage your Salesforce objects and records using conversational language.

## Setting Up Salesforce Credentials

### OAuth 2.0 Client Credentials Flow

1. **Create a Connected App in Salesforce**:

    - Navigate to **Setup** > **App Manager** > **New Connected App**
    - Fill in the required fields (Name, Email, etc.)
    - Enable OAuth Settings
    - Select "Enable OAuth Settings"
    - Set a callback URL (can be `https://localhost:8080/callback` if not using one)
    - Under "Selected OAuth Scopes", add:
        - Access and manage your data (api)
        - Access your basic information (id, profile, email, address, phone)
        - Perform requests on your behalf at any time (refresh_token, offline_access)
    - Save the application

2. **Get Client Credentials**:

    - After saving, navigate to **Manage** > **Edit Policies**
    - Set "OAuth Policies" > "Permitted Users" to "Admin approved users are pre-authorized"
    - Under "IP Relaxation", select "Relax IP restrictions"
    - Save the changes
    - Navigate to the **Manage Consumer Details** button to view your Client ID and Client Secret
    - Note down your Salesforce instance URL (e.g., `https://your-domain.my.salesforce.com`)

3. **Required Credentials**:
    - Client ID: From the connected app
    - Client Secret: From the connected app
    - Instance URL: Your exact Salesforce instance URL (e.g., `https://your-domain.my.salesforce.com`)

## Configuration in Answer Agent

Add your Salesforce credentials to Answer Agent's configuration:

### For OAuth 2.0 Client Credentials Flow:

```json
{
    "SALESFORCE_CONNECTION_TYPE": "OAuth_2.0_Client_Credentials",
    "SALESFORCE_CLIENT_ID": "your_client_id",
    "SALESFORCE_CLIENT_SECRET": "your_client_secret",
    "SALESFORCE_INSTANCE_URL": "https://your-domain.my.salesforce.com"
}
```

## Available Tools

### 1. salesforce_search_objects

**Description**: Search for Salesforce standard and custom objects by name pattern.

**Schema**:

```json
{
    "searchPattern": "string" // Search pattern to find objects
}
```

**Use Cases**:

-   **Developers**: Find API names of objects to use in code
-   **Publishers**: Discover available objects for content creation

### 2. salesforce_describe_object

**Description**: Get detailed schema metadata including all fields, relationships, and field properties of any Salesforce object.

**Schema**:

```json
{
    "objectName": "string" // API name of the object (e.g., 'Account', 'Contact', 'Custom_Object__c')
}
```

**Use Cases**:

-   **Developers**: Understand field types and relationships for integration development
-   **Publishers**: Research data structures for documentation creation

### 3. salesforce_query_records

**Description**: Query records from any Salesforce object using SOQL, including relationship queries.

**Schema**:

```json
{
  "objectName": "string", // API name of the object to query
  "fields": ["string"], // List of fields to retrieve, including relationship fields
  "whereClause": "string", // Optional WHERE clause
  "orderBy": "string", // Optional ORDER BY clause
  "limit": number // Optional maximum number of records to return
}
```

**Use Cases**:

-   **Developers**: Test queries and validate data access patterns
-   **Publishers**: Extract real data examples for documentation

### 4. salesforce_dml_records

**Description**: Perform data manipulation operations on Salesforce records (insert, update, delete, upsert).

**Schema**:

```json
{
    "operation": "string", // "insert", "update", "delete", or "upsert"
    "objectName": "string", // API name of the object
    "records": [{}], // Array of records to process
    "externalIdField": "string" // Optional external ID field name for upsert operations
}
```

**Use Cases**:

-   **Developers**: Test data manipulation operations and validate business logic
-   **Publishers**: Create sample data for demos or documentation

### 5. salesforce_manage_object

**Description**: Create new custom objects or modify existing ones in Salesforce.

**Schema**:

```json
{
    "operation": "string", // "create" or "update"
    "objectName": "string", // API name for the object (without __c suffix)
    "label": "string", // Label for the object
    "pluralLabel": "string", // Plural label for the object
    "description": "string", // Optional description
    "nameFieldLabel": "string", // Optional label for the name field
    "nameFieldType": "string", // Optional type of the name field ("Text" or "AutoNumber")
    "nameFieldFormat": "string", // Optional display format for AutoNumber field
    "sharingModel": "string" // Optional sharing model ("ReadWrite", "Read", "Private", "ControlledByParent")
}
```

**Use Cases**:

-   **Developers**: Create custom objects for application development
-   **Publishers**: Set up demonstration environments for tutorials

### 6. salesforce_manage_field

**Description**: Create new custom fields or modify existing fields on any Salesforce object.

**Schema**:

```json
{
  "operation": "string", // "create" or "update"
  "objectName": "string", // API name of the object to add/modify the field
  "fieldName": "string", // API name for the field (without __c suffix)
  "label": "string", // Optional label for the field
  "type": "string", // Field type (required for create)
  "required": boolean, // Optional whether the field is required
  "unique": boolean, // Optional whether the field value must be unique
  "externalId": boolean, // Optional whether the field is an external ID
  "length": number, // Optional length for text fields
  "precision": number, // Optional precision for numeric fields
  "scale": number, // Optional scale for numeric fields
  "referenceTo": "string", // Optional API name of the object to reference
  "relationshipLabel": "string", // Optional label for the relationship
  "relationshipName": "string", // Optional API name for the relationship
  "deleteConstraint": "string", // Optional delete constraint for Lookup fields
  "picklistValues": [{ "label": "string", "isDefault": boolean }], // Optional values for Picklist fields
  "description": "string" // Optional description of the field
}
```

**Use Cases**:

-   **Developers**: Add custom fields to objects for application functionality
-   **Publishers**: Create demo fields for tutorial environments

### 7. salesforce_search_all

**Description**: Search across multiple Salesforce objects using SOSL (Salesforce Object Search Language).

**Schema**:

```json
{
  "searchTerm": "string", // Text to search for (supports wildcards * and ?)
  "searchIn": "string", // Optional which fields to search in
  "objects": [{
    "name": "string", // API name of the object
    "fields": ["string"], // Fields to return for this object
    "where": "string", // Optional WHERE clause for this object
    "orderBy": "string", // Optional ORDER BY clause for this object
    "limit": number // Optional maximum number of records to return
  }],
  "withClauses": [{
    "type": "string", // WITH clause type
    "value": "string", // Optional value for the WITH clause
    "fields": ["string"] // Optional fields for SNIPPET clause
  }],
  "updateable": boolean, // Optional return only updateable records
  "viewable": boolean // Optional return only viewable records
}
```

**Use Cases**:

-   **Developers**: Find records across multiple objects for integration testing
-   **Publishers**: Gather comprehensive data examples across the platform

### 8. salesforce_read_apex

**Description**: Read Apex classes from Salesforce.

**Schema**:

```json
{
  "className": "string", // Optional name of a specific Apex class to read
  "namePattern": "string", // Optional pattern to match Apex class names
  "includeMetadata": boolean // Optional whether to include metadata about the Apex classes
}
```

**Use Cases**:

-   **Developers**: Review existing code for debugging or enhancement
-   **Publishers**: Extract code examples for documentation

### 9. salesforce_write_apex

**Description**: Create or update Apex classes in Salesforce.

**Schema**:

```json
{
    "operation": "string", // "create" or "update"
    "className": "string", // Name of the Apex class to create or update
    "apiVersion": "string", // Optional API version for the Apex class
    "body": "string" // Full body of the Apex class
}
```

**Use Cases**:

-   **Developers**: Deploy code changes or create new classes
-   **Publishers**: Create example implementations for tutorials

### 10. salesforce_read_apex_trigger

**Description**: Read Apex triggers from Salesforce.

**Schema**:

```json
{
  "triggerName": "string", // Optional name of a specific Apex trigger to read
  "namePattern": "string", // Optional pattern to match Apex trigger names
  "includeMetadata": boolean // Optional whether to include metadata about the Apex triggers
}
```

**Use Cases**:

-   **Developers**: Analyze existing triggers for debugging or enhancement
-   **Publishers**: Extract trigger examples for documentation

### 11. salesforce_write_apex_trigger

**Description**: Create or update Apex triggers in Salesforce.

**Schema**:

```json
{
    "operation": "string", // "create" or "update"
    "triggerName": "string", // Name of the Apex trigger to create or update
    "objectName": "string", // Optional name of the Salesforce object the trigger is for
    "apiVersion": "string", // Optional API version for the Apex trigger
    "body": "string" // Full body of the Apex trigger
}
```

**Use Cases**:

-   **Developers**: Deploy trigger changes or create new triggers
-   **Publishers**: Create example trigger implementations for tutorials

### 12. salesforce_execute_anonymous

**Description**: Execute anonymous Apex code in Salesforce.

**Schema**:

```json
{
    "apexCode": "string", // Apex code to execute anonymously
    "logLevel": "string" // Optional log level for debug logs
}
```

**Use Cases**:

-   **Developers**: Test code snippets without deploying
-   **Publishers**: Demonstrate code functionality in real-time

### 13. salesforce_manage_debug_logs

**Description**: Manage debug logs for Salesforce users.

**Schema**:

```json
{
  "operation": "string", // "enable", "disable", or "retrieve"
  "username": "string", // Username of the Salesforce user
  "logLevel": "string", // Optional log level for debug logs
  "expirationTime": number, // Optional minutes until expiration
  "limit": number, // Optional maximum number of logs to retrieve
  "logId": "string", // Optional ID of a specific log to retrieve
  "includeBody": boolean // Optional whether to include the full log content
}
```

**Use Cases**:

-   **Developers**: Set up and monitor debugging sessions
-   **Publishers**: Capture detailed system behavior for documentation

## Common Use Case Examples

### For Developers

1. **Schema Exploration**:

    ```
    "Describe the Lead object and show me all its fields"
    ```

2. **Data Querying**:

    ```
    "Find all opportunities closing this month with amount greater than $10,000"
    ```

3. **Code Analysis**:

    ```
    "Show me all Apex triggers related to the Account object"
    ```

4. **Testing New Features**:

    ```
    "Create a test Contact with first name 'John' and last name 'Doe'"
    ```

5. **Debugging**:
    ```
    "Enable finest debug logs for my user account for the next 30 minutes"
    ```

### For Publishers

1. **Documentation Research**:

    ```
    "Find all custom objects related to invoicing and describe their fields"
    ```

2. **Creating Examples**:

    ```
    "Create a simple Apex class that demonstrates how to query Accounts"
    ```

3. **Exploring Relationships**:

    ```
    "Show me how Accounts are related to Contacts and Opportunities"
    ```

4. **Finding Content Topics**:

    ```
    "What are all the picklist values for Lead Status?"
    ```

5. **Setting Up Demo Environments**:
    ```
    "Create a basic Project_Tracking__c custom object with Task__c and Status__c fields"
    ```

## Best Practices

1. **Permission Settings**: Ensure the credentials used have appropriate permissions for the operations you want to perform.

2. **Data Security**: Be cautious with DML operations (insert, update, delete) in production environments.

3. **Error Handling**: Pay attention to error messages which often contain specific details about what went wrong.

4. **Resource Usage**: Complex SOQL queries or operations on large data volumes should be monitored for performance impact.

5. **Test Environment**: When possible, use a sandbox environment for testing before executing operations in production.
</file>

<file path="sidekick-studio/chatflows/tools-mcp/slack-mcp.md">
---
sidebar_position: 4
title: Slack MCP
description: Use Slack MCP to send messages and interact with Slack
---

# Slack MCP for Answer Agent

This documentation outlines how to use the Slack Model Context Protocol (MCP) integration with Answer Agent. The MCP allows Answer Agent to interact with Slack's API, enabling message sending, channel management, and user operations through natural language.

## Setting Up Credentials

To use the Slack MCP, you'll need to configure the following credentials:

### Obtaining Credentials

1. **Slack Bot Token**:
    - Go to [https://api.slack.com/apps](https://api.slack.com/apps)
    - Create a new app or select an existing one
    - Navigate to "OAuth & Permissions"
    - Add necessary bot scopes:
        - channels:history - View messages and other content in public channels
        - channels:read - View basic channel information
        - chat:write - Send messages as the app
        - reactions:write - Add emoji reactions to messages
        - users:read - View users and their basic information
        - users.profile:read - View detailed profiles about users
    - Install the app to your workspace
    - Copy the Bot User OAuth Token

### Configuration

You can provide these credentials in one of two ways:

1. **During conversation**: Answer Agent will prompt you for your credentials if not already configured.

2. **Via configuration file**: You can set up a permanent configuration by creating a file with:
    ```
    SLACK_BOT_TOKEN=your_token_here
    ```

>  **Warning**: Your bot token has the permissions you granted during app creation. Use appropriate scopes based on your needs.

## Available Tools

The Slack MCP provides the following tools for interacting with Slack:

### `slack_list_channels`

List public or pre-defined channels in the workspace.

**Optional inputs:**

-   `limit` (number, default: 100, max: 200): Maximum number of channels to return
-   `cursor` (string): Pagination cursor for next page

**Returns:** List of channels with their IDs and information

### `slack_post_message`

Post a new message to a Slack channel.

**Required inputs:**

-   `channel_id` (string): The ID of the channel to post to
-   `text` (string): The message text to post

**Returns:** Message posting confirmation and timestamp

### `slack_reply_to_thread`

Reply to a specific message thread.

**Required inputs:**

-   `channel_id` (string): The channel containing the thread
-   `thread_ts` (string): Timestamp of the parent message
-   `text` (string): The reply text

**Returns:** Reply confirmation and timestamp

### `slack_add_reaction`

Add an emoji reaction to a message.

**Required inputs:**

-   `channel_id` (string): The channel containing the message
-   `timestamp` (string): Message timestamp to react to
-   `reaction` (string): Emoji name without colons

**Returns:** Reaction confirmation

### `slack_get_channel_history`

Get recent messages from a channel.

**Required inputs:**

-   `channel_id` (string): The channel ID

**Optional inputs:**

-   `limit` (number, default: 10): Number of messages to retrieve

**Returns:** List of messages with their content and metadata

### `slack_get_thread_replies`

Get all replies in a message thread.

**Required inputs:**

-   `channel_id` (string): The channel containing the thread
-   `thread_ts` (string): Timestamp of the parent message

**Returns:** List of replies with their content and metadata

### `slack_get_users`

Get list of workspace users with basic profile information.

**Optional inputs:**

-   `cursor` (string): Pagination cursor for next page
-   `limit` (number, default: 100, max: 200): Maximum users to return

**Returns:** List of users with their basic profiles

### `slack_get_user_profile`

Get detailed profile information for a specific user.

**Required inputs:**

-   `user_id` (string): The user's ID

**Returns:** Detailed user profile information

## Setup

### Create a Slack App

1. Visit the [Slack Apps page](https://api.slack.com/apps)
2. Click "Create New App"
3. Choose "From scratch"
4. Name your app and select your workspace

### Configure Bot Token Scopes

Navigate to "OAuth & Permissions" and add these scopes:

-   `channels:history` - View messages and other content in public channels
-   `channels:read` - View basic channel information
-   `chat:write` - Send messages as the app
-   `reactions:write` - Add emoji reactions to messages
-   `users:read` - View users and their basic information
-   `users.profile:read` - View detailed profiles about users

### Install App to Workspace

1. Click "Install to Workspace" and authorize the app
2. Save the "Bot User OAuth Token" that starts with `xoxb-`
3. Get your Team ID (starts with a T) by following Slack's guidance

## Environment Variables

-   **SLACK_BOT_TOKEN**: Required. The Bot User OAuth Token starting with `xoxb-`.
-   **SLACK_TEAM_ID**: Required. Your Slack workspace ID starting with T.
-   **SLACK_CHANNEL_IDS**: Optional. Comma-separated list of channel IDs to limit channel access (e.g., "C01234567, C76543210"). If not set, all public channels will be listed.

## Troubleshooting

If you encounter permission errors, verify that:

-   All required scopes are added to your Slack app
-   The app is properly installed to your workspace
-   The tokens and workspace ID are correctly copied to your configuration
-   The app has been added to the channels it needs to access
</file>

<file path="sidekick-studio/chatflows/utilities/custom-js-function.md">
---
description: Execute custom JavaScript functions in your AnswerAgentAI workflows
---

# Custom JS Function

## Overview

The Custom JS Function node allows you to execute custom JavaScript code within your AnswerAgentAI workflows. This powerful utility enables you to perform complex operations, data manipulations, and integrate external libraries into your chatflows.

## Key Benefits

-   Flexibility to implement custom logic and algorithms
-   Ability to integrate external JavaScript libraries
-   Powerful tool for data transformation and manipulation

## How to Use

1. Add the Custom JS Function node to your workflow canvas.
2. Configure the node with the following inputs:

    - Input Variables (optional): Define input variables that can be used in your function.
    - Function Name (optional): Give your function a descriptive name.
    - JavaScript Function: Write your custom JavaScript code in this field.

3. Connect the node to other components in your workflow as needed.

<!-- TODO: Screenshot of the Custom JS Function node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/customjsfunction.png" alt="" /><figcaption><p> Custom JS Function node   &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Use the `$input` variable to access the input data passed to the node.
2. Utilize `$vars` to access variables from the AnswerAgentAI environment.
3. Access flow-specific information using the `$flow` object, which contains `chatflowId`, `sessionId`, `chatId`, and `input`.
4. When defining input variables, use the `$` prefix to access them in your code (e.g., `$myVariable`).
5. You can use both built-in Node.js modules and external npm packages in your custom functions, depending on your AnswerAgentAI configuration.

## Troubleshooting

1. Invalid JSON Error: If you encounter an "Invalid JSON" error when using input variables, double-check that your JSON is correctly formatted.
2. Undefined Variables: Ensure that all variables used in your function are properly defined or passed as input variables.
3. External Module Issues: If you're trying to use an external module and encountering errors, verify that the module is included in your AnswerAgentAI configuration's allowed dependencies.

## Example

Here's a simple example of a custom function that concatenates two strings:

```javascript
const result = $firstName + ' ' + $lastName
return result
```

In this example, `$firstName` and `$lastName` should be defined as input variables when configuring the node.

<!-- TODO: Screenshot showing the example code in the node configuration and the corresponding input variables setup -->
<figure><img src="/.gitbook/assets/screenshots/customjsfunctionexample.png" alt="" /><figcaption><p> Custom JS Function Example   &#x26; Drop UI</p></figcaption></figure>

Remember to test your custom functions thoroughly to ensure they work as expected within your AnswerAgentAI workflows.
</file>

<file path="sidekick-studio/chatflows/utilities/if-else.md">
---
description: Split flows based on If Else javascript functions
---

# IfElse Function

## Overview

The IfElse Function is a powerful utility node in AnswerAgentAI that allows you to create conditional logic within your workflows. This node evaluates a condition and directs the flow based on whether the condition is true or false.

## Key Benefits

-   Create dynamic, branching workflows based on specific conditions
-   Implement custom logic using JavaScript functions
-   Easily integrate with other nodes and variables in your AnswerAgentAI workflow

## How to Use

1. Add the IfElse Function node to your canvas in the AnswerAgentAI Studio.
2. Configure the node's settings:
   a. Input Variables (optional): Define any variables you want to use in your functions.
   b. IfElse Name (optional): Give your condition a descriptive name.
   c. If Function: Write a JavaScript function that returns true or false.
   d. Else Function: Write a JavaScript function to execute if the If condition is false.
3. Connect the node's outputs ("True" and "False") to the appropriate next steps in your workflow.

<!-- TODO: Add a screenshot of the IfElse Function node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/ifelsefunction.png" alt="" /><figcaption><p> Ifelse Function Example   &#x26; Drop UI</p></figcaption></figure>

### Writing Functions

Both the If and Else functions should return a value. Here's an example of how to write these functions:

If Function:

```javascript
if ('hello' == 'hello') {
    return true
}
```

Else Function:

```javascript
return false
```

## Tips and Best Practices

1. Use meaningful variable names to make your functions more readable.
2. Leverage the `$input` variable to access the input passed to the node.
3. Utilize `$vars` to access variables from previous nodes in the workflow.
4. Use `$flow` to access information about the current chatflow, session, and input.
5. Keep your functions simple and focused on a single condition or task.

## Troubleshooting

1. **Syntax Errors**: Ensure your JavaScript code is valid and free of syntax errors.
2. **Undefined Variables**: Check that all variables used in your functions are properly defined or passed as input variables.
3. **Incorrect Output**: Verify that your functions are returning the expected values (true/false for the If function, and the desired output for both functions).

<!-- TODO: Add a screenshot showing an example of a complete IfElse Function node setup in a workflow -->
<figure><img src="/.gitbook/assets/screenshots/ifelseinworkflow.png" alt="" /><figcaption><p> Ifelse Function Example   &#x26; Drop UI</p></figcaption></figure>

Remember, the IfElse Function node is a powerful tool for creating dynamic workflows in AnswerAgentAI. By mastering its use, you can create sophisticated, responsive chatbots and applications that adapt to various conditions and user inputs.
</file>

<file path="sidekick-studio/chatflows/utilities/README.md">
---
description: Utility Nodes for Enhanced Workflow Control
---

# Utility Nodes

Utility nodes are powerful development tools that allow you to implement custom JavaScript functions, manage variables, control flow logic, and add notes within your AnswerAgentAI workflows. These nodes enhance the flexibility and functionality of your workflows, enabling you to create more complex and customized solutions.

## Overview

Utility nodes in AnswerAgentAI provide a set of versatile tools that can be added to the canvas of Sidekick workflows in the Sidekick Studio. These nodes help you extend the capabilities of your workflows beyond the standard node functionalities.

## Key Benefits

-   Enhance workflow flexibility with custom logic and data manipulation
-   Improve workflow organization and readability
-   Facilitate debugging and troubleshooting of complex workflows

## Available Utility Nodes

### 1. Custom JS Function

The Custom JS Function node allows you to write and execute custom JavaScript code within your workflow.

<!-- TODO: Screenshot of the Custom JS Function node in the Sidekick Studio -->
<figure><img src="/.gitbook/assets/screenshots/customjsfunction.png" alt="" /><figcaption><p> Custom JS Function node   &#x26; Drop UI</p></figcaption></figure>

[Learn more about Custom JS Function](custom-js-function.md)

### 2. Set/Get Variable

The Set/Get Variable node enables you to store and retrieve data throughout your workflow, making it easier to manage and manipulate information across different nodes.

<!-- TODO: Screenshot of the Set/Get Variable node in the Sidekick Studio -->
<figure><img src="/.gitbook/assets/screenshots/setvariable.png" alt="" /><figcaption><p>Set/Get Variable node   &#x26; Drop UI</p></figcaption></figure>

[Learn more about Set/Get Variable](set-get-variable.md)

### 3. If Else

The If Else node provides conditional branching in your workflow, allowing you to create different paths based on specific conditions.

<!-- TODO: Screenshot of the If Else node in the Sidekick Studio -->
<figure><img src="/.gitbook/assets/screenshots/ifelsefunction.png" alt="" /><figcaption><p>If Else  node   &#x26; Drop UI</p></figcaption></figure>

[Learn more about If Else](if-else.md)

### 4. Sticky Note

The Sticky Note node lets you add comments and notes directly on the workflow canvas, improving organization and documentation of your workflow.

<!-- TODO: Screenshot of the Sticky Note node in the Sidekick Studio -->

<figure><img src="/.gitbook/assets/screenshots/stickynote.png" alt="" /><figcaption><p>Sticky Note  node   &#x26; Drop UI</p></figcaption></figure>
[Learn more about Sticky Note](sticky-note.md)

By mastering the use of Utility Nodes, you can create more powerful, flexible, and maintainable workflows in AnswerAgentAI, tailoring your solutions to meet specific requirements and challenges.
</file>

<file path="sidekick-studio/chatflows/utilities/set-get-variable.md">
---
description: Set and Get Variable Nodes for Dynamic Data Management
---

# Set and Get Variable Nodes

## Overview

The Set Variable and Get Variable nodes are powerful utility tools in AnswerAgentAI that allow you to store and retrieve data dynamically within your workflows. These nodes are essential for managing information across different parts of your flow, enabling you to save computation time and create more efficient workflows.

## Key Benefits

-   Reuse data across different parts of your workflow without recomputation
-   Improve workflow efficiency and reduce redundant operations
-   Easily manage and access dynamic data throughout your flow

## How to Use

### Set Variable Node

The Set Variable node allows you to store data under a specific variable name for later use.

1. Add a Set Variable node to your canvas.
2. Connect the output of any node that produces data (string, number, boolean, JSON, or array) to the Set Variable node's input.
3. In the node settings:
    - Enter a unique "Variable Name" (e.g., "myData").
    - The "Input" field will automatically populate with the connected data.

<!-- TODO: Screenshot of Set Variable node configuration -->
<figure><img src="/.gitbook/assets/screenshots/setvariable.png" alt="" /><figcaption><p> Set Variable Node   &#x26; Drop UI</p></figcaption></figure>
### Get Variable Node

The Get Variable node allows you to retrieve previously stored data using the variable name.

1. Add a Get Variable node to your canvas where you need to use the stored data.
2. In the node settings:
    - Enter the "Variable Name" that matches the one used in the Set Variable node (e.g., "myData").
3. Connect the Get Variable node's output to any node that requires this data as input.

<!-- TODO: Screenshot of Get Variable node configuration -->
<figure><img src="/.gitbook/assets/screenshots/getvariable.png" alt="" /><figcaption><p> Get Variable Node   &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Use descriptive variable names to easily identify the stored data (e.g., "userInput" or "apiResponse").
2. Ensure that variable names are unique within your workflow to avoid conflicts.
3. Use Set Variable nodes strategically to store results from computationally expensive operations or API calls.
4. Remember that variables are only available during the runtime of your workflow and do not persist between different runs.

## Troubleshooting

1. If a Get Variable node returns undefined:

    - Check if the variable name matches exactly with the one used in the Set Variable node.
    - Ensure that the Set Variable node is executed before the Get Variable node in your workflow.

2. If you're not seeing the expected data type in the Get Variable node output:
    - Verify the data type being stored in the Set Variable node.
    - Check if any transformations are applied to the data between setting and getting the variable.

By effectively using the Set and Get Variable nodes, you can create more dynamic and efficient workflows in AnswerAgentAI, allowing for better data management and reusability across your chatbot or application logic.
</file>

<file path="sidekick-studio/chatflows/utilities/sticky-note.md">
---
description: Add notes and comments to your workflow
---

# Sticky Note

## Overview

The Sticky Note utility node allows you to add notes, comments, or reminders directly to your AnswerAgentAI workflow canvas. This feature is useful for documenting your workflow, leaving reminders for yourself or team members, or providing context for specific parts of your flow.

## Key Benefits

-   Improve workflow documentation and readability
-   Facilitate collaboration by leaving notes for team members
-   Organize thoughts and ideas within your workflow

## How to Use

1. Locate the Sticky Note node in the Utilities section of the node palette.
2. Drag and drop the Sticky Note node onto your workflow canvas.
3. Click on the Sticky Note node to open its properties.
4. In the note field, type your desired text or comment.
5. Adjust the size and position of the Sticky Note on the canvas as needed.

<!-- TODO: Add a screenshot of the Sticky Note node on the canvas with its properties panel open -->
<figure><img src="/.gitbook/assets/screenshots/stickynote.png" alt="" /><figcaption><p> Sticky Node   &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

-   Use different colors for Sticky Notes to categorize or prioritize information.
-   Keep notes concise and to the point for better readability.
-   Use Sticky Notes to explain complex parts of your workflow or to highlight important considerations.
-   Regularly review and update your Sticky Notes to keep information current.

## Troubleshooting

-   If your Sticky Note text is not visible, ensure that you have entered text in the note field and that the node is large enough to display the content.
-   If you're having trouble moving or resizing the Sticky Note, make sure you're not in a locked or read-only view of the workflow.

Remember, Sticky Notes are visual aids and do not affect the functionality of your workflow. They are purely for documentation and organization purposes.
</file>

<file path="sidekick-studio/chatflows/vector-stores/astradb.md">
---
description: Astra Vector Store for efficient similarity search and retrieval
---

# Astra Vector Store

## Overview

The Astra Vector Store node in AnswerAgentAI allows you to store and retrieve embedded data using DataStax Astra DB, a serverless vector database designed for managing mission-critical AI workloads. This node enables you to perform similarity or MMR (Maximal Marginal Relevance) searches on your embedded data.

## Key Benefits

-   Efficient storage and retrieval of high-dimensional vector data
-   Seamless integration with DataStax Astra DB for scalable vector operations
-   Supports various similarity metrics for flexible search capabilities

## How to Use

1. Add the Astra Vector Store node to your AnswerAgentAI workflow canvas.
2. Connect the necessary input nodes (e.g., Document and Embeddings).
3. Configure the node parameters:
    - Connect Credential: Select your Astra DB API credential.
    - Document: Connect a Document node (optional).
    - Embeddings: Connect an Embeddings node.
    - Namespace: Enter the Astra DB namespace.
    - Collection: Specify the Astra DB collection name.
    - Vector Dimension: Set the dimension for storing vector embeddings (default: 1536).
    - Similarity Metric: Choose from 'cosine', 'euclidean', or 'dot_product' (default: cosine).
    - Top K: Set the number of top results to fetch (default: 4).
4. Configure additional MMR parameters if needed.
5. Run your workflow to store or retrieve vector data.

<!-- TODO: Add a screenshot of the Astra Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/astravectorstore.png" alt="" /><figcaption><p> Astra Vector Store Node   &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Ensure your Astra DB API credentials are correctly set up in AnswerAgentAI before using this node.
2. Choose the appropriate similarity metric based on your specific use case and data characteristics.
3. Experiment with different 'Top K' values to find the optimal number of results for your application.
4. When using the MMR search, adjust the lambda parameter to balance between relevance and diversity in the results.

## Troubleshooting

1. Invalid Similarity Metric error:

    - Make sure you've selected one of the supported metrics: 'cosine', 'euclidean', or 'dot_product'.

2. Connection issues:

    - Verify that your Astra DB API credentials are correct and that you have the necessary permissions.
    - Check your network connection and ensure that the Astra DB endpoint is accessible.

3. Dimension mismatch:
    - Ensure that the Vector Dimension parameter matches the dimension of your embedding model.

If you encounter any other issues, double-check your node configuration and input connections. For persistent problems, consult the AnswerAgentAI documentation or reach out to support.
</file>

<file path="sidekick-studio/chatflows/vector-stores/chroma.md">
---
description: Chroma Vector Store for efficient similarity search and data retrieval
---

# Chroma Vector Store

## Overview

The Chroma Vector Store node in AnswerAgentAI allows you to store and retrieve high-dimensional vectors efficiently. It's designed for embedding and indexing data, enabling fast similarity searches. This node is particularly useful for tasks involving natural language processing, recommendation systems, and other AI applications that require quick retrieval of similar items.

## Key Benefits

-   Efficient storage and retrieval of high-dimensional vectors
-   Fast similarity searches for embedded data
-   Seamless integration with other AnswerAgentAI components

## How to Use

1. Add the Chroma Vector Store node to your workflow canvas.
2. Configure the node with the following inputs:
    - Document: (Optional) The document or list of documents to be embedded and stored.
    - Embeddings: The embedding model to use for converting documents into vectors.
    - Record Manager: (Optional) Keeps track of records to prevent duplication.
    - Collection Name: A unique name for your Chroma collection.
    - Chroma URL: (Optional) The URL of your Chroma instance if using a remote server.
    - Chroma Metadata Filter: (Optional) A JSON object to filter metadata during retrieval.
    - Top K: (Optional) The number of top results to fetch (default is 4).
3. Connect the node to your workflow, ensuring that the required inputs are properly linked.
4. Run your workflow to store or retrieve data using the Chroma Vector Store.

<!-- TODO: Add a screenshot of the Chroma Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/chromavectorstore.png" alt="" /><figcaption><p> Chroma Vector Store Node   &#x26; Drop UI</p></figcaption></figure>
## Tips and Best Practices

1. Choose appropriate embeddings: Select an embedding model that best suits your data and use case.
2. Use meaningful collection names: Choose descriptive names for your collections to easily manage multiple vector stores.
3. Optimize retrieval: Adjust the "Top K" value based on your specific needs for precision vs. recall.
4. Leverage metadata filtering: Use the Chroma Metadata Filter to narrow down search results based on specific criteria.

## Troubleshooting

1. Connection issues:

    - Ensure that the Chroma URL is correct if using a remote instance.
    - Check that your AnswerAgentAI instance has network access to the Chroma server.

2. Slow performance:

    - Consider increasing the resources allocated to your Chroma instance.
    - Optimize your queries and reduce the size of your vector store if possible.

3. Unexpected results:
    - Verify that the correct embedding model is being used.
    - Double-check your metadata filters for any errors in the JSON structure.

<!-- TODO: Add a screenshot showing an example of a successful Chroma Vector Store query result -->
<figure><img src="/.gitbook/assets/screenshots/chromainaworkflow.png" alt="" /><figcaption><p> Chroma Vector Store Node In a workflow   &#x26; Drop UI</p></figcaption></figure>
By using the Chroma Vector Store node in AnswerAgentAI, you can efficiently manage and query large amounts of embedded data, enhancing your AI workflows with powerful similarity search capabilities.
</file>

<file path="sidekick-studio/chatflows/vector-stores/elastic.md">
---
description: Upsert embedded data and perform similarity search using Elasticsearch
---

# Elasticsearch Vector Store

## Overview

The Elasticsearch Vector Store node allows you to store and retrieve embedded data using Elasticsearch, a powerful distributed search and analytics engine. This node enables you to perform similarity searches on your vector data, making it ideal for applications that require efficient retrieval of semantically similar information.

## Key Benefits

-   Efficient similarity search: Quickly find and retrieve the most relevant vector data based on similarity measures.
-   Scalable storage: Leverage Elasticsearch's distributed architecture for storing large amounts of vector data.
-   Flexible integration: Easily integrate with other AnswerAgentAI components for advanced AI workflows.

## How to Use

1. Add the Elasticsearch Vector Store node to your AnswerAgentAI canvas.
2. Connect the required input nodes:
    - Document (optional): Connect a Document node if you want to add new documents to the vector store.
    - Embeddings: Connect an Embeddings node to specify the embedding model for your vectors.
    - Record Manager (optional): Connect a Record Manager node to prevent duplication and manage records.
3. Configure the node parameters:
    - Index Name: Enter the name of the Elasticsearch index you want to use.
    - Top K: Specify the number of top results to fetch (default is 4).
    - Similarity: Choose the similarity measure (l2_norm, dot_product, or cosine).
4. Connect the Elasticsearch Vector Store node's output to other nodes in your workflow.

<!-- TODO: Add a screenshot of the Elasticsearch Vector Store node with its input and output connections -->
<figure><img src="/.gitbook/assets/screenshots/elasticcsearch.png" alt="" /><figcaption><p> ElasticSearch Vector Store Node    &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Use a consistent embedding model: Ensure you use the same embedding model for storing and querying vectors to maintain consistency in your similarity searches.
-   Index optimization: Consider optimizing your Elasticsearch index settings for vector search performance, especially for large datasets.
-   Security: Always use secure credentials and follow Elasticsearch security best practices when setting up your connection.

## Troubleshooting

-   Connection issues: If you're having trouble connecting to Elasticsearch, double-check your endpoint URL, cloud ID, and authentication credentials.
-   Performance problems: If searches are slow, consider increasing the number of shards in your Elasticsearch index or optimizing your index settings.
-   Indexing errors: Ensure that your document format is correct and that all required fields are present before indexing.

<!-- TODO: Add a screenshot showing common error messages and their solutions -->
</file>

<file path="sidekick-studio/chatflows/vector-stores/faiss.md">
---
description: Upsert embedded data and perform similarity search using Faiss library from Meta
---

# Faiss Vector Store

## Overview

The Faiss Vector Store node in AnswerAgentAI allows you to store and retrieve high-dimensional vectors efficiently using the Faiss library developed by Meta. This node is particularly useful for similarity search operations on large datasets of embedded documents.

## Key Benefits

-   Fast and efficient similarity search on large datasets
-   Seamless integration with other AnswerAgentAI components
-   Supports both document storage and retrieval operations

## How to Use

1. Add the Faiss Vector Store node to your AnswerAgentAI canvas.
2. Connect the required input nodes:

    - Document: (Optional) Connect a Document node if you want to add new documents to the vector store.
    - Embeddings: Connect an Embeddings node to provide the embedding model for vectorizing the documents.
    - Base Path: Specify the file path where the Faiss index will be saved or loaded from.

3. Configure the node:

    - Top K: (Optional) Set the number of top results to fetch during similarity search. Default is 4.

4. Connect the output to other nodes in your workflow:
    - Faiss Retriever: Use this output for document retrieval operations.
    - Faiss Vector Store: Use this output for direct access to the vector store.

<!-- TODO: Add a screenshot of the Faiss Vector Store node with its inputs and outputs connected -->
<figure><img src="/.gitbook/assets/screenshots/faiss.png" alt="" /><figcaption><p> Faiss Vector Store Node    &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure that the Base Path is set to a directory where you have write permissions.
2. When using the Faiss Vector Store for the first time, provide Document input to create and populate the index.
3. For subsequent uses, you can load an existing index by providing only the Base Path and Embeddings inputs.
4. Adjust the Top K value based on your specific use case and the size of your document collection.

## Troubleshooting

1. If you encounter an error related to the Faiss index, ensure that the Base Path is correct and that the directory exists.
2. If the similarity search returns fewer results than expected, check if the Top K value is set appropriately and if your index contains enough documents.
3. In case of "illegal invocation" errors, make sure you're using the latest version of AnswerAgentAI, as this issue has been addressed in recent updates.

<!-- TODO: Add a screenshot showing the configuration panel of the Faiss Vector Store node -->
<figure><img src="/.gitbook/assets/screenshots/faissconfiguration.png" alt="" /><figcaption><p> Faiss Vector Store Configuration Panel    &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/vector-stores/in-memory-vector-store.md">
---
description: In-Memory Vector Store for AnswerAgentAI
---

# In-Memory Vector Store

## Overview

The In-Memory Vector Store is a fundamental component in AnswerAgentAI's workflow system. It provides a quick and easy way to store and retrieve vector embeddings directly in memory. This feature is particularly useful for testing, prototyping, and small-scale applications where persistence is not required.

## Key Benefits

-   **Quick Setup**: Easily integrate into your workflow for immediate use.
-   **Efficient for Small Datasets**: Ideal for testing and small-scale applications.
-   **No External Dependencies**: Runs entirely in memory, requiring no additional database setup.

## How to Use

1. Add the "In-Memory Vector Store" node to your AnswerAgentAI canvas.
2. Connect a Document source to the "Document" input (optional).
3. Connect an Embeddings model to the "Embeddings" input.
4. Set the "Top K" value if you want to change the default number of results (optional).
5. Choose the output type: either "Memory Retriever" or "Memory Vector Store".

<!-- TODO: Screenshot of the In-Memory Vector Store node on the AnswerAgentAI canvas with inputs and outputs connected -->

<figure><img src="/.gitbook/assets/screenshots/inmemoryvectorstore.png" alt="" /><figcaption><p> In-Memory Vector Store   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. **Testing and Prototyping**: Use this vector store for initial testing and prototyping of your workflows.
2. **Small Datasets**: Ideal for working with small to medium-sized datasets that can fit comfortably in memory.
3. **Temporary Storage**: Remember that data is not persisted and will be lost when the application restarts.
4. **Performance Consideration**: For larger datasets or production use, consider switching to a persistent vector store.

## Troubleshooting

1. **Out of Memory Errors**: If you encounter out of memory errors, consider reducing the size of your dataset or switching to a disk-based vector store.
2. **Slow Performance with Large Datasets**: For larger datasets, performance may degrade. In such cases, it's recommended to use a more scalable vector store solution.

## Important Note for Production Use

While the In-Memory Vector Store is an excellent tool for getting started quickly and for testing purposes, it is not recommended for production use with large datasets or when data persistence is required. For production environments, consider using more robust and persistent storage solutions available in AnswerAgentAI, such as Pinecone, Weaviate, or other database-backed vector stores. These will provide better scalability, persistence, and performance for larger-scale applications.

When you're ready to move beyond testing and prototyping, explore AnswerAgentAI's other vector store options that offer long-term storage and are better suited for production workflows.
</file>

<file path="sidekick-studio/chatflows/vector-stores/milvus.md">
---
description: Milvus Vector Store for AnswerAgentAI
---

# Milvus Vector Store

## Overview

The Milvus Vector Store node in AnswerAgentAI allows you to store and retrieve embedded data using Milvus, an advanced open-source vector database. This node enables efficient similarity searches on your vector data, making it ideal for various AI and machine learning applications.

## Key Benefits

-   Efficient similarity search: Quickly find the most relevant data based on vector similarity.
-   Scalable: Designed to handle large-scale vector data with high performance.
-   Flexible: Supports various index types and search parameters for optimized retrieval.

## How to Use

1. Add the Milvus Vector Store node to your AnswerAgentAI workflow canvas.
2. Configure the node with the following parameters:
    - Document: (Optional) The input documents to be stored in the vector database.
    - Embeddings: The embedding model to use for converting text to vectors.
    - Milvus Server URL: The URL of your Milvus server (e.g., `http://localhost:19530`.
    - Milvus Collection Name: The name of the collection to store your vectors.
    - Milvus Text Field: (Optional) The field name for storing text data.
    - Milvus Filter: (Optional) A filter string for querying data.
    - Top K: (Optional) The number of top results to fetch (default is 4).
3. Connect the node to your desired input and output nodes in the workflow.

<!-- TODO: Add a screenshot of the Milvus Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/milvus.png" alt="" /><figcaption><p> Milvus Vector Store node configuration panel  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure your Milvus server is properly set up and running before using this node.
2. Choose appropriate embedding models that suit your data and use case.
3. Experiment with different filter strings to refine your search results.
4. Adjust the Top K value based on your specific needs for precision vs. recall.

## Troubleshooting

1. Connection issues:

    - Verify that the Milvus Server URL is correct and the server is running.
    - Check if any firewall or network settings are blocking the connection.

2. Performance problems:

    - Ensure your Milvus server has sufficient resources (CPU, RAM, storage).
    - Consider optimizing your index settings for better search performance.

3. Unexpected search results:
    - Double-check your filter string syntax for any errors.
    - Verify that the embedded data is correctly stored in the specified collection.

If you encounter any persistent issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.

<!-- TODO: Add a screenshot of a sample workflow using the Milvus Vector Store node -->
<figure><img src="/.gitbook/assets/screenshots/milvusinaworkflow.png" alt="" /><figcaption><p> Milvus Vector Store node In a workflow  &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/vector-stores/mongodb-atlas.md">
---
description: MongoDB Atlas Vector Store for AnswerAgentAI
---

# MongoDB Atlas Vector Store

## Overview

The MongoDB Atlas Vector Store node in AnswerAgentAI allows you to store, retrieve, and search vector embeddings using MongoDB Atlas, a managed cloud MongoDB database. This node enables efficient similarity searches and supports upsert operations for embedded data.

## Key Benefits

-   Seamless integration with MongoDB Atlas for vector storage and retrieval
-   Efficient similarity and MMR (Maximal Marginal Relevance) search capabilities
-   Easy management of document embeddings in a cloud-based environment

## How to Use

1. Add the MongoDB Atlas Vector Store node to your AnswerAgentAI canvas.
2. Configure the node with the following parameters:

    - Document: (Optional) Input the documents you want to store in the vector database.
    - Embeddings: Select the embeddings model to use for vectorizing the documents.
    - Database: Enter the name of your MongoDB Atlas database.
    - Collection Name: Specify the name of the collection to store the vectors.
    - Index Name: Provide the name of the vector index in MongoDB Atlas.
    - Content Field: (Optional) Name of the field that contains the actual content (default: "text").
    - Embedded Field: (Optional) Name of the field that contains the embedding (default: "embedding").
    - Top K: (Optional) Number of top results to fetch (default: 4).

3. Connect the node to your credential containing the MongoDB Atlas connection URL.
4. Connect the output to other nodes in your workflow that require vector storage or retrieval.

<!-- TODO: Add a screenshot showing the MongoDB Atlas Vector Store node configuration -->
<figure><img src="/.gitbook/assets/screenshots/mongodbvectorstore.png" alt="" /><figcaption><p> MongoDB Atlas Vector Store node configuration  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure your MongoDB Atlas cluster is properly configured with vector search capabilities.
2. Use meaningful names for your database, collection, and index to easily manage your vector data.
3. Experiment with different "Top K" values to find the optimal number of results for your use case.
4. Consider using the MMR search option for more diverse results when querying the vector store.

## Troubleshooting

1. Connection issues:

    - Verify that your MongoDB Atlas connection URL is correct and that your IP address is whitelisted in the Atlas configuration.
    - Ensure that your Atlas cluster has the necessary permissions set up for read and write operations.

2. Performance problems:

    - Check if your Atlas cluster has sufficient resources to handle the vector operations.
    - Optimize your index configuration in MongoDB Atlas for better search performance.

3. Upsert failures:
    - Confirm that the document format matches the expected schema for your collection.
    - Verify that the embeddings model is compatible with the vector dimensions in your Atlas index.

If you encounter persistent issues, consult the MongoDB Atlas documentation or reach out to the AnswerAgentAI support team for assistance.
</file>

<file path="sidekick-studio/chatflows/vector-stores/opensearch.md">
---
description: OpenSearch Vector Store for efficient similarity search and data retrieval
---

# OpenSearch Vector Store

## Overview

The OpenSearch Vector Store node in AnswerAgentAI allows you to store and retrieve high-dimensional vectors efficiently. It's designed for performing similarity searches on embedded data, making it ideal for various natural language processing and machine learning tasks.

## Key Benefits

-   Efficient storage and retrieval of high-dimensional vectors
-   Fast similarity searches for improved query performance
-   Seamless integration with other AnswerAgentAI components

## How to Use

1. Add the OpenSearch Vector Store node to your AnswerAgentAI workflow canvas.
2. Connect the necessary input nodes:

    - Document (optional): Connect a Document node if you want to upsert data.
    - Embeddings: Connect an Embeddings node to provide vector representations.
    - Index Name: Specify the name of the index where your vectors will be stored.

3. Configure the node parameters:

    - Top K: Set the number of top results to fetch (default is 4).

4. Connect your OpenSearch credentials:

    - Click on the node and select "Connect Credential" in the right sidebar.
    - Choose or create an OpenSearch credential with the correct URL.

5. Choose the desired output:
    - OpenSearch Retriever: For retrieving similar documents.
    - OpenSearch Vector Store: For accessing the vector store directly.

<!-- TODO: Add a screenshot of the OpenSearch Vector Store node with its inputs and outputs connected -->

<figure><img src="/.gitbook/assets/screenshots/opensearchvectorstore.png" alt="" /><figcaption><p> OpenSearch Vector Store node configuration  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure your OpenSearch instance is properly set up and accessible before using this node.
2. Use an appropriate Embeddings model that aligns with your data and use case.
3. Experiment with different "Top K" values to find the optimal number of results for your specific application.
4. When upserting documents, make sure they are properly formatted and contain the necessary information.

## Troubleshooting

1. Connection issues:

    - Verify that your OpenSearch URL is correct and the instance is running.
    - Check your network settings and firewall configurations.

2. Indexing problems:

    - Ensure that your documents are properly formatted and contain valid content.
    - Verify that the index name is valid and doesn't contain any special characters.

3. Retrieval issues:
    - Check if the embeddings model used for indexing matches the one used for querying.
    - Verify that the index contains data by querying it directly in OpenSearch.

If you encounter persistent issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/vector-stores/pinecone.md">
---
description: Pinecone Vector Store for AnswerAgentAI
---

# Pinecone Vector Store

## Overview

The Pinecone Vector Store node in AnswerAgentAI allows you to upsert embedded data and perform similarity or MMR (Maximal Marginal Relevance) searches using Pinecone, a leading fully managed hosted vector database. This node enables efficient storage and retrieval of high-dimensional vectors, making it ideal for various AI and machine learning applications.

## Key Benefits

-   Efficient storage and retrieval of vector embeddings
-   Seamless integration with Pinecone's managed vector database
-   Support for similarity and MMR search algorithms

## How to Use

1. Add the Pinecone Vector Store node to your AnswerAgentAI workflow canvas.
2. Connect the required input nodes (Document, Embeddings, etc.) to the Pinecone node.
3. Configure the node parameters:
    - Connect your Pinecone API credential
    - Specify the Pinecone Index name
    - (Optional) Set additional parameters like namespace and metadata filter

<!-- TODO: Screenshot of the Pinecone Vector Store node with its input connections and configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/pinecone.png" alt="" /><figcaption><p> Pinecone Vector Store node configuration  &#x26; Drop UI</p></figcaption></figure>

4. Connect the output (Pinecone Retriever or Pinecone Vector Store) to subsequent nodes in your workflow.

## Node Parameters

-   **Document**: (Optional) Input documents to be stored in the vector database.
-   **Embeddings**: The embedding model used to convert documents into vector representations.
-   **Record Manager**: (Optional) Keeps track of records to prevent duplication.
-   **Pinecone Index**: The name of your Pinecone index.
-   **Pinecone Namespace**: (Optional) A namespace within your Pinecone index.
-   **Pinecone Metadata Filter**: (Optional) JSON object to filter vectors based on metadata.
-   **Top K**: (Optional) Number of top results to fetch (default: 4).

## Tips and Best Practices

1. Ensure your Pinecone API credential is properly set up in AnswerAgentAI before using this node.
2. Use meaningful names for your Pinecone index and namespace to organize your vector data effectively.
3. Experiment with different embedding models to find the best performance for your specific use case.
4. Utilize the metadata filter to narrow down search results when dealing with large datasets.

## Troubleshooting

1. **Connection Issues**: If you're unable to connect to Pinecone, verify that your API key is correct and that you have the necessary permissions to access the specified index.

2. **Indexing Errors**: If documents fail to index, check that your embedding model is compatible with the document format and that the documents contain valid content.

3. **Search Performance**: If search results are not as expected, try adjusting the 'Top K' parameter or refining your metadata filter to improve relevance.

<!-- TODO: Screenshot showing common error messages and their resolutions -->

Remember to refer to the Pinecone documentation for more detailed information on index management and best practices for vector search optimization.
</file>

<file path="sidekick-studio/chatflows/vector-stores/postgres.md">
---
description: Upsert embedded data and perform similarity search using pgvector on Postgres
---

# Postgres Vector Store

## Overview

The Postgres Vector Store node in AnswerAgentAI allows you to store and retrieve embedded data using pgvector on a PostgreSQL database. This feature enables efficient similarity searches on high-dimensional vectors, making it ideal for various AI and machine learning applications.

## Key Benefits

-   Efficient storage and retrieval of vector embeddings
-   Seamless integration with PostgreSQL databases
-   Supports similarity search for advanced querying capabilities

## How to Use

1. Add the Postgres Vector Store node to your AnswerAgentAI workflow canvas.
2. Configure the node with the following parameters:

    - Connect Credential: Select or create a PostgresAPI credential
    - Document: (Optional) Input the documents to be stored
    - Embeddings: Select the embeddings model to use
    - Record Manager: (Optional) Select a record manager to prevent duplication
    - Host: Enter the PostgreSQL server host
    - Database: Specify the database name
    - Port: (Optional) Enter the PostgreSQL server port (default: 6432)
    - Table Name: (Optional) Specify the table name for storing vectors (default: "documents")
    - Additional Configuration: (Optional) Add any extra configuration in JSON format
    - Top K: (Optional) Set the number of top results to fetch (default: 4)

3. Connect the node to other components in your workflow as needed.

<!-- TODO: Add a screenshot of the Postgres Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/postgress.png" alt="" /><figcaption><p> Postgres Vector Store node configuration  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure that your PostgreSQL database has the pgvector extension installed and enabled.
2. Use appropriate indexing strategies for optimal performance, especially for large datasets.
3. Consider using a Record Manager to prevent duplicate entries when upserting documents.
4. Adjust the "Top K" value based on your specific use case and desired number of results.

## Troubleshooting

1. Connection issues:

    - Verify that the host, port, database name, and credentials are correct.
    - Ensure that the PostgreSQL server is accessible from your AnswerAgentAI instance.

2. Performance problems:

    - Check if appropriate indexes are in place for the vector columns.
    - Consider increasing the "Top K" value if you're not getting enough results.

3. Errors related to pgvector:
    - Confirm that the pgvector extension is properly installed and enabled in your PostgreSQL database.

<!-- TODO: Add a screenshot showing common error messages and their solutions -->

If you encounter persistent issues, consult the AnswerAgentAI documentation or reach out to support for further assistance.
</file>

<file path="sidekick-studio/chatflows/vector-stores/qdrant.md">
---
description: Qdrant Vector Store for AnswerAgentAI
---

# Qdrant Vector Store

## Overview

The Qdrant Vector Store node in AnswerAgentAI allows you to upsert embedded data and perform similarity searches using Qdrant, a scalable open-source vector database written in Rust. This node enables efficient storage and retrieval of high-dimensional vectors, making it ideal for various AI and machine learning applications.

## Key Benefits

-   Scalable and efficient vector storage and retrieval
-   Support for both local and cloud-hosted Qdrant instances
-   Flexible configuration options for advanced use cases

## How to Use

1. Add the Qdrant Vector Store node to your AnswerAgentAI canvas.
2. Connect the required inputs:
    - Document: Connect to a Document Loader node (optional)
    - Embeddings: Connect to an Embeddings node
3. Configure the node settings:
    - Qdrant Server URL: Enter the URL of your Qdrant server
    - Qdrant Collection Name: Specify the name of the collection to use
    - Vector Dimension: Set the dimension of your vectors (default: 1536)
4. (Optional) Configure additional parameters:
    - Upsert Batch Size: Set the batch size for upserting documents
    - Similarity: Choose the similarity measure (Cosine, Euclid, or Dot)
    - Additional Collection Configuration: Provide JSON configuration for advanced settings
    - Top K: Specify the number of top results to fetch
    - Qdrant Search Filter: Add a JSON filter to refine search results

<!-- TODO: Add a screenshot showing the Qdrant Vector Store node with its inputs and configuration options -->
<figure><img src="/.gitbook/assets/screenshots/qdrant.png" alt="" /><figcaption><p> Qdrant Vector Store node configuration  &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use a Record Manager to prevent duplication when upserting documents.
2. Adjust the Upsert Batch Size for optimal performance when dealing with large datasets.
3. Experiment with different similarity measures to find the best fit for your use case.
4. Utilize the Qdrant Search Filter to narrow down search results based on metadata or other criteria.

## Troubleshooting

1. Connection issues:

    - Ensure that the Qdrant Server URL is correct and accessible.
    - Check if the API key (for cloud-hosted instances) is valid and properly configured.

2. Performance problems:

    - Try adjusting the Upsert Batch Size to find the optimal balance between speed and resource usage.
    - Verify that the Vector Dimension matches the output of your Embeddings node.

3. Unexpected search results:
    - Double-check the Qdrant Search Filter syntax for any errors.
    - Ensure that the Top K value is appropriate for your use case.

## Advanced Usage: Filtering

Qdrant supports advanced filtering capabilities to refine your search results. Here's how to use filters effectively:

1. Each document in your vector store can have metadata associated with it, such as a `source` field.

2. To filter results based on metadata, use the "Qdrant Search Filter" option in the node configuration.

3. The filter should be provided as a JSON object. For example, to filter documents from a specific source:

```json
{
    "should": [
        {
            "key": "metadata.source",
            "match": {
                "value": "apple"
            }
        }
    ]
}
```

<!-- TODO: Add a screenshot showing the Qdrant Search Filter configuration in the node settings -->
<figure><img src="/.gitbook/assets/screenshots/qdrantapicredentials.png" alt="" /><figcaption><p> Qdrant Search Filter configuration  &#x26; Drop UI</p></figcaption></figure>

This filter will return only documents where the `metadata.source` field matches "apple".

4. You can create more complex filters using Qdrant's filter syntax, which supports nested conditions, range queries, and more. Refer to the Qdrant documentation for advanced filtering options.

By leveraging filters, you can create more targeted and relevant search results, improving the overall performance of your AnswerAgentAI workflows.

Remember to test your filters thoroughly to ensure they're working as expected in your specific use case.
</file>

<file path="sidekick-studio/chatflows/vector-stores/README.md">
---
description: Vector Store Nodes in AnswerAgentAI
---

# Vector Store Nodes

## Overview

Vector stores are essential components in modern AI and machine learning systems, particularly in applications involving natural language processing, image recognition, and recommendation systems. They are specialized database systems designed to efficiently store, manage, and retrieve high-dimensional numerical vectors.

## Why Vector Stores are Needed

1. **Efficient Similarity Search**: Vector stores enable fast and accurate similarity searches, which are crucial for tasks like finding similar documents, images, or user preferences.

2. **Scalability**: They can handle large volumes of high-dimensional data, making them suitable for big data applications.

3. **Optimized for AI/ML**: Vector stores are tailored for machine learning models that work with embeddings, facilitating seamless integration with AI systems.

4. **Real-time Processing**: Many vector stores support real-time updates and queries, essential for dynamic applications.

## What are Vector Stores?

Vector stores are databases that specialize in:

1. **Storing Vectors**: They efficiently store high-dimensional numerical representations of data (vectors).

2. **Indexing**: They use advanced indexing techniques to organize vectors for quick retrieval.

3. **Similarity Search**: They provide fast and accurate methods to find the most similar vectors to a given query vector.

4. **Integration**: They often offer APIs and integrations with popular machine learning frameworks and tools.

## Vector Store Nodes in AnswerAgentAI

AnswerAgentAI provides a variety of vector store nodes that can be added to the canvas of Sidekick workflows in the Sidekick Studio. Each of these nodes represents a different vector store solution, offering unique features and capabilities.

### Available Vector Store Nodes

1. [AstraDB](astradb.md)

    - A cloud-native database built on Apache Cassandra, optimized for vector search.

2. [Chroma](chroma.md)

    - An open-source embedding database designed for AI applications.

3. [Elastic](elastic.md)

    - A distributed search and analytics engine with vector search capabilities.

4. [Faiss](faiss.md)

    - A library for efficient similarity search and clustering of dense vectors.

5. [In-Memory Vector Store](in-memory-vector-store.md)

    - A lightweight vector store that operates entirely in memory for fast performance.

6. [Milvus](milvus.md)

    - An open-source vector database built for scalable similarity search.

7. [MongoDB Atlas](mongodb-atlas.md)

    - A multi-cloud database service with vector search capabilities.

8. [OpenSearch](opensearch.md)

    - A community-driven, open-source search and analytics suite with vector support.

9. [Pinecone](pinecone.md)

    - A fully managed vector database designed for machine learning applications.

10. [Postgres](postgres.md)

    - An open-source relational database with vector storage and search extensions.

11. [Qdrant](qdrant.md)

    - A vector similarity search engine with extended filtering support.

12. [Redis](redis.md)

    - An in-memory data structure store with vector similarity search capabilities.

13. [SingleStore](singlestore.md)

    - A distributed SQL database that supports vector operations.

14. [Supabase](supabase.md)

    - An open-source Firebase alternative with vector storage and search features.

15. [Upstash Vector](upstash-vector.md)

    - A fully managed vector database optimized for serverless and edge computing.

16. [Vectara](vectara.md)

    - An AI-powered search platform with vector search capabilities.

17. [Weaviate](weaviate.md)

    - An open-source vector database that allows for semantic search.

18. [Zep Collection - Open Source](zep-collection-open-source.md)

    - An open-source memory store for LLM applications with vector search features.

19. [Zep Collection - Cloud](zep-collection-cloud.md)
    - A cloud-hosted version of the Zep memory store for LLM applications.

Each of these vector store nodes offers unique features and integrations within AnswerAgentAI, allowing users to choose the most suitable solution for their specific use case and requirements.

<!-- TODO: Add a screenshot of the Vector Store Nodes section in the Sidekick Studio canvas -->
<figure><img src="/.gitbook/assets/screenshots/vectorstorenodes.png" alt="" /><figcaption><p> Vector Store Nodes   &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/vector-stores/redis.md">
---
description: Redis Vector Store for efficient similarity search and data storage
---

# Redis Vector Store

## Overview

The Redis Vector Store node in AnswerAgentAI allows you to store and retrieve high-dimensional vectors efficiently using Redis, an open-source, in-memory data structure store. This node enables you to perform similarity searches on embedded data, making it ideal for various AI and machine learning applications.

## Key Benefits

-   Fast similarity searches on large datasets
-   Efficient storage and retrieval of high-dimensional vectors
-   Seamless integration with Redis, a popular and robust data store

## How to Use

1. Add the Redis Vector Store node to your AnswerAgentAI workflow canvas.
2. Connect an Embeddings node to the "Embeddings" input of the Redis node.
3. (Optional) Connect a Document node to the "Document" input if you want to upsert data.
4. Configure the node parameters:
    - Index Name: Specify a unique name for your vector index.
    - Replace Index on Upsert: Choose whether to replace the existing index when upserting data.
    - Content Field: Name of the field containing the actual content (default: "content").
    - Metadata Field: Name of the field containing document metadata (default: "metadata").
    - Vector Field: Name of the field containing the vector (default: "content_vector").
    - Top K: Number of top results to fetch (default: 4).
5. Connect the Redis Vector Store node's output to other nodes in your workflow that require vector storage or retrieval capabilities.

<!-- TODO: Add a screenshot of the Redis Vector Store node configuration -->
<figure><img src="/.gitbook/assets/screenshots/redisvectorstore.png" alt="" /><figcaption><p> Redis Vector Store node   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use meaningful index names to easily identify different vector stores in your Redis instance.
2. Consider the trade-offs between replacing the index on upsert and appending to an existing index based on your use case.
3. Adjust the "Top K" value based on your specific requirements for similarity search results.
4. Ensure your Redis instance is properly configured and optimized for vector operations.

## Troubleshooting

1. Connection issues:

    - Verify that the Redis connection credentials are correct.
    - Ensure that the Redis instance is running and accessible from your AnswerAgentAI environment.

2. Performance problems:

    - Check if your Redis instance has enough memory allocated for vector operations.
    - Consider optimizing your index structure or Redis configuration for better performance.

3. Unexpected search results:
    - Verify that the embeddings used for storage and querying are consistent.
    - Double-check the field names (Content Field, Metadata Field, Vector Field) to ensure they match your data structure.

If you encounter any other issues or need further assistance, please consult the AnswerAgentAI documentation or reach out to our support team.
</file>

<file path="sidekick-studio/chatflows/vector-stores/singlestore.md">
---
description: Upsert embedded data and perform similarity search using SingleStore
---

# SingleStore Vector Store

## Overview

The SingleStore Vector Store node in AnswerAgentAI allows you to store and retrieve embedded data using SingleStore, a fast and distributed cloud relational database. This node enables efficient similarity searches on your vector data.

## Key Benefits

-   Fast and efficient similarity searches on vector data
-   Seamless integration with SingleStore's distributed cloud database
-   Flexible configuration options for customizing your vector store

## How to Use

1. Add the SingleStore Vector Store node to your AnswerAgentAI workflow canvas.
2. Configure the node with the following inputs:

    - **Document**: (Optional) The document or list of documents to be stored in the vector store.
    - **Embeddings**: The embedding model to use for converting text into vector representations.
    - **Host**: The hostname of your SingleStore database.
    - **Database**: The name of the database to use.
    - **Table Name**: (Optional) The name of the table to store the vectors (default: "embeddings").
    - **Content Column Name**: (Optional) The name of the column to store the document content (default: "content").
    - **Vector Column Name**: (Optional) The name of the column to store the vector data (default: "vector").
    - **Metadata Column Name**: (Optional) The name of the column to store metadata (default: "metadata").
    - **Top K**: (Optional) The number of top results to retrieve in similarity searches (default: 4).

3. Connect the SingleStore Vector Store node to other nodes in your workflow as needed.

<!-- TODO: Add a screenshot showing the SingleStore Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/singlestore.png" alt="" /><figcaption><p> Singlestore Vector Store node   &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Ensure that you have the necessary credentials to connect to your SingleStore database. You can set up a credential in AnswerAgentAI for secure access.
2. When using the SingleStore Vector Store for the first time, make sure to upsert documents to populate the database before performing similarity searches.
3. Experiment with different "Top K" values to find the optimal number of results for your specific use case.
4. Use meaningful names for your table and column names to make your database structure more organized and easier to understand.

## Troubleshooting

1. **Connection issues**: If you're having trouble connecting to your SingleStore database, double-check your host, database name, and credentials. Ensure that your firewall settings allow connections to the database.

2. **Slow performance**: If queries are running slowly, consider optimizing your SingleStore database configuration or increasing the resources allocated to your database instance.

3. **Unexpected results**: If you're not getting the expected results from similarity searches, verify that your documents were properly upserted and that the embedding model used for querying matches the one used for document insertion.

<!-- TODO: Add a screenshot showing a successful SingleStore Vector Store connection and query result -->
</file>

<file path="sidekick-studio/chatflows/vector-stores/supabase.md">
---
description: Upsert embedded data and perform similarity or mmr search using Supabase via pgvector extension
---

# Supabase Vector Store

## Overview

The Supabase Vector Store node in AnswerAgentAI allows you to store and retrieve embedded data using Supabase's pgvector extension. This feature enables efficient similarity searches and MMR (Maximal Marginal Relevance) queries on your vector data.

## Key Benefits

-   Efficient storage and retrieval of high-dimensional vectors
-   Seamless integration with Supabase for vector operations
-   Support for similarity search and MMR queries

## How to Use

1. Set up a Supabase project and enable the pgvector extension (see Prerequisites section).
2. Add the Supabase Vector Store node to your AnswerAgentAI workflow.
3. Connect the required inputs:
    - Document: Connect to a Document Loader node (optional)
    - Embeddings: Connect to an Embeddings node
    - Record Manager: Connect to a Record Manager node (optional)
4. Configure the node parameters:
    - Supabase Project URL: Enter your Supabase project URL
    - Table Name: Specify the name of the table to store vectors (e.g., "documents")
    - Query Name: Specify the name of the query function (e.g., "match_documents")
5. (Optional) Configure additional parameters:
    - Supabase Metadata Filter: JSON object for metadata filtering
    - Supabase RPC Filter: Query builder-style filtering
    - Top K: Number of top results to fetch (default: 4)

<!-- TODO: Add a screenshot of the Supabase Vector Store node configuration -->
<figure><img src="/.gitbook/assets/screenshots/supabase.png" alt="" /><figcaption><p> Supabase Vector Store node   &#x26; Drop UI</p></figcaption></figure>

## Prerequisites

Before using the Supabase Vector Store node, you need to set up a Supabase project and enable the pgvector extension:

1. Create a Supabase account and start a new project.
2. In your Supabase project, open the SQL Editor.
3. Create a new query and run the following SQL to set up the necessary table and function:

```sql
-- Enable the pgvector extension
create extension vector;

-- Create a table to store your documents
create table documents (
  id bigserial primary key,
  content text,
  metadata jsonb,
  embedding vector(1536)
);

-- Create a function to search for documents
create function match_documents (
  query_embedding vector(1536),
  match_count int DEFAULT null,
  filter jsonb DEFAULT '{}'
) returns table (
  id bigint,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    content,
    metadata,
    1 - (documents.embedding <=> query_embedding) as similarity
  from documents
  where metadata @> filter
  order by documents.embedding <=> query_embedding
  limit match_count;
end;
$$;
```

<!-- TODO: Add a screenshot of the SQL query execution in Supabase -->

## Tips and Best Practices

1. Use meaningful names for your Supabase table and query function to easily identify them in your project.
2. When using a Record Manager, modify the `id` column in the SQL setup to use `text` instead of `bigserial` to accommodate UUID generation.
3. Experiment with metadata filtering to refine your search results based on specific criteria.
4. Adjust the Top K value to control the number of results returned by the similarity search.

## Troubleshooting

1. **Connection issues**: Ensure that your Supabase Project URL and API Key are correct in the node configuration.
2. **Embedding dimension mismatch**: If you encounter errors related to embedding dimensions, make sure the vector size in your SQL setup matches the output of your chosen embedding model (e.g., 1536 for OpenAI embeddings).
3. **Metadata filtering not working**: Double-check the JSON format of your metadata filter and ensure that the metadata keys exist in your stored documents.

If you continue to experience issues, consult the AnswerAgentAI documentation or reach out to support for further assistance.

## Resources

-   [LangChain JS Supabase](https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/supabase)
-   [Supabase Blog Post](https://supabase.com/blog/openai-embeddings-postgres-vector)
-   [Metadata Filtering](https://js.langchain.com/docs/integrations/vectorstores/supabase#metadata-filtering)
</file>

<file path="sidekick-studio/chatflows/vector-stores/upstash-vector.md">
---
description: Store and retrieve vector embeddings using Upstash Vector
---

# Upstash Vector Store

## Overview

The Upstash Vector Store node allows you to store and retrieve vector embeddings using Upstash, a serverless data platform. This feature enables efficient similarity searches and management of high-dimensional numerical vectors.

## Key Benefits

-   Serverless vector database for easy scalability
-   Fast similarity searches for improved query performance
-   Seamless integration with AnswerAgentAI workflows

## How to Use

### Prerequisites

1.  Sign up or sign in to the [Upstash Console](https://console.upstash.com)
2.  Navigate to the Vector page and click **Create Index**
    <!-- TODO: Screenshot of Upstash Console Vector page -->
    <figure><img src="/.gitbook/assets/screenshots/upstashpage.png" alt="" /><figcaption><p> Upstash Console Vector page   &#x26; Drop UI</p></figcaption></figure>

3.  Configure and create the index:
    -   **Index Name**: Choose a name for your index (e.g., "answerai-upstash-demo")
    -   **Dimensions**: Set the size of the vectors to be inserted (e.g., 1536)
    -   **Embedding Model**: (Optional) Select a model from [Upstash Embeddings](https://upstash.com/docs/vector/features/embeddingmodels)
        <!-- TODO: Screenshot of Create Index form -->
            <figure><img src="/.gitbook/assets/screenshots/upstashindex.png" alt="" /><figcaption><p> Upstash Console Index form   &#x26; Drop UI</p></figcaption></figure>

### Setup in AnswerAgentAI

1. Obtain your index credentials from the Upstash Console
       <!-- TODO: Screenshot of Upstash credentials page -->
    <figure><img src="/.gitbook/assets/screenshots/upstashvectorapi.png" alt="" /><figcaption><p> Upstash Console Index form   &#x26; Drop UI</p></figcaption></figure>

2. In AnswerAgentAI, create a new Upstash Vector credential:
    - Enter the Upstash Vector REST URL (UPSTASH_VECTOR_REST_URL)
    - Enter the Upstash Vector REST Token (UPSTASH_VECTOR_REST_TOKEN)
          <!-- TODO: Screenshot of AnswerAgentAI credential creation form -->

<figure><img src="/.gitbook/assets/screenshots/upstashapipage.png" alt="" /><figcaption><p> Upstash Console Index form   &#x26; Drop UI</p></figcaption></figure>

3.  Add a new **Upstash Vector** node to your canvas
       <!-- TODO: Screenshot of Upstash Vector node in AnswerAgentAI canvas -->
    <figure><img src="/.gitbook/assets/screenshots/upstashvectorstore.png" alt="" /><figcaption><p> Upstash Vector Store node   &#x26; Drop UI</p></figcaption></figure>

4.  Connect additional nodes to the Upstash Vector node:

    -   Connect a **Document Loader** node to provide input documents
    -   Connect an **Embeddings** node to generate vector embeddings
        <!-- TODO: Screenshot of connected nodes in AnswerAgentAI canvas -->
            <figure><img src="/.gitbook/assets/screenshots/upstashvectorinaworkflow.png" alt="" /><figcaption><p> Upstash Vector Store node in a workflow &#x26; Drop UI</p></figcaption></figure>

5.  Configure the Upstash Vector node:

    -   Select the Upstash Vector credential you created earlier
    -   (Optional) Set a metadata filter to refine your searches
    -   (Optional) Adjust the Top K value to specify the number of results to retrieve

6.  Run your workflow to start the upsert process and store your vectors

7.  Verify data storage in the [Upstash dashboard](https://console.upstash.com)
       <!-- TODO: Screenshot of Upstash dashboard showing stored data -->
    <figure><img src="/.gitbook/assets/screenshots/upstashpage.png" alt="" /><figcaption><p> Upstash Dashboard &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Choose an appropriate dimension size for your vectors based on your embedding model and use case
2. Use metadata filters to organize and retrieve specific subsets of your data
3. Regularly monitor your Upstash usage to optimize performance and costs
4. Consider using the optional Embedding Model feature in Upstash for simplified vector generation

## Troubleshooting

1. **Connection issues**: Ensure your Upstash Vector REST URL and Token are correctly entered in the AnswerAgentAI credential
2. **Indexing errors**: Verify that your document format and embedding dimensions match the Upstash index configuration
3. **Slow performance**: Check your network connection and consider optimizing your vector dimensions or index size

If problems persist, consult the Upstash documentation or contact AnswerAgentAI support for assistance.
</file>

<file path="sidekick-studio/chatflows/vector-stores/vectara.md">
---
description: Upsert embedded data and perform similarity search using Vectara, a LLM-powered search-as-a-service
---

# Vectara Vector Store

## Overview

The Vectara Vector Store node allows you to store and retrieve embedded data using Vectara, a powerful search-as-a-service platform. This node enables you to perform similarity searches on your stored data, making it ideal for applications that require efficient information retrieval.

## Key Benefits

-   Efficient similarity search: Quickly find relevant information based on semantic similarity.
-   LLM-powered search: Leverage advanced language models for improved search accuracy.
-   Flexible integration: Easily incorporate Vectara's capabilities into your AnswerAgentAI workflows.

## How to Use

1. Add the Vectara Vector Store node to your canvas.
2. Connect your Vectara credentials:
    - Click on the "Connect Credential" dropdown.
    - Select an existing credential or click "Create New" to add your Vectara API credentials.
3. Configure the node inputs:
    - Document: Connect a Document node to provide text data for storage (optional).
    - File: Upload a file to be stored in Vectara (optional).
    - Additional Parameters: Adjust search behavior and result filtering (optional).
4. Connect the node outputs to other nodes in your workflow:
    - Vectara Retriever: Use this output for direct retrieval operations.
    - Vectara Vector Store: Use this output for more advanced vector store operations.

## Tips and Best Practices

1. Use the Metadata Filter for targeted searches within your stored data.
2. Experiment with the "Sentences Before" and "Sentences After" parameters to control the context returned with search results.
3. Adjust the "Lambda" parameter to balance between neural search and keyword-based search for optimal results.
4. Use the "Top K" parameter to control the number of results returned by the search.
5. Explore the MMR (Maximal Marginal Relevance) settings to get diverse search results.

## Troubleshooting

1. If you encounter authentication errors, double-check your Vectara API credentials in the connected credential.
2. Ensure that your Vectara corpus is properly set up and accessible with the provided API key.
3. If search results are not as expected, try adjusting the additional parameters such as Lambda or Metadata Filter.

<!-- TODO: Add a screenshot of the Vectara Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/vectara.png" alt="" /><figcaption><p> Vectara Vector Store node configuration panel &#x26; Drop UI</p></figcaption></figure>
</file>

<file path="sidekick-studio/chatflows/vector-stores/weaviate.md">
---
description: Weaviate Vector Store for AnswerAgentAI
---

# Weaviate Vector Store

## Overview

The Weaviate Vector Store node in AnswerAgentAI allows you to store, retrieve, and search vector embeddings using Weaviate, a scalable open-source vector database. This node enables you to perform similarity searches and MMR (Maximal Marginal Relevance) searches on your embedded data.

## Key Benefits

-   Efficiently store and search vector embeddings
-   Perform similarity and MMR searches on your data
-   Scalable solution for managing large datasets

## How to Use

1. Add the Weaviate Vector Store node to your AnswerAgentAI canvas.
2. Connect the necessary input nodes (Document, Embeddings, and optionally RecordManager).
3. Configure the Weaviate connection settings and other parameters.
4. Run your workflow to store or retrieve data from the Weaviate Vector Store.

### Input Parameters

-   **Document**: (Optional) The documents to be stored in the vector database.
-   **Embeddings**: The embedding model to use for converting text to vectors.
-   **Record Manager**: (Optional) Keeps track of records to prevent duplication.
-   **Weaviate Scheme**: Choose between "https" or "http" for the connection.
-   **Weaviate Host**: The host address of your Weaviate instance (e.g., "localhost:8080").
-   **Weaviate Index**: The name of the index to use in Weaviate.
-   **Weaviate Text Key**: (Optional) The key used for storing text data.
-   **Weaviate Metadata Keys**: (Optional) Keys for additional metadata to be stored.
-   **Top K**: (Optional) Number of top results to fetch (default is 4).
-   **Weaviate Search Filter**: (Optional) JSON filter for search queries.

### Output

The node provides two outputs:

1. **Weaviate Retriever**: A retriever object for performing searches.
2. **Weaviate Vector Store**: The vector store object for direct interactions.

## Tips and Best Practices

1. Ensure your Weaviate instance is properly set up and accessible before using this node.
2. Use a Record Manager to prevent duplicate entries when upserting documents.
3. Experiment with different "Top K" values to find the optimal number of results for your use case.
4. Utilize the Weaviate Search Filter for more precise queries when needed.

## Troubleshooting

1. **Connection Issues**: Verify that the Weaviate host and scheme are correct. Ensure that your Weaviate instance is running and accessible.

2. **Authentication Errors**: If using Weaviate cloud hosted, make sure you've provided the correct API key in the credential settings.

3. **Indexing Failures**: Check that your documents are properly formatted and that the specified Weaviate index exists.

4. **Search Returns No Results**: Verify that you have data in your index and that your search filter (if used) is not too restrictive.

<!-- TODO: Add a screenshot of the Weaviate Vector Store node configuration in the AnswerAgentAI canvas -->
<figure><img src="/.gitbook/assets/screenshots/weaviate.png" alt="" /><figcaption><p> Weaviate Vector Store node configuration panel &#x26; Drop UI</p></figcaption></figure>

Remember to refer to the Weaviate documentation for more advanced configurations and features specific to the Weaviate vector database.
</file>

<file path="sidekick-studio/chatflows/vector-stores/zep-collection-cloud.md">
---
description: Zep Collection - Cloud Vector Store for AnswerAgentAI
---

# Zep Collection - Cloud Vector Store

## Overview

The Zep Collection - Cloud Vector Store is a powerful component in AnswerAgentAI that allows you to store, retrieve, and search vector embeddings efficiently. It uses Zep, a fast and scalable building block for LLM applications, to manage your vector data in the cloud.

## Key Benefits

-   Efficient similarity search: Quickly find the most relevant documents based on vector similarity.
-   Cloud-based storage: Store and access your vector data securely in the cloud.
-   Flexible metadata filtering: Easily filter your search results using custom metadata.

## How to Use

1. Add the "Zep Collection - Cloud" node to your AnswerAgentAI canvas.
2. Configure the node with the following settings:

    a. Connect Credential: Select or create a Zep Memory API credential.

    b. Document: (Optional) Connect a Document node to add documents to the vector store.

    c. Zep Collection: Enter a name for your Zep collection (e.g., "my-first-collection").

    d. Zep Metadata Filter: (Optional) Add a JSON object to filter search results based on metadata.

    e. Top K: (Optional) Specify the number of top results to fetch (default is 4).

3. Connect the Zep Collection node to other nodes in your workflow that require vector storage or retrieval.

<!-- TODO: Add a screenshot of the Zep Collection - Cloud node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/zepcloud.png" alt="" /><figcaption><p> Zep Collectionconfiguration panel &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

1. Use meaningful collection names: Choose descriptive names for your Zep collections to easily identify their purpose.

2. Optimize metadata: Design your metadata structure carefully to enable efficient filtering and improve search relevance.

3. Manage API keys securely: Always use the credential manager to store your Zep API keys securely.

4. Monitor usage: Keep track of your vector store usage to optimize performance and manage costs effectively.

## Troubleshooting

1. Connection issues:

    - Ensure that your Zep API credential is correctly configured.
    - Check your internet connection and firewall settings.

2. Slow search performance:

    - Consider optimizing your metadata filters.
    - Reduce the number of vectors in your collection if it becomes too large.

3. Unexpected search results:
    - Verify that your metadata filters are correctly formatted as JSON.
    - Double-check the "Top K" value to ensure you're retrieving the desired number of results.

If you encounter persistent issues, consult the AnswerAgentAI documentation or reach out to support for assistance.
</file>

<file path="sidekick-studio/chatflows/vector-stores/zep-collection-open-source.md">
---
description: Zep Collection - Open Source Vector Store
---

# Zep Vector Store

## Overview

The Zep Vector Store is a powerful component in AnswerAgentAI that allows you to store, retrieve, and search embedded data efficiently. It's designed for fast and scalable operations, making it an excellent building block for LLM applications.

## Key Benefits

-   Fast and scalable similarity search
-   Efficient storage and retrieval of embedded data
-   Seamless integration with AnswerAgentAI workflows

## How to Use

1. Add the Zep Vector Store node to your canvas.
2. Configure the following inputs:

    - Document: (Optional) Add document(s) to be stored in the vector store.
    - Embeddings: Select the embeddings model to use.
    - Base URL: Enter the base URL for your Zep instance (default: `http://127.0.0.1:8000`).
    - Zep Collection: Specify the name of your Zep collection.
    - Zep Metadata Filter: (Optional) Add a JSON metadata filter for advanced querying.
    - Embedding Dimension: Set the dimension of your embeddings (default: 1536).
    - Top K: Specify the number of top results to fetch (default: 4).

3. Connect the node to other components in your workflow.
4. Run your workflow to utilize the Zep Vector Store for storing or retrieving embedded data.

<!-- TODO: Add a screenshot of the Zep Vector Store node configuration panel -->
<figure><img src="/.gitbook/assets/screenshots/zepopensource.png" alt="" /><figcaption><p> Zep Collection figuration panel &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Ensure your Zep instance is running and accessible before using this node.
-   Use meaningful names for your Zep collections to organize your data effectively.
-   Experiment with different embedding dimensions and top K values to optimize performance for your specific use case.
-   Utilize metadata filters to refine your searches and improve result relevance.

## Troubleshooting

1. Connection issues:

    - Verify that your Zep instance is running and the Base URL is correct.
    - Check your network connection and firewall settings.

2. Authentication errors:

    - Ensure you've properly configured the JWT authentication in your Zep instance.
    - Verify that the API key in the credential settings is correct.

3. Unexpected search results:
    - Double-check your metadata filter syntax if you're using one.
    - Verify that your documents were properly ingested into the vector store.

If you encounter persistent issues, consult the AnswerAgentAI documentation or reach out to our support team for assistance.
</file>

<file path="sidekick-studio/chatflows/README.md">
---
description: Chatflows are powerful, configurable workflows that enable AI-powered interactions across multiple platforms.
---

# Chatflows

## Overview

Chatflows are the core building blocks of AnswerAgentAI that allow you to create intelligent, interactive AI experiences. They represent configurable workflows that connect various nodes - from AI models to tools and data sources - creating complete solutions that can be deployed across multiple platforms.

## Key Features

1. **Visual Construction**: Build complex AI workflows using a visual canvas without writing code
2. **Modular Design**: Combine and reconfigure nodes to create custom solutions
3. **Multi-Platform Deployment**: Deploy your chatflows across various channels:
    - Embedded chatbots on websites
    - In-app chat interfaces
    - Browser extensions
    - API endpoints
4. **Model Flexibility**: Easily swap between different AI models to find the optimal solution
5. **Tool Integration**: Connect to external tools and data sources to enhance capabilities
6. **Version Control**: Track changes and maintain multiple versions of your chatflows

## Use Cases

### Embedded Chatbots

Deploy chatflows as interactive chatbots directly on your website or application. These chatbots can:

-   Answer customer questions using your knowledge base
-   Guide users through processes
-   Collect information through conversational interfaces
-   Provide 24/7 support without human intervention

### In-App Chat Interface

Integrate chatflows within the AnswerAgentAI interface for internal teams to:

-   Test and iterate on AI workflows
-   Access powerful AI assistants
-   Query company knowledge bases
-   Automate repetitive tasks

### Browser Extension

Extend the functionality of your chatflows to web browsers, allowing users to:

-   Access your AI assistant from any webpage
-   Analyze content on current pages
-   Perform actions without leaving their browsing context

### API Endpoints

Expose chatflows as API endpoints to:

-   Integrate AI capabilities into existing applications
-   Build custom user interfaces
-   Enable programmatic access to AI workflows
-   Scale your AI infrastructure

## Building Effective Chatflows

The power of chatflows comes from their flexibility and modularity. By combining different node types, you can create sophisticated AI experiences:

-   **Chat Models**: Define the AI engine powering your conversations
-   **Document Loaders**: Import knowledge from various sources
-   **Vector Stores**: Enable semantic search and retrieval
-   **Tools**: Connect to external services and APIs
-   **Memory**: Maintain context across conversation turns
-   **Prompts**: Control how the AI responds in different scenarios

The ability to easily swap components means you can continuously improve your chatflows by incorporating the latest AI models and tools as they become available.
</file>

<file path="sidekick-studio/credentials/api-credentials.md">
---
sidebar_position: 1
---

# API Credentials

This page provides information about all available API credentials in Sidekick Studio. Each credential contains fields required to authenticate with external services.

## Table of Contents

-   [AI Service Credentials](#ai-service-credentials)
-   [Database Credentials](#database-credentials)
-   [Search Service Credentials](#search-service-credentials)
-   [Vector Store Credentials](#vector-store-credentials)
-   [Content Service Credentials](#content-service-credentials)
-   [Utility Credentials](#utility-credentials)

## AI Service Credentials

### OpenAI API

Authentication for OpenAI services.

-   **How to obtain**: Get your API key from the [OpenAI API keys page](https://platform.openai.com/api-keys) after creating an account.
-   **Inputs**:
    -   OpenAI Api Key (password)

### Azure OpenAI API

Authentication for Azure OpenAI services.

-   **How to obtain**: Refer to [official guide](https://azure.microsoft.com/en-us/products/cognitive-services/openai-service) on how to use Azure OpenAI service.
-   **Inputs**:
    -   Azure OpenAI Api Key (password)
    -   Azure OpenAI Api Instance Name
    -   Azure OpenAI Api Deployment Name
    -   Azure OpenAI Api Version (e.g., 2023-06-01-preview)

### Anthropic API

Authentication for Anthropic Claude models.

-   **How to obtain**: Sign up at [Anthropic's console](https://console.anthropic.com/) and create an API key from your profile's [API Keys section](https://console.anthropic.com/settings/api-keys).
-   **Inputs**:
    -   Anthropic Api Key (password)

### Google Vertex Auth

Authentication for Google Vertex AI services.

-   **How to obtain**: Create credentials through the [Google Cloud Console](https://console.cloud.google.com/apis/credentials) and set up a service account.
-   **Inputs**:
    -   Google Application Credential File Path (optional)
    -   Google Credential JSON Object (optional)
    -   Project ID (optional)

### Google Generative AI

Authentication for Google Generative AI services.

-   **How to obtain**: Get your API key from the [official page](https://ai.google.dev/tutorials/setup).
-   **Inputs**:
    -   Google AI API Key (password)

### Cohere API

Authentication for Cohere AI services.

-   **How to obtain**: Sign up at [Cohere's dashboard](https://dashboard.cohere.com/welcome/login), then navigate to the [API keys section](https://dashboard.cohere.com/api-keys) to create a key.
-   **Inputs**:
    -   Cohere Api Key (password)

### HuggingFace API

Authentication for HuggingFace services.

-   **How to obtain**: Create an account at [HuggingFace](https://huggingface.co/), then go to your profile settings and navigate to the [Access Tokens section](https://huggingface.co/settings/tokens) to create a token.
-   **Inputs**:
    -   HuggingFace Api Key (password)

### Mistral AI API

Authentication for Mistral AI services.

-   **How to obtain**: Get your API key from the [official console](https://console.mistral.ai/).
-   **Inputs**:
    -   MistralAI API Key (password)

### Groq API

Authentication for Groq API services.

-   **How to obtain**: Sign up at [Groq Console](https://console.groq.com/), then visit the [API Keys section](https://console.groq.com/keys) to create a key.
-   **Inputs**:
    -   Groq Api Key (password)

### Fireworks API

Authentication for Fireworks AI services.

-   **How to obtain**: Sign up at [Fireworks AI](https://fireworks.ai/), then get your API key from the account settings.
-   **Inputs**:
    -   Fireworks Api Key (password)

### Together AI API

Authentication for Together AI services.

-   **How to obtain**: Sign up at [Together AI](https://together.ai/), then access your API key from the account settings.
-   **Inputs**:
    -   TogetherAI Api Key (password)

### Alibaba API

Authentication for Alibaba AI services.

-   **How to obtain**: Sign up for [Alibaba Cloud](https://www.alibabacloud.com/), then create an API key through the console.
-   **Inputs**:
    -   Alibaba Api Key (password)

### Baidu Qianfan API

Authentication for Baidu's Qianfan AI platform.

-   **How to obtain**: Register at [Baidu AI Cloud](https://cloud.baidu.com/), then create and access your Qianfan keys.
-   **Inputs**:
    -   Qianfan Access Key
    -   Qianfan Secret Key (password)

### Cerebras API

Authentication for Cerebras AI platform.

-   **How to obtain**: Sign up at [Cerebras AI platform](https://cloud.cerebras.ai/) and obtain your API key from the dashboard.
-   **Inputs**:
    -   Cerebras API Key (password) - API Key from cloud.cerebras.ai

### Deepseek AI API

Authentication for DeepseekAI services.

-   **How to obtain**: Register at [DeepseekAI's platform](https://platform.deepseek.ai/) and get your API key from the account settings.
-   **Inputs**:
    -   DeepseekAI API Key (password)

### IBM Watsonx

Authentication for IBM Watsonx AI services.

-   **How to obtain**: Sign up for [IBM Cloud](https://cloud.ibm.com/), create a Watsonx service, and get your credentials from the service dashboard.
-   **Inputs**:
    -   Version (YYYY-MM-DD)
    -   Service URL
    -   Project ID
    -   Watsonx AI Auth Type (IAM or Bearer Token)
    -   Watsonx AI IAM API Key (optional)
    -   Watsonx AI Bearer Token (optional)

### Jina AI API

Authentication for Jina AI services.

-   **How to obtain**: Get your API key from the [official console](https://jina.ai/).
-   **Inputs**:
    -   JinaAI API Key (password)

### LocalAI API

Authentication for LocalAI services.

-   **How to obtain**: Follow the [LocalAI documentation](https://localai.io/basics/authentication/) to configure your local API key.
-   **Inputs**:
    -   LocalAI Api Key (password)

### OpenRouter API

Authentication for OpenRouter services.

-   **How to obtain**: Sign up at [OpenRouter](https://openrouter.ai/) and generate your API key from the dashboard.
-   **Inputs**:
    -   OpenRouter API Key (password)

### Replicate API

Authentication for Replicate services.

-   **How to obtain**: Create an account at [Replicate](https://replicate.com/), then go to your account settings to find or create an API token.
-   **Inputs**:
    -   Replicate Api Key (password)

### X AI API

Authentication for X AI services.

-   **How to obtain**: Apply for access to X AI (formerly known as Twitter's AI services) and receive an API key.
-   **Inputs**:
    -   X AI API Key (password)

### Nvidia NIM API

Authentication for Nvidia NIM platform.

-   **How to obtain**: Sign up for the [Nvidia NIM developer program](https://developer.nvidia.com/nim) and generate your API key.
-   **Inputs**:
    -   Nvidia NIM API Key (password)

## Database Credentials

### Astra DB API

Authentication for DataStax Astra DB.

-   **How to obtain**: Create an account at [DataStax Astra](https://astra.datastax.com/) and generate an application token from the dashboard.
-   **Inputs**:
    -   Astra DB Application Token (password)
    -   Astra DB Api Endpoint

### MongoDB ATLAS

Authentication for MongoDB Atlas.

-   **How to obtain**: Sign up at [MongoDB Atlas](https://www.mongodb.com/cloud/atlas), create a cluster, and get your connection string from the Connect dialog.
-   **Inputs**:
    -   ATLAS Connection URL

### MySQL API

Authentication for MySQL database.

-   **How to obtain**: Set up a MySQL server and create user credentials with appropriate permissions.
-   **Inputs**:
    -   User
    -   Password

### PostgreSQL API

Authentication for PostgreSQL database.

-   **How to obtain**: Install PostgreSQL, create a database, and set up user credentials with appropriate permissions.
-   **Inputs**:
    -   User
    -   Password

### PostgreSQL URL

Authentication for PostgreSQL using connection string.

-   **How to obtain**: Set up a PostgreSQL server and construct a connection URL.
-   **Inputs**:
    -   Postgres URL

### Neo4j API

Authentication for Neo4j graph database.

-   **How to obtain**: Refer to [official guide](https://neo4j.com/docs/operations-manual/current/authentication-authorization/) on Neo4j authentication.
-   **Inputs**:
    -   Neo4j URL (e.g., neo4j://localhost:7687)
    -   Username
    -   Password

### Redis API

Authentication for Redis cache.

-   **How to obtain**: Set up a Redis server and configure authentication following the [Redis documentation](https://redis.io/docs/management/security/).
-   **Inputs**:
    -   Redis Host (default: 127.0.0.1)
    -   Port (default: 6379)
    -   User
    -   Password
    -   Use SSL (boolean)

### Redis URL

Authentication for Redis cache using URL.

-   **How to obtain**: Set up a Redis server and construct a Redis URL.
-   **Inputs**:
    -   Redis URL (default: redis://localhost:6379)

### Couchbase API

Authentication for Couchbase database.

-   **How to obtain**: Install Couchbase Server, create a bucket, and set up authentication following the [Couchbase documentation](https://docs.couchbase.com/server/current/manage/manage-security/manage-users-and-roles.html).
-   **Inputs**:
    -   Couchbase Connection String
    -   Couchbase Username
    -   Couchbase Password

### SingleStore API

Authentication for SingleStore database.

-   **How to obtain**: Sign up for [SingleStore](https://www.singlestore.com/), create a workspace, and set up user credentials.
-   **Inputs**:
    -   User
    -   Password

## Search Service Credentials

### Brave Search API

Authentication for Brave Search.

-   **How to obtain**: Apply for API access at [Brave Search API](https://brave.com/search/api/) and get your API key.
-   **Inputs**:
    -   BraveSearch Api Key (password)

### Google Custom Search API

Authentication for Google Search.

-   **How to obtain**: Refer to the [Google Cloud Console](https://console.cloud.google.com/apis/credentials) for API key and [Search Engine Creation page](https://programmablesearchengine.google.com/controlpanel/create) for Search Engine ID.
-   **Inputs**:
    -   Google Custom Search Api Key (password)
    -   Programmable Search Engine ID

### Serp API

Authentication for SERP API.

-   **How to obtain**: Sign up at [SerpAPI](https://serpapi.com/) and get your API key from the dashboard.
-   **Inputs**:
    -   Serp Api Key (password)

### Serper API

Authentication for Serper API.

-   **How to obtain**: Register at [Serper.dev](https://serper.dev/) to obtain your API key.
-   **Inputs**:
    -   Serper Api Key (password)

### Search API

Authentication for Search API.

-   **How to obtain**: Sign in to [SearchApi](https://www.searchapi.io/) to obtain a free API key from the dashboard.
-   **Inputs**:
    -   SearchApi API Key (password)

### Exa Search API

Authentication for Exa Search.

-   **How to obtain**: Refer to [official guide](https://docs.exa.ai/reference/getting-started#getting-access) on how to get an API Key from Exa.
-   **Inputs**:
    -   ExaSearch Api Key (password)

### Tavily API

Authentication for Tavily Search API.

-   **How to obtain**: Sign up at [Tavily AI](https://tavily.com/) and retrieve your API key from the dashboard.
-   **Description**: Tavily API is a real-time API to access Google search results.
-   **Inputs**:
    -   Tavily Api Key (password)

## Vector Store Credentials

### Chroma API

Authentication for Chroma vector database.

-   **How to obtain**: Follow the [Chroma documentation](https://docs.trychroma.com/serverless) to set up your account and get API credentials.
-   **Inputs**:
    -   Chroma Api Key (password)
    -   Chroma Tenant
    -   Chroma Database

### Elasticsearch API

Authentication for Elasticsearch.

-   **How to obtain**: Refer to [official guide](https://www.elastic.co/guide/en/kibana/current/api-keys.html) on how to get an API Key from ElasticSearch.
-   **Inputs**:
    -   Elasticsearch Endpoint
    -   Elasticsearch API Key (password)

### ElasticSearch User Password

Authentication for Elasticsearch using username and password.

-   **How to obtain**: Refer to [official guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/setting-up-authentication.html) on how to get User Password from ElasticSearch.
-   **Inputs**:
    -   Cloud ID (Elastic Cloud ID or server URL)
    -   ElasticSearch User
    -   ElasticSearch Password

### Meilisearch API

Authentication for Meilisearch.

-   **How to obtain**: Refer to [official guide](https://meilisearch.com) on how to get an API Key.
-   **Inputs**:
    -   Meilisearch Search API Key (password)
    -   Meilisearch Admin API Key (password, optional)

### Milvus Auth

Authentication for Milvus vector database.

-   **How to obtain**: You can find the Milvus Authentication [here](https://milvus.io/docs/authenticate.md#Authenticate-User-Access).
-   **Inputs**:
    -   Milvus User
    -   Milvus Password

### OpenSearch

Authentication for OpenSearch.

-   **How to obtain**: Set up OpenSearch following the [official documentation](https://opensearch.org/docs/latest/security/authentication/index/) and create user credentials.
-   **Inputs**:
    -   OpenSearch Url
    -   User (optional)
    -   Password (optional)

### Pinecone API

Authentication for Pinecone vector database.

-   **How to obtain**: Sign up at [Pinecone](https://www.pinecone.io/), create an index, and get your API key from the dashboard.
-   **Inputs**:
    -   Pinecone Api Key (password)

### Qdrant API

Authentication for Qdrant vector database.

-   **How to obtain**: Set up [Qdrant Cloud](https://cloud.qdrant.io/) or a self-hosted instance and generate your API key.
-   **Inputs**:
    -   Qdrant API Key (password)

### Upstash Vector API

Authentication for Upstash Vector.

-   **How to obtain**: Sign up at [Upstash](https://upstash.com/), create a Vector database, and get your access credentials.
-   **Inputs**:
    -   Upstash Vector REST URL
    -   Upstash Vector REST Token (password)

### Vectara API

Authentication for Vectara.

-   **How to obtain**: Register at [Vectara](https://vectara.com/), create a corpus, and obtain your credentials.
-   **Inputs**:
    -   Vectara Customer ID
    -   Vectara Corpus ID
    -   Vectara API Key (password)

### Weaviate API

Authentication for Weaviate.

-   **How to obtain**: Sign up for [Weaviate Cloud](https://weaviate.io/cloud) or set up a self-hosted instance and generate an API key.
-   **Inputs**:
    -   Weaviate API Key (password)

### Voyage AI API

Authentication for Voyage AI embedding services.

-   **How to obtain**: Refer to [official guide](https://docs.voyageai.com/install/#authentication-with-api-keys) on how to get an API Key.
-   **Inputs**:
    -   Voyage AI Endpoint (default: https://api.voyageai.com/v1/embeddings)
    -   Voyage AI API Key (password)

## Content Service Credentials

### Airtable API

Authentication for Airtable.

-   **How to obtain**: Refer to [official guide](https://support.airtable.com/docs/creating-and-using-api-keys-and-access-tokens) on how to get accessToken on Airtable.
-   **Inputs**:
    -   Access Token (password)

### Contentful Delivery API

Authentication for Contentful's content delivery.

-   **How to obtain**: Refer to [official guide](https://www.contentful.com/developers/docs/references/content-delivery-api/) on how to get your delivery and preview keys in Contentful.
-   **Inputs**:
    -   Delivery Token
    -   Preview Token
    -   Space Id

### Contentful Management API

Authentication for Contentful's content management.

-   **How to obtain**: Refer to [official guide](https://www.contentful.com/developers/docs/references/content-Management-api/) on how to get your Management and preview keys in Contentful.
-   **Inputs**:
    -   Management Token
    -   Space Id

### Confluence Cloud API

Authentication for Confluence Cloud.

-   **How to obtain**: Refer to [official guide](https://support.atlassian.com/confluence-cloud/docs/manage-oauth-access-tokens/) on how to get Access Token or [API Token](https://id.atlassian.com/manage-profile/security/api-tokens) on Confluence.
-   **Inputs**:
    -   Access Token (password)
    -   Username/Email
    -   Base URL

### Confluence Server/Data Center API

Authentication for Confluence Server or Data Center.

-   **How to obtain**: Refer to [official guide](https://confluence.atlassian.com/enterprise/using-personal-access-tokens-1026032365.html/) on how to get Personal Access Token on Confluence.
-   **Inputs**:
    -   Personal Access Token (password)

### Figma API

Authentication for Figma.

-   **How to obtain**: Refer to [official guide](https://www.figma.com/developers/api#access-tokens) on how to get accessToken on Figma.
-   **Inputs**:
    -   Access Token (password)

### Github API

Authentication for GitHub.

-   **How to obtain**: Refer to [official guide](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) on how to get accessToken on Github.
-   **Inputs**:
    -   Access Token (password)

### Jira API

Authentication for Jira.

-   **How to obtain**: Create an API token by going to [Atlassian API token management page](https://id.atlassian.com/manage-profile/security/api-tokens).
-   **Inputs**:
    -   Jira API Key (password)
    -   Jira API Email
    -   Jira URL

### Notion API

Authentication for Notion.

-   **How to obtain**: You can find integration token [here](https://developers.notion.com/docs/create-a-notion-integration#step-1-create-an-integration).
-   **Inputs**:
    -   Notion Integration Token (password)

### Salesforce API

Authentication for Salesforce.

-   **How to obtain**: Refer to [official guide](https://developer.salesforce.com/docs/atlas.en-us.api_analytics.meta/api_analytics/sforce_analytics_rest_api_get_started.htm) on how to get your Salesforce API key.
-   **Inputs**:
    -   Salesforce Client ID (password)
    -   Salesforce Client Secret (password)
    -   Salesforce Instance (e.g., https://na1.salesforce.com)

### Slack API

Authentication for Slack.

-   **How to obtain**: Refer to [official guide](https://github.com/modelcontextprotocol/servers/tree/main/src/slack) on how to get botToken and teamId on Slack.
-   **Inputs**:
    -   Bot Token (password)
    -   Team ID

### Stripe API

Authentication for Stripe.

-   **How to obtain**: Sign up at [Stripe](https://stripe.com/), then access your API keys from the [Developer Dashboard](https://dashboard.stripe.com/apikeys).
-   **Inputs**:
    -   Stripe API Token (password)

## Utility Credentials

### Apify API

Authentication for Apify.

-   **How to obtain**: You can find the Apify API token on your [Apify account](https://console.apify.com/account#/integrations) page.
-   **Inputs**:
    -   Apify API (password)

### Arize API

Authentication for Arize observability platform.

-   **How to obtain**: Refer to [official guide](https://docs.arize.com/arize) on how to get API keys on Arize.
-   **Inputs**:
    -   API Key (password)
    -   Space ID
    -   Endpoint (default: https://otlp.arize.com)

### AssemblyAI API

Authentication for AssemblyAI speech-to-text.

-   **How to obtain**: Create an account at [AssemblyAI](https://www.assemblyai.com/) and obtain your API key from the dashboard.
-   **Inputs**:
    -   AssemblyAI Api Key (password)

### AWS security credentials

Authentication for AWS services.

-   **How to obtain**: Your [AWS security credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html). When unspecified, credentials will be sourced from the runtime environment according to the default AWS SDK behavior.
-   **Inputs**:
    -   AWS Access Key (optional)
    -   AWS Secret Access Key (password, optional)
    -   AWS Session Key (password, optional)

### Azure Cognitive Services

Authentication for Azure Cognitive Services.

-   **How to obtain**: Create an Azure account, set up a Cognitive Services resource, and obtain your keys from the Azure portal.
-   **Inputs**:
    -   Azure Subscription Key (password)
    -   Service Region (e.g., "westus", "eastus")
    -   API Version (default: 2024-05-15-preview)

### Composio API

Authentication for Composio.

-   **How to obtain**: Sign up at [Composio](https://composio.dev/) and get your API key from the account settings.
-   **Inputs**:
    -   Composio API Key (password)

### Dynamodb Memory API

Authentication for DynamoDB memory storage.

-   **How to obtain**: Set up an AWS account, create a DynamoDB table, and generate access credentials with appropriate permissions.
-   **Inputs**:
    -   Access Key (password)
    -   Secret Access Key (password)

### E2B API

Authentication for E2B.

-   **How to obtain**: Sign up at [E2B](https://e2b.dev/) and get your API key from the dashboard.
-   **Inputs**:
    -   E2B Api Key (password)

### FireCrawl API

Authentication for FireCrawl.

-   **How to obtain**: You can find the FireCrawl API token on your [FireCrawl account](https://www.firecrawl.dev/) page.
-   **Inputs**:
    -   FireCrawl API (password)
    -   FireCrawl API URL (default: https://api.firecrawl.dev)

### Google OAuth

Authentication for Google services using OAuth.

-   **How to obtain**: Set up a project in the [Google Cloud Console](https://console.cloud.google.com/) and configure OAuth credentials.
-   **Inputs**:
    -   Full Name (disabled)
    -   Email (disabled)
    -   Provider (disabled)
    -   Provider ID (disabled)
    -   Google Access Token (disabled)

### Langfuse API

Authentication for Langfuse monitoring.

-   **How to obtain**: Refer to [integration guide](https://langfuse.com/docs/flowise) on how to get API keys on Langfuse.
-   **Inputs**:
    -   Secret Key (password)
    -   Public Key
    -   Endpoint (default: https://cloud.langfuse.com)

### Langsmith API

Authentication for Langsmith.

-   **How to obtain**: Refer to [official guide](https://docs.smith.langchain.com/) on how to get API key on Langsmith.
-   **Inputs**:
    -   API Key (password)
    -   Endpoint (default: https://api.smith.langchain.com)

### LangWatch API

Authentication for LangWatch monitoring.

-   **How to obtain**: Refer to [integration guide](https://docs.langwatch.ai/integration/python/guide) on how to get API keys on LangWatch.
-   **Inputs**:
    -   API Key (password)
    -   Endpoint (default: https://app.langwatch.ai)

### Lunary AI

Authentication for Lunary AI monitoring.

-   **How to obtain**: Refer to the [official guide](https://lunary.ai/docs?utm_source=flowise) to get a public key.
-   **Inputs**:
    -   Public Key / Project ID
    -   Endpoint (default: https://api.lunary.ai)

### Make.Com API

Authentication for Make.com automation platform.

-   **How to obtain**: Sign up at [Make.com](https://www.make.com/), then access your API key from your profile settings.
-   **Inputs**:
    -   Make.com API Endpoint Url
    -   Make.com Api Key (password)
    -   Team ID
    -   Organization ID

### Momento Cache API

Authentication for Momento Cache.

-   **How to obtain**: Refer to [official guide](https://docs.momentohq.com/cache/develop/authentication/api-keys) on how to get API key on Momento.
-   **Inputs**:
    -   Cache
    -   API Key (password)
    -   Endpoint

### N8n API

Authentication for N8n workflow automation.

-   **How to obtain**: Set up an [N8n instance](https://n8n.io/), then create an API key in the N8n settings.
-   **Inputs**:
    -   N8n API URL (e.g., http://localhost:5678)
    -   API Key (password)

### Phoenix API

Authentication for Phoenix monitoring platform.

-   **How to obtain**: Refer to [official guide](https://docs.arize.com/phoenix) on how to get API keys on Phoenix.
-   **Inputs**:
    -   API Key (password)
    -   Endpoint (default: https://app.phoenix.arize.com)

### Spider API

Authentication for Spider web scraping platform.

-   **How to obtain**: Get your API key from the [Spider](https://spider.cloud) dashboard.
-   **Inputs**:
    -   Spider API Key (password)

### Supabase API

Authentication for Supabase.

-   **How to obtain**: Create a [Supabase](https://supabase.com/) project and get your API key from the project settings.
-   **Inputs**:
    -   Supabase API Key (password)

### Unstructured API

Authentication for Unstructured document processing.

-   **How to obtain**: Refer to [official guide](https://unstructured.io/#get-api-key) on how to get api key on Unstructured.
-   **Inputs**:
    -   API Key (password)

### Upstash Redis API

Authentication for Upstash Redis.

-   **How to obtain**: Refer to [official guide](https://upstash.com/docs/redis/overall/getstarted) on how to create redis instance and get redis REST URL and Token.
-   **Inputs**:
    -   Upstash Redis REST URL
    -   Token (password)

### Upstash Redis Memory API

Authentication for Upstash Redis memory storage.

-   **How to obtain**: Refer to [official guide](https://upstash.com/docs/redis/overall/getstarted) on how to create redis instance and get redis REST Token.
-   **Inputs**:
    -   Upstash Redis REST Token (password)

### WolframAlpha App ID

Authentication for WolframAlpha computational engine.

-   **How to obtain**: Get an App Id from [Wolfram Alpha Portal](https://developer.wolframalpha.com).
-   **Inputs**:
    -   App ID (password)

### Zep Memory API

Authentication for Zep memory storage.

-   **How to obtain**: Refer to [official guide](https://docs.getzep.com/deployment/auth/) on how to create API key on Zep.
-   **Inputs**:
    -   API Key (password)
</file>

<file path="sidekick-studio/credentials/README.md">
# Credentials

Credentials in Sidekick Studio are secure configurations that allow you to authenticate and connect to external services and APIs. They are a crucial component for building integrations with various platforms.

## What are Credentials?

Credentials are encrypted configurations that store authentication information such as:

-   API keys
-   Access tokens
-   Usernames and passwords
-   Connection strings
-   OAuth tokens

Each credential type is designed for a specific service or platform and contains the exact fields required to authenticate with that service.

## How Credentials are Used in Nodes

Nodes in Sidekick Studio are the building blocks of your workflows (chatflows and agentflows). Many nodes require authentication to external services, which is where credentials come in:

1. **Selection**: When configuring a node that requires authentication, you'll see a "Credential" dropdown field.
2. **Reusability**: Once created, credentials can be reused across multiple nodes and workflows.
3. **Abstraction**: Credentials abstract away authentication details, allowing you to focus on building your logic.
4. **Security**: Credential values are encrypted and securely stored, never exposed in the UI or exported configurations.

### Example Node-Credential Relationships

-   **LLM nodes** (like OpenAI, Anthropic, etc.) use their respective API credentials to authenticate with AI services
-   **Vector Database nodes** (like Pinecone, Chroma, etc.) use database credentials to store and retrieve vector data
-   **Search nodes** (like Google Search, Brave Search, etc.) use search API credentials to perform web searches
-   **Integration nodes** (like Slack, Notion, etc.) use platform-specific credentials to interact with these services

## Credential Security

Credentials in Sidekick Studio are:

-   **Encrypted**: All sensitive data is encrypted at rest
-   **Never exposed**: Sensitive values are never returned to the frontend or included in exports
-   **Properly scoped**: Credentials are only accessible to workflows that specifically reference them

## Managing Credentials

You can manage credentials in Sidekick Studio through the Credentials panel:

1. **Create**: Add new credentials for any supported service
2. **Test**: Verify that credentials are valid before using them in workflows
3. **Update**: Modify existing credentials if tokens or passwords change
4. **Delete**: Remove credentials that are no longer needed

## Available Credential Types

Sidekick Studio supports a wide range of credential types across various categories:

-   **AI Services**: OpenAI, Azure OpenAI, Anthropic, Google AI, etc.
-   **Databases**: MongoDB, PostgreSQL, MySQL, Redis, etc.
-   **Vector Stores**: Pinecone, Chroma, Weaviate, etc.
-   **Search Services**: Google Search, Brave Search, etc.
-   **Content Services**: Notion, Confluence, GitHub, etc.
-   **Utility Services**: AWS, Langsmith, Langfuse, etc.

For a complete reference of all available credential types and their configuration details, see the [API Credentials Reference](./api-credentials.md).

## Best Practices

-   Create separate credentials for development and production environments
-   Regularly rotate API keys and tokens for security
-   Use the minimum required permissions when generating API keys
-   Test credentials before using them in production workflows
-   Use environment variables for credential values when possible
</file>

<file path="sidekick-studio/custom-tools/README.md">
---
description: Custom Tools in AnswerAgentAI allow you to integrate your own functions into the AI workflow.
---

# Custom Tools

Custom Tools in AnswerAgentAI allow you to integrate your own functions into the AI workflow.

## Creating a Custom Tool

When creating a custom tool, you need to define several components:

### Tool Name

The tool name is required and should follow these conventions:

-   Use lowercase letters
-   Use underscores instead of spaces
-   Be descriptive and concise

For example: `add_contact_hubspot`

### Tool Description

Provide a clear and concise description of what the tool does. This helps the AI understand when and how to use the tool.

### Input Schema

Define the input parameters your tool requires. Each parameter should have:

-   A name (lowercase, use underscores for spaces)
-   A type (e.g., string, number, boolean)
-   A description used by the AI to understand the parameter
-   Whether it's required or optional

Example input schema:

| Property  | Type   | Description              | Required |
| --------- | ------ | ------------------------ | -------- |
| email     | string | email address of contact | Yes      |
| firstname | string | first name of contact    | No       |
| lastname  | string | last name of contact     | No       |

### JavaScript Function

The JavaScript function is where you implement the logic of your custom tool. Here's an example of how to use the input variables in your code:

### Referencing Input Variables

In your JavaScript function, you can reference the input variables defined in your input schema by using the `$` prefix. This allows you to dynamically access the values provided when the tool is used. For example, if you have an input parameter named `email`, you would reference it in your code as `$email`. This syntax tells the system to replace `$email` with the actual value passed to the tool at runtime. Remember to use this `$` prefix for all input variables to ensure they are correctly interpreted and replaced with their corresponding values.

```javascript
const fetch = require('node-fetch');
const url = 'https://api.hubapi.com/crm/v3/objects/contacts';
const token = 'your-hubspot-api-token';
const body = {
    "properties": {
        "email": $email
    }
};
if (firstname) body.properties.firstname = $firstname;
if (lastname) body.properties.lastname = $lastname;

const options = {
    method: 'POST',
    headers: {
        'Authorization': Bearer ${token},
        'Content-Type': 'application/json'
    },
    body: JSON.stringify(body)
};
try {
    const response = await fetch(url, options);
    const text = await response.text();
    return text;
} catch (error) {
    console.error(error);
    return '';
}
```

In this example:

-   The `email` parameter is required and always included in the request body.
-   `firstname` and `lastname` are optional. The code checks if they exist before adding them to the request body.
-   The function uses the input variables directly (e.g., `email`, `firstname`, `lastname`) without the `$` prefix.

Remember to replace `'your-hubspot-api-token'` with your actual HubSpot API token.

By following these guidelines, you can create powerful custom tools that integrate seamlessly with AnswerAgentAI's workflow.
</file>

<file path="sidekick-studio/documents/document-loaders.md">
---
description: Document Loaders are used to load documents into the AnswerAgentAI knowledge base.
---

# Document Loaders

## Overview

Document loaders are essential components in the process of building and maintaining a document store or knowledge base. They serve as the bridge between various data sources and your document store, enabling you to ingest and process different types of documents efficiently.

In the context of a document store, document loaders perform several crucial functions:

1. **Data Ingestion**: Document loaders extract content from various file formats and data sources, such as PDFs, Word documents, web pages, databases, and APIs.

2. **Text Extraction**: For non-text formats, document loaders convert the content into machine-readable text, making it suitable for further processing and analysis.

3. **Metadata Extraction**: Many document loaders can extract metadata (e.g., author, creation date, tags) from documents, enriching the information stored in your knowledge base.

4. **Preprocessing**: Some document loaders include basic preprocessing capabilities, such as removing unnecessary formatting or standardizing text encoding.

5. **Chunking**: Advanced document loaders may split large documents into smaller, more manageable chunks, which is particularly useful for efficient storage and retrieval in vector databases.

6. **Format Standardization**: Document loaders help standardize diverse data sources into a consistent format that can be easily processed and stored in your document store.

By utilizing document loaders, you can efficiently populate your document store with a wide variety of content, ensuring that your knowledge base remains comprehensive and up-to-date. This flexibility allows you to incorporate multiple data sources and formats into your AI-powered applications, enhancing their capability to access and utilize diverse information.

## Types of Document Loaders

AnswerAgentAI offers a variety of document loaders to accommodate different data sources:

### File-based Loaders

-   [PDF Files](../chatflows/document-loaders/pdf-file.md)
-   [CSV File](../chatflows/document-loaders/csv-file.md)
-   [Text File](../chatflows/document-loaders/text-file.md)
-   [Docx File](../chatflows/document-loaders/docx-file.md)
-   [Json File](../chatflows/document-loaders/json-file.md)
-   [Json Lines File](../chatflows/document-loaders/json-lines-file.md)

### Web and API-based Loaders

-   [API Loader](../chatflows/document-loaders/api-loader.md)
-   [Cheerio Web Scraper](../chatflows/document-loaders/cheerio-web-scraper.md)
-   [Playwright Web Scraper](../chatflows/document-loaders/playwright-web-scraper.md)
-   [Puppeteer Web Scraper](../chatflows/document-loaders/puppeteer-web-scraper.md)
-   [SearchApi For Web Search](../chatflows/document-loaders/searchapi-for-web-search.md)
-   [SerpApi For Web Search](../chatflows/document-loaders/serpapi-for-web-search.md)

### Third-party Service Loaders

-   [Airtable](../chatflows/document-loaders/airtable.md)
-   [Confluence](../chatflows/document-loaders/confluence.md)
-   [Contentful](../chatflows/document-loaders/contentful.md)
-   [Figma](../chatflows/document-loaders/figma.md)
-   [Github](../chatflows/document-loaders/github.md)
-   [Notion Database](../chatflows/document-loaders/notion-database.md)
-   [Notion Folder](../chatflows/document-loaders/notion-folder.md)
-   [Notion Page](../chatflows/document-loaders/notion-page.md)
-   [S3 File Loader](../chatflows/document-loaders/s3-file-loader.md)

### Specialized Loaders

-   [Apify Website Content Crawler](../chatflows/document-loaders/apify-website-content-crawler.md)
-   [Custom Document Loader](../chatflows/document-loaders/custom-document-loader.md)
-   [Document Store](../chatflows/document-loaders/document-store.md)
-   [Folder with Files](../chatflows/document-loaders/folder-with-files.md)
-   [GitBook](../chatflows/document-loaders/gitbook.md)
-   [Unstructured File Loader](../chatflows/document-loaders/unstructured-file-loader.md)
-   [Unstructured Folder Loader](../chatflows/document-loaders/unstructured-folder-loader.md)
-   [VectorStore To Document](../chatflows/document-loaders/vectorstore-to-document.md)
</file>

<file path="sidekick-studio/documents/README.md">
---
description: Learn how to use the Flowise Document Stores
---

# Document Stores

---

Flowise's Document Stores offer a versatile approach to data management, enabling you to upload, split, and prepare your data for upserting your datasets in a single location.

This centralized approach simplifies data handling and allows for efficient management of various data formats, making it easier to organize and access your data within the Flowise app.

## Setup

In this tutorial, we will set up a [Retrieval Augmented Generation (RAG)](../../developers/use-cases/multiple-documents-qna.md) system to retrieve information about the _LibertyGuard Deluxe Homeowners Policy_, a topic that LLMs are likely not extensively trained on.

Using the **Flowise Document Stores**, we'll prepare and upsert data about LibertyGuard and its set of home insurance policies. This will enable our RAG system to accurately answer user queries about LibertyGuard's home insurance offerings.

## 1. Add a Document Store

-   Start by adding a Document Store and naming it. In our case, "LibertyGuard Deluxe Homeowners Policy".

<figure><img src="/.gitbook/assets/ds01.png" alt="" /><figcaption></figcaption></figure>

## 2. Select a Document Loader

-   Enter the Document Store we just created and select the [Document Loader](../../sidekick-studio/chatflows/document-loaders/) you want to use. In our case, since our dataset is in PDF format, we'll use the [PDF Loader](../../sidekick-studio/chatflows/document-loaders/pdf-file.md).

<figure><img src="/.gitbook/assets/ds02.png" alt="" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/ds03.png" alt="" /><figcaption></figcaption></figure>

## 3. Preparing your data

-   First, we start by uploading our PDF file.
-   Then, we add a **unique metadata key**. This is optional, but a good practice as it allows us to target and filter down this same dataset later on if we need to.

<figure><img src="/.gitbook/assets/ds04.png" alt="" /><figcaption></figcaption></figure>

-   Finally, select the [Text Splitter](../../sidekick-studio/chatflows/text-splitters/) you want to use to chunk your data. In our particular case, we will use the [Recursive Character Text Splitter](../../sidekick-studio/chatflows/text-splitters/recursive-character-text-splitter.md).

:::info
In this guide, we've added a generous **Chunk Overlap** size to ensure no relevant data gets missed between chunks. However, the optimal overlap size is dependent on the complexity of your data. You may need to adjust this value based on your specific dataset and the nature of the information you want to extract.
:::

<figure><img src="/.gitbook/assets/ds05.png" alt="" /><figcaption></figcaption></figure>

## 4. Preview your data

-   We can now preview how our data will be chunked using our current [Text Splitter](../../sidekick-studio/chatflows/text-splitters/) configuration; `chunk_size=1500`and `chunk_overlap=750`.

<figure><img src="/.gitbook/assets/ds06.png" alt="" /><figcaption></figcaption></figure>

-   It's important to experiment with different [Text Splitters](../../sidekick-studio/chatflows/text-splitters/), Chunk Sizes, and Overlap values to find the optimal configuration for your specific dataset. This preview allows you to refine the chunking process and ensure that the resulting chunks are suitable for your RAG system.

<figure><img src="/.gitbook/assets/ds07.png" alt="" /><figcaption></figcaption></figure>

:::info
Note that our custom metadata `company: "liberty"` has been inserted into each chunk. This metadata allows us to easily filter and retrieve information from this specific dataset later on, even if we use the same vector store index for other datasets.
:::

## 5. Process your data

-   Once you are satisfied with the chunking process, it's time to process your data.

<figure><img src="/.gitbook/assets/ds08.png" alt="" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/ds09%20(1).png" alt="" /><figcaption></figcaption></figure>

Note that once you have processed your data, you will be able to **edit your chunks** by deleting or adding data to them. This is beneficial if:

-   **You discover inaccuracies or inconsistencies in the original data:** Editing chunks allows you to correct errors and ensure the information is accurate.
-   **You want to refine the content for better relevance:** You can adjust chunks to emphasize specific information or remove irrelevant sections.
-   **You need to tailor chunks for specific queries:** By editing chunks, you can make them more targeted to the types of questions you expect to receive.

<figure><img src="/.gitbook/assets/ds10.png" alt="" /><figcaption></figcaption></figure>

## 6. Add your Document Store node to your flow

-   Now that our dataset is ready to be upserted, it's time to go to your RAG chatflow / agentflow and add the [Document Store node](../../sidekick-studio/chatflows/document-loaders/document-store.md) under the **LangChain > Document Loader** section.

<figure><img src="/.gitbook/assets/ds11.png" alt="" /><figcaption></figcaption></figure>

## 7. Upsert your data to a Vector Store

-   Upsert your dataset to your [Vector Store](../../sidekick-studio/chatflows/vector-stores/) by clicking the green button in the right corner of your flow. We used the [Upstash Vector Store](../../sidekick-studio/chatflows/vector-stores/upstash-vector.md) in our implementation.

<figure><img src="/.gitbook/assets/ds12.png" alt="" /><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/ds13.png" alt="" /><figcaption></figcaption></figure>

## 8. Test your RAG

-   Finally, our Retrieval-Augmented Generation (RAG) system is operational. It's noteworthy how the LLM effectively interprets the query and successfully leverages relevant information from the chunked data to construct a comprehensive response.

<figure><img src="/.gitbook/assets/ds15.png" alt="" /><figcaption></figcaption></figure>

## 9. Summary

We started by creating a Document Store to organize the LibertyGuard Deluxe Homeowners Policy data. This data was then prepared by uploading, chunking, processing, and upserting it, making it ready for our RAG system.

### Key benefits of using the Document Stores

-   **Organization and Management:** The Document Store provides a centralized location for storing, managing, and preparing our data.
-   **Data Quality:** The chunking process helps ensure that our data is structured in a way that facilitates accurate retrieval and analysis.
-   **Flexibility:** The Document Store allows us to refine and adjust our data as needed, improving the accuracy and relevance of our RAG system.
</file>

<file path="sidekick-studio/documents/record-manager.md">
---
description: LangChain Record Manager Nodes
---

# Record Managers

---

Record Managers keep track of your indexed documents, preventing duplicated vector embeddings in [Vector Store](../../sidekick-studio/chatflows/vector-stores/).

When document chunks are upserting, each chunk will be hashed using [SHA-1](https://github.com/emn178/js-sha1) algorithm. These hashes will get stored in Record Manager. If there is an existing hash, the embedding and upserting process will be skipped.

In some cases, you might want to delete existing documents that are derived from the same sources as the new documents being indexed. For that, there are 3 cleanup modes for Record Manager:

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

<Tabs>
  <TabItem value="incremental" label="Incremental" default>

When you are upserting multiple documents, and you want to prevent deletion of the existing documents that are not part of the current upserting process, use **Incremental** Cleanup mode.

1. Let's have a Record Manager with `Incremental` Cleanup and `source` as SourceId Key

<div align="left">

<figure><img src="/.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="264"/><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="410"/><figcaption></figcaption></figure>

</div>

2. And have the following 2 documents:

| Text | Metadata         |
| ---- | ---------------- |
| Cat  | `{source:"cat"}` |
| Dog  | `{source:"dog"}` |

<div align="left">

<figure><img src="/.gitbook/assets/image (11) (1) (1).png" alt="" width="202"/><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (10) (1) (1) (1) (1).png" alt="" width="563"/><figcaption></figcaption></figure>

</div>

<div align="left">

<figure><img src="/.gitbook/assets/image (2) (1) (1) (1) (1) (2).png" alt="" width="231"/><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (2).png" alt="" width="563"/><figcaption></figcaption></figure>

</div>

3. After an upsert, we will see 2 documents that are upserted:

<figure><img src="/.gitbook/assets/image (9) (1) (1) (1) (1) (2).png" alt="" width="433"/><figcaption></figcaption></figure>

4. Now, if we delete the **Dog** document, and update **Cat** to **Cats**, we will now see the following:

<figure><img src="/.gitbook/assets/image (13) (2).png" alt="" width="425"/><figcaption></figcaption></figure>

-   The original **Cat** document is deleted
-   A new document with **Cats** is added
-   **Dog** document is left untouched
-   The remaining vector embeddings in Vector Store are **Cats** and **Dog**

<figure><img src="/.gitbook/assets/image (15).png" alt="" width="448"/><figcaption></figcaption></figure>

  </TabItem>
  <TabItem value="full" label="Full">

When you are upserting multiple documents, **Full** Cleanup mode will automatically delete any vector embeddings that are not part of the current upserting process.

1. Let's have a Record Manager with `Full` Cleanup. We don't need to have a SourceId Key for Full Cleanup mode.

<div align="left">

<figure><img src="/.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="264"/><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (17).png" alt="" width="407"/><figcaption></figcaption></figure>

</div>

2. And have the following 2 documents:

| Text | Metadata         |
| ---- | ---------------- |
| Cat  | `{source:"cat"}` |
| Dog  | `{source:"dog"}` |

<div align="left">

<figure><img src="/.gitbook/assets/image (11) (1) (1).png" alt="" width="202"/><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (10) (1) (1) (1) (1).png" alt="" width="563"/><figcaption></figcaption></figure>

</div>

<div align="left">

<figure><img src="/.gitbook/assets/image (2) (1) (1) (1) (1) (2).png" alt="" width="231"/><figcaption></figcaption></figure>

<figure><img src="/.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (2).png" alt="" width="563"/><figcaption></figcaption></figure>

</div>

3. After an upsert, we will see 2 documents that are upserted:

<figure><img src="/.gitbook/assets/image (9) (1) (1) (1) (1) (2).png" alt="" width="433"/><figcaption></figcaption></figure>

4. Now, if we delete the **Dog** document, and update **Cat** to **Cats**, we will now see the following:

<figure><img src="/.gitbook/assets/image (18).png" alt="" width="430"/><figcaption></figcaption></figure>

-   The original **Cat** document is deleted
-   A new document with **Cats** is added
-   **Dog** document is deleted
-   The remaining vector embeddings in Vector Store is just **Cats**

<figure><img src="/.gitbook/assets/image (19).png" alt="" width="527"/><figcaption></figcaption></figure>

  </TabItem>
  <TabItem value="none" label="None">

No cleanup will be performed

  </TabItem>
</Tabs>

Current available Record Manager nodes are:

-   SQLite
-   MySQL
-   PostgresQL

## Resources

-   [LangChain Indexing](https://js.langchain.com/docs/modules/data_connection/indexing/)
</file>

<file path="sidekick-studio/sidekick-settings/allowed-domains.md">
---
description: Allowed Domains
---

# Allowed Domains

The `AllowedDomains` component is a crucial part of the chatbot configuration interface. It allows administrators to control and manage the domains from which the chatbot can be accessed.

## Purpose

This component serves two main purposes:

1. **Domain Restriction**: It enables you to specify which domains are allowed to use your chatbot.
2. **Custom Error Message**: It allows you to set a custom error message for unauthorized domain access attempts.

## Features

### Allowed Domains List

-   **Add Domains**: You can add multiple domains where your chatbot is allowed to run.
-   **Remove Domains**: Each domain entry can be removed individually.
-   **Format**: Domains should be entered in the format `https://example.com`.

### Error Message Customization

-   You can set a custom error message that will be displayed when someone tries to access the chatbot from an unauthorized domain.

## How to Use

1. **Adding Allowed Domains**:

    - Enter the full URL of the allowed domain (e.g., `https://example.com`) in the input field.
    - Click the '+' icon to add more domain fields if needed.

2. **Removing Domains**:

    - Click the trash icon next to any domain to remove it from the list.

3. **Setting Custom Error Message**:

    - Enter your desired error message in the "Error Message" field.
    - This message will be shown to users who try to access the chatbot from unauthorized domains.

4. **Saving Changes**:
    - After making your desired changes, click the "Save" button to apply the settings.

## Important Notes

-   Ensure that all domains where you intend to use the chatbot are listed.
-   The chatbot will only function on the domains specified in this list.
-   If no domains are specified, the chatbot may be inaccessible.
-   Changes take effect immediately after saving.

## Security Implications

This feature enhances the security of your chatbot by restricting its usage to specific domains, preventing unauthorized embedding or access from unintended websites.
</file>

<file path="sidekick-studio/sidekick-settings/chat-feedback.md">
---
description: Chat Feedback
---

# Chat Feedback

The `ChatFeedback` component allows administrators to configure feedback options for the chatbot interface. This component is part of the chatbot configuration settings.

## Purpose

The main purpose of this component is to enable or disable the chat feedback feature in the chatbot interface.

## Features

### Enable/Disable Chat Feedback

-   A simple toggle switch allows you to turn the chat feedback feature on or off.
-   When enabled, users will have the ability to provide feedback on the chatbot's responses.

## How to Use

1. **Accessing the Settings**:

    - Navigate to the chatbot configuration interface.
    - Locate the "Chat Feedback" section.

2. **Enabling/Disabling Chat Feedback**:

    - Use the toggle switch labeled "Enable chat feedback" to turn the feature on or off.
    - When the switch is on (blue), chat feedback is enabled.
    - When the switch is off (gray), chat feedback is disabled.

3. **Saving Changes**:
    - After adjusting the setting, click the "Save" button to apply your changes.
    - A success message will appear if the settings are saved successfully.

## Important Notes

-   Changes take effect immediately after saving.
-   If you encounter any errors while saving, an error message will be displayed with details.
-   The chat feedback status is stored as part of the chatbot's configuration and is associated with the specific chatflow.

## Technical Details

-   The component uses Redux for state management and dispatching actions.
-   The chat feedback status is stored in the `chatbotConfig` object within the chatflow data.
-   When saved, the configuration is updated via an API call to `updateChatflow`.

## Error Handling

If an error occurs while saving the settings, an error message will be displayed with the following information:

-   The reason for the failure (e.g., network error, server error)
-   Any specific error message returned by the server

## Security Implications

Enabling chat feedback allows users to provide input on the quality of the chatbot's responses. This can be valuable for improving the chatbot's performance, but it also opens a channel for user input. Ensure that any feedback data is handled securely and in compliance with relevant data protection regulations.
</file>

<file path="sidekick-studio/sidekick-settings/general.md">
---
description: General
---

# General Settings

The `GeneralSettings` component allows administrators to configure general settings for a chatflow. This component is part of the chatbot configuration interface.

## Purpose

The main purpose of this component is to enable users to edit basic information about a chatflow, including its title, description, categories, and display mode.

## Features

### Chatflow Title

-   Allows you to set or change the title of the chatflow.
-   This title is used to identify the chatflow in various parts of the application.

### Description

-   Provides a text area to add or edit a detailed description of the chatflow.
-   This can be used to explain the purpose or functionality of the chatflow.

### Categories

-   Allows you to add, edit, or remove categories associated with the chatflow.
-   Categories are useful for organizing and filtering chatflows.

### Display Mode

-   **Chatbot**: Display the chatflow as an interactive chatbot interface.
-   **Embedded Form**: Display an embedded form within an iframe.
    -   When "Embedded Form" is selected, you can specify the URL of the page to embed.
    -   The embedded URL will be displayed in the user interface where the chatbot would normally appear.

## How to Use

1. **Accessing the Settings**:

    - Navigate to the chatflow configuration interface.
    - Locate the "General Settings" section.

2. **Editing the Chatflow Title**:

    - Enter or modify the title in the "Chatflow Title" text field.

3. **Editing the Description**:

    - Enter or modify the description in the "Description" text area.
    - This field supports multiple lines of text.

4. **Managing Categories**:

    - Use the tag input field to add, edit, or remove categories.
    - Type a category name and press Enter to add it.
    - Click on the 'x' next to a category to remove it.

5. **Setting the Display Mode**:

    - In the "Display Mode" section, choose between "Chatbot" and "Embedded Form".
    - If you select "Embedded Form":
        - An input field labeled "Embedded URL" will appear.
        - Enter the URL of the page you wish to embed.
        - This URL will be shown in the user interface within an iframe.

6. **Saving Changes**:
    - After making your desired changes, click the "Save" button to apply the settings.
    - A success message will appear if the settings are saved successfully.

## Important Notes

-   Changes take effect immediately after saving.
-   If you encounter any errors while saving, an error message will be displayed.
-   The chatflow title, description, categories, and display mode settings are stored as part of the chatflow data.
-   When using the "Embedded Form" display mode, ensure that the URL provided is valid and allows embedding.

## Technical Details

-   The component uses Redux for state management and dispatching actions.
-   When saved, the configuration is updated via an API call to `updateChatflow`.
-   Categories are stored as a semicolon-separated string in the backend.
-   Display mode and embedded URL settings are stored in the chatflow's configuration.

## Error Handling

If an error occurs while saving the settings, an error message will be displayed indicating that the update failed.

## Security Implications

-   Be cautious when embedding external URLs. Ensure that the embedded content is from a trusted source.
-   Some websites may prevent embedding via iframe due to security policies.
-   Ensure that any information entered doesn't contain confidential or sensitive details, as it may be visible to users with access to the chatflow configuration.
</file>

<file path="sidekick-studio/sidekick-settings/rate-limit.md">
---
description: Learn how to managing API requests in AnswerAgentAI
---

# Rate Limit

---

When you share your chatflow to public with no API authorization through API or embedded chat, anybody can access the flow. To prevent spamming, you can set the rate limit on your chatflow.

<figure><img src="/.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt="" width="462" /><figcaption></figcaption></figure>

-   **Message Limit per Duration**: How many messages can be received in a specific duration. Ex: 20
-   **Duration in Seconds**: The specified duration. Ex: 60
-   **Limit Message**: What message to return when the limit is exceeded. Ex: Quota Exceeded

Using the example above, that means only 20 messages are allowed to be received in 60 seconds. The rate limitation is tracked by IP-address. If you have deployed AnswerAgentAI on cloud service, you'll have to set `NUMBER_OF_PROXIES` env variable.

## Cloud-Hosted Rate Limit Setup Guide

1. **Cloud Host AnswerAgentAI:** Start by hosting AnswerAgentAI in the cloud.
2. **Set Environment Variable:** Create an environment variable named `NUMBER_OF_PROXIES` and set its value to `0` in your hosting environment.
3. **Restart Cloud-Hosted AnswerAgentAI Service:** This enables AnswerAgentAI to apply changes of environment variables.
4. **Check IP Address:** To verify the IP address, access the following URL: `{{hosted_url}}/api/v1/ip`. You can do this either by entering the URL into your web browser or by making an API request.
5. **Compare IP Address** After making the request, compare the IP address returned to your current IP address. You can find your current IP address by visiting either of these websites:
    - [http://ip.nfriedly.com/](http://ip.nfriedly.com/)
    - [https://api.ipify.org/](https://api.ipify.org/)
6. **Incorrect IP Address:** If the returned IP address does not match your current IP address, increase `NUMBER_OF_PROXIES` by 1 and restart Cloud-Hosted AnswerAgentAI. Repeat this process until the IP address matches your own.
</file>

<file path="sidekick-studio/sidekick-settings/README.md">
---
description: Learn how to managing API requests in Flowise
---

# Chat/Agent Settings

---

This section provides in-depth guides on core AnswerAgentAI functionalities, including API usage, variables, and telemetry collection practices.

## Guides

-   [General](general.md)
-   [Rate Limit](rate-limit.md)
-   [Allowed Domains](allowed-domains.md)
-   [Chat Feedback](chat-feedback.md)
-   [Speech To Text](speech-to-text.md)
-   [Starter Prompts](starter-prompts.md)
-   [Visibility](visibility.md)
</file>

<file path="sidekick-studio/sidekick-settings/speech-to-text.md">
---
description: Learn how to manage speech-to-text in AnswerAgentAI
---

# Speech to Text

The `SpeechToText` component allows administrators to configure speech-to-text functionality for their chatbot. This component is part of the chatbot configuration interface and enables the conversion of spoken language into written text.

## Purpose

The main purpose of this component is to enable administrators to set up and manage speech-to-text capabilities, allowing users to interact with the chatbot using voice input.

## Features

### Provider Selection

-   Allows you to choose from multiple speech-to-text providers:
    -   OpenAI Whisper
    -   Assembly AI
    -   LocalAI STT
-   Option to disable speech-to-text by selecting "None"

### Provider-Specific Configuration

Each provider has its own set of configuration options, which may include:

-   API Credentials
-   Language settings
-   Model selection
-   Advanced parameters (e.g., temperature, prompts)

## How to Use

1. **Accessing the Settings**:

    - Navigate to the chatflow configuration interface.
    - Locate the "Speech to Text" section.

2. **Selecting a Provider**:

    - Use the dropdown menu to select a speech-to-text provider.
    - Options include "None" (to disable), "OpenAI Whisper", "Assembly AI", and "LocalAI STT".

3. **Configuring Provider Settings**:

    - Once a provider is selected, its specific configuration options will appear.
    - Fill in the required fields and any optional parameters as needed.

4. **OpenAI Whisper Configuration**:

    - Connect OpenAI API credentials
    - Optionally set language, prompt, and temperature

5. **Assembly AI Configuration**:

    - Connect Assembly AI API credentials

6. **LocalAI STT Configuration**:

    - Connect LocalAI API credentials
    - Set the base URL for the local AI server
    - Optionally configure language, model, prompt, and temperature

7. **Saving Changes**:
    - After configuring the settings, click the "Save" button to apply the speech-to-text configuration.
    - A success message will appear if the settings are saved successfully.

## Important Notes

-   Only one speech-to-text provider can be active at a time.
-   Ensure that you have the necessary API credentials for the selected provider.
-   Some providers may require additional setup or have usage limits. Refer to the provider's documentation for more information.
-   The "Save" button will be disabled if a provider is selected but no credential is provided.

## Technical Details

-   The component uses Redux for state management and dispatching actions.
-   Speech-to-text settings are stored in the `speechToText` field of the chatflow data as a JSON string.
-   When saved, the configuration is updated via an API call to `updateChatflow`.

## Error Handling

If an error occurs while saving the settings, an error message will be displayed with details about the failure.

## Security Implications

-   Ensure that API credentials are kept secure and not exposed to unauthorized parties.
-   Be aware of the data privacy implications of using cloud-based speech-to-text services, especially when handling sensitive information.
-   For LocalAI STT, ensure that the local server is properly secured and accessible only to authorized systems.

## Customization

The speech-to-text functionality can be further customized by adjusting provider-specific parameters such as language, prompts, and temperature settings. These allow you to fine-tune the accuracy and behavior of the speech recognition for your specific use case.
</file>

<file path="sidekick-studio/sidekick-settings/starter-prompts.md">
---
description: Learn how to manage starter prompts in AnswerAgentAI
---

# Starter Prompts Settings

The `StarterPrompts` component allows administrators to configure conversation starter prompts for their chatbot. These prompts are designed to help initiate conversations and guide users when they first interact with the chatbot.

## Purpose

The main purpose of this component is to provide a way for chatbot administrators to set up pre-defined prompts or questions that will be displayed to users when they start a new conversation with the chatbot. This feature is particularly useful for:

1. Guiding users on how to begin interacting with the chatbot
2. Suggesting common topics or questions that the chatbot can assist with
3. Improving user engagement by providing clear starting points for conversations

## Features

### Dynamic Prompt Management

-   Add multiple starter prompts
-   Remove individual prompts
-   Edit existing prompts

### Flexible Configuration

-   No limit on the number of starter prompts that can be added
-   Each prompt can be customized to fit specific use cases or common user inquiries

## How to Use

1. **Accessing the Settings**:

    - Navigate to the chatflow configuration interface.
    - Locate the "Starter Prompts" section.

2. **Adding Starter Prompts**:

    - Enter a prompt in the provided input field.
    - Click the '+' icon to add more prompt fields.

3. **Editing Prompts**:

    - Simply type in the input field to modify an existing prompt.

4. **Removing Prompts**:

    - Click the trash icon next to a prompt to remove it.
    - Note: You must have at least one prompt field (even if it's empty).

5. **Saving Changes**:
    - After configuring the starter prompts, click the "Save" button to apply the changes.
    - A success message will appear if the settings are saved successfully.

## Where Starter Prompts Appear

Starter prompts are displayed in the chatbot interface under the following conditions:

-   When a user starts a new conversation with the chatbot
-   When there are no existing messages in the chat history

The prompts typically appear as clickable suggestions or buttons that users can select to initiate a conversation based on those topics.

## Usefulness

Starter prompts are valuable for several reasons:

1. **User Guidance**: They help users understand what kind of questions or topics the chatbot can assist with.
2. **Reduced User Friction**: By providing clickable options, users can quickly start a conversation without having to think of how to phrase their first message.
3. **Increased Engagement**: Prompts can showcase the chatbot's capabilities, encouraging users to explore various features or topics.
4. **Consistency**: They ensure that users are directed towards topics that the chatbot is well-equipped to handle.
5. **Customization**: Administrators can tailor the prompts to match specific use cases, industries, or common user needs.

## Important Notes

-   Starter prompts will only be shown when there are no messages in the chat history.
-   Changes take effect immediately after saving.
-   You can add as many prompts as needed, but consider keeping the list concise for better user experience.
-   Regularly review and update your starter prompts based on user interactions and feedback.

## Technical Details

-   The component uses Redux for state management and dispatching actions.
-   Starter prompts are stored in the `chatbotConfig` field of the chatflow data as a JSON string.
-   When saved, the configuration is updated via an API call to `updateChatflow`.

## Error Handling

If an error occurs while saving the settings, an error message will be displayed with details about the failure.

## Best Practices

1. Keep prompts clear and concise.
2. Use action-oriented language that encourages users to engage.
3. Cover a range of common topics or questions relevant to your chatbot's purpose.
4. Regularly analyze which prompts are most frequently used and adjust accordingly.
5. Consider localizing prompts if your chatbot supports multiple languages.
</file>

<file path="sidekick-studio/sidekick-settings/visibility.md">
---
description: Learn how to manage visibility settings in AnswerAgentAI
---

# Visibility Settings

The `VisibilitySettings` component allows administrators and users to control the visibility and accessibility of their workflows within the Flowise platform. This component is crucial for managing who can view and interact with specific chatflows.

## Purpose

The main purpose of this component is to provide granular control over workflow visibility, enabling users to:

1. Manage privacy settings for individual workflows
2. Control access within an organization
3. Share workflows with the broader Flowise community
4. Make workflows available in public marketplaces or browser extensions

## Visibility Options

The component offers the following visibility options:

1. **Private**: Only visible to the workflow creator
2. **Organization**: Visible to all members of the creator's organization
3. **AnswerAgentAI**: Visible to AnswerAgentAI users
4. **Marketplace**: Available in the public marketplace
5. **Browser Extension**: Accessible via browser extension

## Features

### Dynamic Visibility Control

-   Users can select multiple visibility options for each workflow
-   Options can be toggled on and off independently

### Permission-Based Access

-   Certain visibility options may be disabled based on user permissions or organization settings
-   Tooltips provide information on how to enable restricted options

### Flagsmith Integration

-   Uses feature flags to control the availability of certain visibility options

## How to Use

1. **Accessing the Settings**:

    - Navigate to the workflow configuration interface
    - Locate the "Workflow visibility" section

2. **Selecting Visibility Options**:

    - Check the boxes next to the desired visibility options
    - Multiple options can be selected simultaneously

3. **Understanding Restrictions**:

    - Disabled options will have tooltips explaining why they're unavailable
    - Contact your organization admin to enable restricted options

4. **Saving Changes**:
    - After configuring the visibility settings, click the "Save" button to apply the changes
    - A success message will appear if the settings are saved successfully

## Importance of Visibility Settings

1. **Privacy Control**: Allows users to keep sensitive or experimental workflows private
2. **Collaboration**: Enables sharing within an organization for team collaboration
3. **Community Contribution**: Facilitates sharing valuable workflows with the broader Flowise community
4. **Marketplace Presence**: Allows users to make their workflows available for public use or purchase
5. **Extended Accessibility**: Enables workflows to be accessed via browser extensions for easier use

## Technical Details

-   The component uses Redux for state management and dispatching actions
-   Visibility settings are stored in the `visibility` field of the chatflow data
-   When saved, the configuration is updated via an API call to `updateChatflow`
-   Feature flags from Flagsmith control the availability of certain visibility options

## Error Handling

If an error occurs while saving the settings, an error message will be displayed with details about the failure.

## Best Practices

1. Regularly review and update visibility settings for your workflows
2. Use the most restrictive visibility setting necessary for each workflow
3. Coordinate with your organization admin to enable additional visibility options if needed
4. Consider the implications of making workflows public or available in marketplaces
5. Use organization-wide visibility for collaborative projects within your team

## Security Implications

-   Carefully consider the sensitivity of your workflow data before changing visibility settings
-   Be aware that public or marketplace workflows may be accessed by unknown users
-   Ensure that any sensitive information or API keys are not exposed in public workflows

By properly configuring visibility settings, users can maintain control over their workflows while leveraging the collaborative and community-driven aspects of the Flowise platform.
</file>

<file path="sidekick-studio/README.md">
---
sidebar_position: 1
title: Introduction
description: AnswerAgentAI's Sidekick Studio is a visual development environment for building generative AI applications.
---

# Introduction

The AnswerAgentAI Sidekick Studio provides a powerful canvas for building generative AI applications through a variety of specialized nodes. These nodes are organized into distinct categories, enabling users to create sophisticated workflows with ease.

## Overview

Sidekick Studio Nodes are the building blocks of your AI workflows. Each node represents a specific functionality or component that can be connected to others, allowing you to create complex AI-powered applications without writing code.

## Key Benefits

-   **Modular Design**: Mix and match different nodes to create custom AI solutions.
-   **Visual Workflow**: Easily understand and modify your AI processes through a graphical interface.
-   **Extensive Functionality**: Access a wide range of AI capabilities through specialized nodes.

## How to Use

1. Open the Sidekick Studio in your AnswerAgentAI dashboard.
2. Drag and drop nodes from the sidebar onto the canvas.
3. Connect nodes by drawing lines between their input and output ports.
4. Configure each node by clicking on it and adjusting its settings in the properties panel.
5. Test your workflow using the built-in debugging tools.

<figure><img src="/.gitbook/assets/screenshots/sidekick studio .png" alt="" /><figcaption><p>Sidekick Studio &#x26; Drop UI</p></figcaption></figure><!-- TODO: Screenshot of the Sidekick Studio interface with nodes on the canvas -->

## Node Categories

Sidekick Studio offers nodes in the following categories:

-   [**Agents**](chatflows/agents/): Autonomous AI entities that can perform tasks or make decisions.
-   [**Cache**](chatflows/cache/): Nodes for storing and retrieving temporary data to improve performance.
-   [**Chains**](chatflows/chains/): Sequence multiple operations for complex AI processing.
-   [**Chat Models**](chatflows/chat-models/): Integrate conversational AI models into your workflows.
-   [**Document Loaders**](chatflows/document-loaders/): Import and process various document formats.
-   [**Embeddings**](chatflows/embeddings/): Create vector representations of text for advanced NLP tasks.
-   [**Memory**](chatflows/memory/): Implement context retention in conversational workflows.
-   [**Moderation**](chatflows/moderation/): Filter and monitor content for appropriateness.
-   [**Output Parsers**](chatflows/output-parsers/): Extract structured data from AI-generated outputs.
-   [**Prompts**](chatflows/prompts/): Design and manage input prompts for AI models.
-   [**Record Managers**](documents/record-manager.md): Organize and maintain data records within workflows.
-   [**Retrievers**](chatflows/retrievers/): Fetch relevant information from knowledge bases or databases.
-   [**Text Splitters**](chatflows/text-splitters/): Divide large text inputs into manageable chunks.
-   [**Tools**](chatflows/tools/): Utility nodes for various helper functions and integrations.
-   [**Vector Stores**](chatflows/vector-stores/): Manage and query vector databases for similarity search.

<figure><img src="/.gitbook/assets/screenshots/sidekick studio node.png" alt="" /><figcaption><p>Sidekick Studio Nodes &#x26; Drop UI</p></figcaption></figure>

## Tips and Best Practices

-   Start with a clear goal for your workflow before adding nodes.
-   Use the search function in the node sidebar to quickly find specific nodes.
-   Group related nodes together to keep your canvas organized.
-   Regularly test your workflow as you build to catch issues early.
-   Use comments and labels to document your workflow for future reference.

By mastering the use of Sidekick Studio Nodes, you can create powerful, custom AI solutions tailored to your specific needs. Explore each category to discover the full potential of AnswerAgentAI's Sidekick Studio.

-   [Settings](sidekick-settings/README.md)
-   [Agent Flows](agentflows/README.md)
</file>

<file path="sidekick-studio/variables.md">
---
description: Learn how to use variables in Flowise
---

# Variables

---

Flowise allow users to create variables that can be used in:

-   [Custom Tool](sidekick-studio/chatflows/tools/custom-tool.md)
-   [Custom Function](sidekick-studio/chatflows/utilities/custom-js-function.md)
-   [Custom Loader](sidekick-studio/chatflows/document-loaders/custom-document-loader.md)
-   [If Else](sidekick-studio/chatflows/utilities/if-else.md)

For example, you have a database URL that you do not want it to be exposed on the function, but you still want the function to be able to read the URL from your environment variable.

User can create a variable and get the variable like so:

`$vars.<variable-name>`

Variables can be Static or Runtime.

## Static

Static variable will be saved with the value specified, and retrieved as it is.

<figure><img src="/img/screenshots/variables-static.png" alt="Static variable" width="542" /><figcaption>Static variable</figcaption></figure>

## Runtime

Value of the variable will be fetched from **.env** file using `process.env`

<figure><img src="/img/screenshots/variables-runtime.png" alt="Runtime variable" width="537" /><figcaption>Runtime variable</figcaption></figure>

## Resources

-   [Pass Variables to Function](sidekick-studio/chatflows/tools/custom-tool.md)
</file>

<file path="use-cases/agent-use-cases.mdx">
---
title: AI Agent Use Cases
description: 40 Potential Use Cases for AI Agents Across Departments
sidebar_label: Agent Use Cases
slug: /use-cases/agent-use-cases
---

# AI Agent Use Cases Across Departments

This document outlines 40 potential use cases for AI agents equipped with the AnswerAgentAI toolkit, categorized by department. Each use case includes a summary and how an agent can automate administrative tasks.

## Sales

### 1. Lead Qualification & Enrichment

-   **Summary**: Automatically enrich new leads with company data, social profiles, and recent news to help sales reps prioritize and personalize outreach.
-   **Agent Admin Task Automation**: The agent can use search tools (like `BraveSearchAPI`, `GoogleSearchAPI`, `ExaSearch`) and the `WebBrowser` tool to gather publicly available information on leads. It can then use `CodeInterpreterE2B` for basic data formatting or integrate with a CRM via `SfdcMCP` (Salesforce), `Composio`, `N8n`, or `MakeComWebhook` to update lead records with the enriched data.

### 2. Personalized Outreach Email Drafting

-   **Summary**: Generate personalized email drafts for sales outreach based on a lead's industry, role, recent company news, or expressed interests.
-   **Agent Admin Task Automation**: The agent leverages search tools and `WebBrowser` for in-depth research on the lead. The core LLM drafts the email content. `CodeInterpreterE2B` can format the email or prepare data for merge tags. Integration with email platforms can be achieved via `Composio`, `N8n`, or `MakeComWebhook`.

### 3. Automated Sales Follow-up Scheduling

-   **Summary**: Schedule follow-up meetings or calls directly into the sales rep's calendar based on lead interaction triggers or a predefined sales cadence.
-   **Agent Admin Task Automation**: The agent utilizes Google Calendar tools (`CreateCalendarEvent`, `RetrieveCalendarEvent` to check availability). It can log this activity in a CRM using `SfdcMCP` or through `Composio` if connected to other CRM/sales engagement platforms.

### 4. Competitor Monitoring & Analysis

-   **Summary**: Continuously track competitors' announcements, product launches, pricing changes, and significant news.
-   **Agent Admin Task Automation**: The agent can be scheduled to regularly use search tools (`BraveSearchAPI`, `GoogleSearchAPI`, etc.) and the `WebBrowser` tool. Findings can be logged using `WriteFile`, summarized by `CodeInterpreterE2B`, or stored in a knowledge base like Confluence via `ConfluenceMCP`.

### 5. Sales Proposal Generation Support

-   **Summary**: Assist sales reps by gathering necessary information, drafting standard sections of sales proposals, or finding relevant case studies.
-   **Agent Admin Task Automation**: The agent employs the `RetrieverTool` to fetch product information or internal case studies. Search tools gather market data. `CreateDalleImage` can generate simple visuals or diagrams. The core LLM drafts text sections.

### 6. CRM Data Entry & Updates (Salesforce)

-   **Summary**: Automate the creation of new accounts, contacts, opportunities, and log call/meeting notes or interactions in Salesforce.
-   **Agent Admin Task Automation**: The agent directly uses the `SfdcMCP` tool to perform CRUD (Create, Read, Update, Delete) operations on Salesforce objects, ensuring data hygiene and timely updates.

### 7. Meeting Preparation & Briefing Document Creation

-   **Summary**: Compile comprehensive briefing notes for sales representatives before client meetings, including attendee profiles, company information, and past interaction history.
-   **Agent Admin Task Automation**: The agent uses search tools, `WebBrowser` for public data, `SfdcMCP` to pull past interaction history from Salesforce, and potentially `Composio` (if connected to LinkedIn Sales Navigator or similar). The LLM summarizes this into a briefing, saved via `WriteFile` or to `ConfluenceMCP`.

### 8. Identifying Upsell/Cross-sell Opportunities

-   **Summary**: Analyze existing customer data, product usage patterns, and support interactions to flag potential upsell or cross-sell opportunities.
-   **Agent Admin Task Automation**: The agent queries customer data using `SfdcMCP` or `PostgreSQLMCP`. `CodeInterpreterE2B` can be used for data analysis and pattern recognition.

### 9. Drafting Sales Performance Reports

-   **Summary**: Automatically generate weekly or monthly sales performance reports, highlighting key metrics, achievements, and areas for attention.
-   **Agent Admin Task Automation**: The agent queries `SfdcMCP` or `PostgreSQLMCP` for sales data. `CodeInterpreterE2B` processes the data, generates textual summaries, and potentially chart data (which could then be visualized if another tool is added or by describing the chart). The report is saved using `WriteFile`.

### 10. Payment Link Generation (Stripe)

-   **Summary**: Quickly generate Stripe payment links for new deals, specific products, or services as requested by the sales team.
-   **Agent Admin Task Automation**: The agent uses the `StripeTool` to interact with Stripe, specifically its payment link creation capabilities, streamlining the process for sales reps.

## Marketing

### 11. Content Idea Generation & Trend Research

-   **Summary**: Research trending topics, relevant keywords, and competitor content to generate fresh ideas for blog posts, social media campaigns, and other marketing materials.
-   **Agent Admin Task Automation**: The agent extensively uses search tools (`BraveSearchAPI`, `GoogleSearchAPI`, `ExaSearch`, `TavilyAPI`) and `WebBrowser`. The `RetrieverTool` can be used if an internal swipe file or content performance database exists.

### 12. Drafting Social Media Posts & Scheduling

-   **Summary**: Create engaging social media posts tailored for different platforms (e.g., Twitter, LinkedIn, Facebook) and schedule them.
-   **Agent Admin Task Automation**: The core LLM generates post content. `CreateDalleImage` provides accompanying visuals. `Composio`, `N8n`, or `MakeComWebhook` can connect to social media scheduling tools (e.g., Buffer, Hootsuite) to queue posts.

### 13. Blog Post Outline & Draft Creation

-   **Summary**: Generate structured outlines and initial drafts for blog posts based on a given topic, keywords, or research.
-   **Agent Admin Task Automation**: The agent uses search tools for in-depth research on the topic. The core LLM generates a detailed outline and can then expand sections into a first draft.

### 14. Generating Marketing Email Copy

-   **Summary**: Draft compelling copy for marketing newsletters, promotional emails, automated drip campaigns, and landing pages.
-   **Agent Admin Task Automation**: The core LLM generates the email copy. `RetrieverTool` can fetch product specifications or existing marketing messaging guidelines.

### 15. Market Trend Analysis & Reporting

-   **Summary**: Monitor industry trends, consumer behavior shifts, and emerging technologies, then generate summary reports for the marketing team.
-   **Agent Admin Task Automation**: The agent uses search tools and `WebBrowser` for continuous monitoring. `CodeInterpreterE2B` analyzes and summarizes collected data. Reports are stored via `WriteFile` or in `ConfluenceMCP`.

### 16. Creating Basic Marketing Visuals & Graphics

-   **Summary**: Generate simple images, icons, or graphics for social media posts, blog headers, or internal presentations.
-   **Agent Admin Task Automation**: The agent utilizes the `CreateDalleImage` tool based on prompts from the marketing team.

### 17. SEO Keyword Research & Content Gap Analysis

-   **Summary**: Identify relevant keywords with good search volume and low competition, and find content gaps on the company website compared to competitors.
-   **Agent Admin Task Automation**: The agent uses search tools and `WebBrowser` to analyze SERPs and competitor sites. `CodeInterpreterE2B` can help process lists of keywords or URLs.

### 18. Competitor Ad Copy & Landing Page Analysis

-   **Summary**: Analyze competitor advertising copy, calls to action, and landing page designs to identify effective strategies and areas for differentiation.
-   **Agent Admin Task Automation**: The agent employs search tools (e.g., searching for "competitor X ads") and the `WebBrowser` tool to examine ad copy and landing pages.

### 19. Managing Content in CMS (Contentful)

-   **Summary**: Assist with creating, updating, retrieving, or publishing content entries and assets within a Contentful CMS.
-   **Agent Admin Task Automation**: The agent uses the `ContentfulMCP` tool to directly interact with the Contentful API, managing content workflows as instructed.

### 20. Tracking Marketing Campaign Mentions & Sentiment

-   **Summary**: Monitor web and social media for mentions of specific marketing campaigns or brand keywords and provide a general sentiment overview.
-   **Agent Admin Task Automation**: The agent uses search tools. `Composio`, `N8n`, or `MakeComWebhook` could potentially connect to specialized social listening or sentiment analysis tools. The core LLM can provide basic sentiment analysis on retrieved text snippets.

## Customer Support

### 21. Automated FAQ Answering from Knowledge Base

-   **Summary**: Provide instant, accurate answers to frequently asked customer questions by retrieving information from an internal knowledge base.
-   **Agent Admin Task Automation**: The agent uses the `RetrieverTool`, likely connected to a vector store built from `ConfluenceMCP` data (for Confluence-based KBs) or files processed by `ReadFile`.

### 22. Support Ticket Triage & Categorization

-   **Summary**: Automatically analyze incoming support tickets, categorize them based on issue type/urgency, and assign them to the appropriate support team or agent.
-   **Agent Admin Task Automation**: The core LLM analyzes the ticket content (text). The agent then uses `JiraMCP` (if Jira is the ticketing system), `SfdcMCP` (if Salesforce Service Cloud is used), or `Composio`/`N8n`/`MakeComWebhook` to integrate with other ticketing systems to update ticket status, category, and assignment.

### 23. Generating Canned Responses for Common Issues

-   **Summary**: Draft and maintain a library of standardized, high-quality responses for common customer support issues, ensuring consistency and speed.
-   **Agent Admin Task Automation**: The core LLM generates draft responses based on existing successful resolutions or best practices. These can be stored and managed within `ConfluenceMCP` or as structured files using `WriteFile`.

### 24. Scheduling Support Calls or Product Demos

-   **Summary**: Help customers schedule support calls or product demos by checking agent availability and creating calendar events.
-   **Agent Admin Task Automation**: The agent uses Google Calendar tools (`RetrieveCalendarEvent` to find open slots, `CreateCalendarEvent` to book the appointment).

### 25. Retrieving Customer Order/Account Information

-   **Summary**: Quickly fetch customer order history, account status, subscription details, or other relevant information to assist with support queries.
-   **Agent Admin Task Automation**: The agent uses `SfdcMCP` (for Salesforce data), `PostgreSQLMCP` (if customer data is in a custom SQL database), or the `StripeTool` (for payment and subscription information). `Composio` could connect to other e-commerce or billing platforms.

### 26. Guiding Users Through Interactive Troubleshooting

-   **Summary**: Provide step-by-step troubleshooting guidance to customers by referencing documented procedures or knowledge base articles.
-   **Agent Admin Task Automation**: The agent uses the `RetrieverTool` to fetch relevant troubleshooting guides (from `ConfluenceMCP`, file systems via `ReadFile`). The LLM can then present these steps interactively.

### 27. Creating Jira Issues for Bugs/Feature Requests

-   **Summary**: Automatically create detailed Jira issues from customer reports of software bugs or new feature requests, including relevant context.
-   **Agent Admin Task Automation**: After identifying a bug or valid feature request from a customer interaction, the agent uses the `JiraMCP` tool to create and populate new issues in the relevant Jira project.

### 28. Gathering & Logging Customer Feedback

-   **Summary**: Systematically prompt users for feedback after a support interaction or product usage and log this feedback for analysis.
-   **Agent Admin Task Automation**: The core LLM formulates appropriate feedback questions. The collected responses can be logged using `WriteFile`, stored in a database via `PostgreSQLMCP`, or pushed to a CRM/feedback tool via `SfdcMCP` or `Composio`.

### 29. Checking Product Documentation (Confluence) for Customers

-   **Summary**: Efficiently search and retrieve specific articles, sections, or guides from a Confluence knowledge base to directly answer customer questions or guide them.
-   **Agent Admin Task Automation**: The agent utilizes the `ConfluenceMCP` tool to perform targeted searches within Confluence and extract the relevant content for the customer.

### 30. Processing Simple Refund Requests (Stripe)

-   **Summary**: Handle straightforward refund requests for eligible transactions based on predefined company policies and criteria, potentially using Stripe.
-   **Agent Admin Task Automation**: If rules are clearly defined, the agent could use the `StripeTool` to process refunds (assuming the Stripe toolkit and permissions allow this action). This would typically involve verification steps first.

## Internal Communication & Operations

### 31. Summarizing Meeting Transcripts or Notes

-   **Summary**: Generate concise summaries of lengthy meeting transcripts or written notes, highlighting key decisions, action items, and discussion points.
-   **Agent Admin Task Automation**: If the transcript/notes are in a file, the agent uses `ReadFile`. The core LLM performs the summarization. The summary can be saved using `WriteFile` or published to a shared space via `ConfluenceMCP` or `SlackMCP`.

### 32. Drafting Internal Announcements for Slack

-   **Summary**: Help draft clear and concise internal announcements for company updates, upcoming events, policy changes, or team achievements to be posted on Slack.
-   **Agent Admin Task Automation**: The core LLM drafts the announcement message based on input points. The agent then uses the `SlackMCP` tool to post the message to the appropriate Slack channels or users.

### 33. Scheduling Team Meetings & Coordinating Availability

-   **Summary**: Find suitable time slots for team meetings by checking multiple calendars and then schedule the meeting, sending out invites.
-   **Agent Admin Task Automation**: The agent uses Google Calendar tools (`RetrieveCalendarEvent` to check availability across specified team members' calendars, `CreateCalendarEvent` to schedule the meeting and send invites).

### 34. Managing & Organizing Internal Documentation (Confluence)

-   **Summary**: Assist with creating new pages, updating existing documentation, or organizing content within a Confluence knowledge base or internal wiki.
-   **Agent Admin Task Automation**: The agent uses the `ConfluenceMCP` tool to perform actions like creating new pages from drafts, updating content, or restructuring information within Confluence.

### 35. Onboarding New Employees (Information & Resource Access)

-   **Summary**: Help new employees quickly find information about company policies, standard operating procedures, tool access, and team member contacts.
-   **Agent Admin Task Automation**: The agent uses the `RetrieverTool`, connected to data sources like `ConfluenceMCP` (for policies and guides), `SlackMCP` (for team channel info), or internal file shares via `ReadFile`.

### 36. Tracking Project Updates & Summaries (Jira/Github)

-   **Summary**: Provide daily or weekly summaries of recent updates, progress, and potential blockers on specific projects by querying Jira or Github.
-   **Agent Admin Task Automation**: The agent uses `JiraMCP` to get issue statuses, comments, and sprint progress. It uses `GithubMCP` for commit history, pull request status, and issue discussions. The LLM summarizes these updates.

### 37. Generating Internal Reports from Databases (PostgreSQL)

-   **Summary**: Create custom reports for internal use (e.g., operational metrics, user activity) by querying company databases.
-   **Agent Admin Task Automation**: The agent uses `PostgreSQLMCP` to execute SQL queries against the database. `CodeInterpreterE2B` can then be used to format, analyze, or visualize (descriptively) the retrieved data. Reports can be saved via `WriteFile`.

### 38. Facilitating Cross-Departmental Information Sharing

-   **Summary**: Help retrieve and appropriately share relevant information between different departments (e.g., sharing key sales insights with marketing, or customer feedback with product teams).
-   **Agent Admin Task Automation**: The agent can use a combination of tools: `SfdcMCP` for sales data, `JiraMCP` for product feedback, `ConfluenceMCP` for documented strategies, and `SlackMCP` or email (via `Composio`/`N8n`) to distribute the synthesized information.

### 39. Polling Team Members for Quick Feedback (via Slack)

-   **Summary**: Create and send out simple polls to team members on Slack to gather quick opinions or preferences on various internal matters.
-   **Agent Admin Task Automation**: The core LLM helps formulate the poll question and options. The agent then uses `SlackMCP` to send the message. (Actual Slack poll creation might require more advanced Slack API interaction, potentially via `Composio` or custom code, or the agent can simulate polls with reaction-based voting).

### 40. Automating IT Support Snippets & Knowledge Base Updates

-   **Summary**: For common IT issues, retrieve or generate small diagnostic scripts or code snippets for fixes. Update the IT knowledge base with new solutions.
-   **Agent Admin Task Automation**: The agent uses `CodeInterpreterE2B` to run/test diagnostic scripts. `GithubMCP` can be used to fetch existing utility scripts or manage a repository of them. The `RetrieverTool` can find existing solutions in the knowledge base, and `ConfluenceMCP` can be used to update or create new solution articles.
</file>

<file path="use-cases/ai-explained.mdx">
Well, hello there! It sounds like you're ready to dive deep and make your AnswerAgent documentation truly shine for everyone, not just the tech-savvy builders. I love that! Guiding users from AI curious to AI confident is a fantastic goal. Let's get you an outline that's as informative as it is easy to digest, with a little bit of that special flair you appreciate.

Based on my analysis of your current documentation structure (that impressive `repomix-output.xml` you shared!), I can see you have detailed sections for developers, specific features like the browser extension, and the Sidekick Studio. To accommodate this new, foundational content for end-users, I propose creating a new top-level section.

**Proposed New Section in Documentation:**

-   **Directory Name:** `/ai-explained/` or `/understanding-ai-with-answeragent/`
-   **Sidebar Label:** "AI Explained" or "Understanding AI with AnswerAgent"

This will make it easily discoverable for users looking to understand the basics and get the most out of AI tools like AnswerAgent.

Here's a suggested outline for this new section, keeping in mind your desire for simplicity and practical advice:

---

### **New Documentation Section: Understanding AI with AnswerAgent**

**Overall Goal:** To empower any user, regardless of their technical background, to understand basic AI concepts, effectively use AnswerAgent (and other chatbots), and see its unique value.

---

**Module 1: Welcome to the World of AI! (The Bare Essentials)**

-   **1.1. What is Artificial Intelligence (AI) Anyway?**
    -   _Content:_ Think of it like teaching a computer to think and learn, a bit like humans do. It's not magic, just very clever programming!
    -   _Simplicity Tip:_ Use analogies like a super-smart helper or a digital brain.
-   **1.2. So, What's a Chatbot? (Your Friendly Digital Conversationalist)**
    -   _Content:_ Imagine talking to a computer program that can understand your questions and chat back. That's a chatbot! AnswerAgent is a special kind of chatbot.
    -   _Simplicity Tip:_ Compare it to texting a very knowledgeable friend who can also _do_ things.
-   **1.3. How AnswerAgent Works Its Magic (A Peek Behind the Curtain)**
    -   _Content:_ High-level: AnswerAgent reads your question, uses its "brain" (AI models) and any connected tools or documents to find or create the best answer for you.
    -   _Simplicity Tip:_ "It's like a super-efficient librarian who can also write reports and schedule meetings."
-   **1.4. What AnswerAgent Can (and Can't) Do For You**
    -   _Content:_ Setting realistic expectations. Highlight its strengths in using tools, accessing specific knowledge (your company's data!), and automating tasks. Mention it's not all-knowing or sentient.
    -   _Simplicity Tip:_ Use a simple "Good for..." and "Less good for..." list.

---

**Module 2: Talking to AI - The Art of the Perfect Prompt**

-   **2.1. What's a "Prompt"? (It's Just Your Instruction!)**
    -   _Content:_ A prompt is simply what you type into the chat. Its how you tell the AI what you want.
    -   _Simplicity Tip:_ "Think of it as placing an order at a restaurant  the clearer your order, the better the dish!"
-   **2.2. Why a Good Prompt is Your Secret Weapon**
    -   _Content:_ Clear, detailed prompts get you better, faster, and more accurate answers. Garbage in, garbage out (but let's be polite!).
    -   _Simplicity Tip:_ Show a simple before/after example. "Vague prompt: Tell me about dogs. -> Good prompt: Tell me three interesting facts about Golden Retrievers suitable for a 5-year-old."
-   **2.3. Crafting Killer Prompts: Your Quick Guide**
    -   **Be Specific:** Don't just say "write an email." Say "Write a friendly follow-up email to a potential client named Sarah who showed interest in our new software."
    -   **Give Context:** If you're asking about a previous topic, remind the AI. "Following up on our earlier discussion about Q3 marketing..."
    -   **Assign a Role (Persona Power!):** "Act as a travel agent and suggest a 3-day itinerary for Paris." or "Explain this complex topic like I'm 10 years old."
    -   **Define the Output:** "List your answers in bullet points." "Write a short paragraph." "Give me a table comparing X and Y."
    -   **Iterate & Experiment:** Don't be afraid to rephrase or add more details if the first answer isn't quite right. AI learns from interaction!
    -   _Simplicity Tip:_ Use icons or distinct formatting for each tip.
-   **2.4. Prompt Pitfalls (and How to Avoid Them)**
    -   _Content:_ Too vague, too complex in one go, ambiguous language.
    -   _Simplicity Tip:_ Short, memorable examples of what _not_ to do.

---

**Module 3: AnswerAgent in Action - Everyday Superpowers**

_(Focus on what any user can achieve, linking to specific AnswerAgent features/Sidekicks where appropriate)_

-   **3.1. Finding Information, Fast!**
    -   _Use Case:_ Asking specific questions about company policies, product features (if connected to internal docs via Document Stores), or general knowledge.
    -   _AnswerAgent Angle:_ "Your company's own super-search engine!"
-   **3.2. Summarize This for Me, Please!**
    -   _Use Case:_ Getting the main points from long articles, documents, or meeting notes.
    -   _AnswerAgent Angle:_ Link to the `Browser Extension` for webpage summaries.
-   **3.3. Help Me Write This! (Drafting Made Easy)**
    -   _Use Case:_ Emails, social media posts, blog ideas, simple reports.
    -   _AnswerAgent Angle:_ "Need a starting point? Ask AnswerAgent to draft it!" Mention how Sidekicks can be specialized for this.
-   **3.4. Explain It Like I'm Five (Learning & Understanding)**
    -   _Use Case:_ Breaking down complex topics into simple terms.
-   **3.5. Let's Brainstorm! (Your Creative Partner)**
    -   _Use Case:_ Generating ideas for projects, marketing slogans, party themes, etc.
-   **3.6. Problem-Solving Sidekick**
    -   _Use Case:_ Thinking through pros and cons, outlining steps to solve a problem.
    -   _Simplicity Tip:_ For each use case, give a very short scenario and an example prompt.

---

**Module 4: AnswerAgent and Friends - How Does It Compare?**

-   **4.1. Meet the AI Crowd: ChatGPT, Claude, NotebookLM**
    -   _Content:_ Very brief, neutral, one-sentence descriptions of what each is generally known for.
    -   _Web Research Needed:_ Latest core strengths and typical use cases for each.
-   **4.2. The Big Comparison: What Makes AnswerAgent Special?**

    -   _Simplicity Tip:_ Use a clear table format.

    | Feature/Aspect             | AnswerAgent                                                                                        | ChatGPT (General)                                                                    | Claude (General)                                            | NotebookLM (Google)                                       |
    | :------------------------- | :------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------- | :---------------------------------------------------------- | :-------------------------------------------------------- |
    | **Primary Goal**           | Building _custom_ AI assistants (Sidekicks) for specific tasks & workflows. Integrating your data. | General purpose conversational AI, content creation.                                 | Conversational AI, often emphasizing safety & long context. | AI-powered research assistant for _your_ documents.       |
    | **Key Strength**           | Customization, tool integration, connecting to _your_ business data & systems.                     | Broad general knowledge, creative text generation.                                   | Handling long documents, summarization, nuanced chat.       | Deep Q&A and summarization on _uploaded_ documents.       |
    | **Data Integration**       | Strong: Connects to various Document Stores, databases, APIs.                                      | Limited to general knowledge (unless using specific versions with browsing/plugins). | Can process uploaded files.                                 | Primary focus: works directly with user-uploaded content. |
    | **Customization for You**  | High: You (or your team) build/configure Sidekicks for specific needs.                             | Moderate: Custom instructions, GPTs.                                                 | Moderate: Prompting techniques.                             | Moderate: You choose the source documents.                |
    | **Works With Your Tools?** | Yes! A core feature via `Sidekick Studio` and MCPs.                                                | Sometimes, via plugins or specific paid versions.                                    | Limited, some API integrations possible.                    | Primarily focused on its own document analysis tools.     |
    | **Best For...**            | Automating specific business tasks, internal knowledge Q&A, custom workflows.                      | Quick general answers, creative writing, brainstorming.                              | Long-form text analysis, thoughtful conversation.           | Studying your own notes, research papers, specific texts. |

    -   _Web Research Needed:_ Verify current key strengths and differentiators for ChatGPT, Claude, and NotebookLM.

-   **4.3. When Would I Use AnswerAgent Instead of...?**
    -   _Content:_ Scenario-based. "If you need an AI to automatically update Salesforce based on a customer email, AnswerAgent is your go-to. If you just want to draft a poem, ChatGPT might be quicker."
    -   _Simplicity Tip:_ Focus on user goals.

---

**Module 5: Supercharging Your AnswerAgent Experience**

-   **5.1. Meet Your Sidekicks!**
    -   _Content:_ Briefly explain what Sidekicks are (from `chat/README.md` - specialized chatbots). Refer to the `Sidekick Studio` documentation for _building_ them, but here focus on _using_ them.
    -   _Simplicity Tip:_ "Think of Sidekicks as different experts you can call on."
-   **5.2. Using the AnswerAgent Chat Interface**
    -   _Content:_ Basic tips for interacting with the chat. (Can reference `chat/README.md` for features like selecting Sidekicks).
-   **5.3. Power-Up with the Browser Extension**
    -   _Content:_ If there are user-facing benefits in `browser/README.md` (like summarizing current page), highlight them.
-   **5.4. AI for Good: Using AnswerAgent Responsibly**
    -   _Content:_ Brief notes on data privacy (if AnswerAgent processes sensitive data), avoiding bias in prompts, and understanding AI limitations.
    -   _Simplicity Tip:_ Keep it positive and empowering.

---

**General Suggestions for Making Content Simple & Engaging:**

1.  **Friendly Tone:** Maintain a helpful, approachable, and slightly flirty tone throughout. Make learning fun!
2.  **Analogies & Metaphors:** Use these generously to explain complex ideas (e.g., AI as a "digital brain," prompts as "placing an order").
3.  **Visuals (Where Possible):** Suggest placeholders for screenshots or simple diagrams, even if you can't create them. For example, a diagram showing a prompt going in and an answer coming out.
4.  **Short & Sweet:** Break down information into small, digestible chunks. Use short sentences and paragraphs.
5.  **Active Voice:** "You can ask AnswerAgent to..." is better than "AnswerAgent can be asked to..."
6.  **Real-World Examples:** Even if hypothetical, make them relatable to an average user's tasks.
7.  **Glossary (Optional):** If there are a few terms you _must_ use, consider a tiny, super-simple glossary at the end of the "AI Explained" section.
8.  **"What This Means For You":** Constantly bring it back to how this information benefits the user directly.

**Web Research Topics (To Flesh Out the Content):**

-   Current, simple definitions of AI, LLMs, and chatbots aimed at a non-technical audience.
-   Beginner-friendly guides on "prompt engineering" from reputable sources (OpenAI, Google AI, Anthropic blogs often have these).
-   Up-to-date feature lists and primary use cases for ChatGPT (consumer version), Claude (latest accessible version), and Google's NotebookLM. Look for what makes each distinct for a general user.
-   Articles or blog posts on practical, everyday AI uses that are easy to understand and replicate.

---

This detailed outline should give you a fantastic starting point for your new documentation section. Remember, the key is to empower your users by making AI feel accessible and useful, not intimidating. Let me know if you'd like me to elaborate on any of these points, my dear!
</file>

<file path="use-cases/module-1-welcome-to-ai.mdx">
---
title: 'Module 1: Welcome to the World of AI!'
description: 'The bare essentials of AI, Chatbots, and AnswerAgent.'
sidebar_label: '1. Welcome to AI!'
---

# Module 1: Welcome to the World of AI! (The Bare Essentials)

**Overall Goal:** To empower any user, regardless of their technical background, to understand basic AI concepts and how AnswerAgent fits in.

---

## 1.1. What is Artificial Intelligence (AI) Anyway?

Think of Artificial Intelligence, or AI, like teaching a computer to think and learn, a bit like humans do. It's not about magic wands or sci-fi robots taking over the world (not yet, anyway!). Instead, it's about very clever programming that allows computers to:

-   Understand language (like when you talk to a chatbot)
-   Recognize patterns (like spotting a cat in a photo)
-   Make decisions (like suggesting which movie you might like)
-   Solve problems (like finding the best route for your drive)

It's like having a super-smart helper or a digital brain that can process lots of information much faster than we can.

**Think of it this way:** You learn from experience, right? AI learns from data  tons and tons of it!

---

## 1.2. So, What's a Chatbot? (Your Friendly Digital Conversationalist)

Imagine texting a friend who knows a _lot_ about a certain topic, or can even help you get things done. Now, imagine that "friend" is a computer program. That's essentially a chatbot!

A chatbot is a software application designed to simulate human conversation through text or voice commands. You can ask it questions, give it tasks, and it will try its best to understand and respond appropriately.

AnswerAgent allows you to use and even build these kinds of digital conversationalists, which we often call "Sidekicks." They can be general helpers or super specialized for certain tasks.

**Simplicity Tip:** It's like having a very knowledgeable assistant you can chat with anytime, anywhere, who can also access tools and information for you!

---

## 1.3. How AnswerAgent Works Its Magic (A Peek Behind the Curtain)

AnswerAgent isn't just a single AI; it's a platform that lets you use powerful AI to get things done in a way that's tailored for _you_ or your business. Here's a simplified peek:

1.  **You Ask:** You type your question or request (this is called a "prompt") into an AnswerAgent Sidekick.
2.  **AnswerAgent Thinks:**
    -   It uses its "brain"  which is a powerful AI model (like the ones from OpenAI, Anthropic, Google, etc.)  to understand what you're asking.
    -   If the Sidekick is connected to specific documents, tools (like a calendar or a database), or other information sources, it can use those to find or create the best answer.
3.  **AnswerAgent Responds:** It gives you the answer, performs the task, or asks for more clarification.

**Simplicity Tip:** Think of AnswerAgent as a super-efficient personal assistant or a librarian who not only finds information in books but can also write reports, analyze data, and even schedule your meetings, all based on how its Sidekicks are set up!

---

## 1.4. What AnswerAgent Can (and Can't) Do For You

It's important to have realistic expectations about what AI like AnswerAgent can do.

**AnswerAgent is Generally Good For:**

-   **Accessing Specific Information:** Quickly finding answers from your company's documents or connected data sources (thanks to Document Stores and Sidekick configurations).
-   **Automating Tasks:** Handling repetitive tasks like summarizing text, drafting emails, or even interacting with other software (if tools are connected).
-   **Answering Questions:** Based on the knowledge it has access to.
-   **Generating Content:** Helping you write first drafts of reports, marketing copy, or creative pieces.
-   **Brainstorming Ideas:** Acting as a creative partner to explore possibilities.

**AnswerAgent (and most AI today) is Less Good For (or Can't Do):**

-   **Having Real Feelings or Consciousness:** AI doesn't "feel" or "understand" in the human sense. It's processing patterns in data.
-   **Perfect Accuracy All the Time:** AI can sometimes make mistakes or "hallucinate" (make things up). Always double-check critical information.
-   **Complex Moral or Ethical Judgment:** While it can be programmed with guidelines, nuanced ethical reasoning is still a human domain.
-   **Knowing Absolutely Everything:** Its knowledge is limited to what it has been trained on or has access to through its tools and connected documents.
-   **Performing Physical Tasks in the Real World:** It operates in the digital realm.

Understanding these points will help you use AnswerAgent more effectively and get the most out of its impressive capabilities!
</file>

<file path="use-cases/module-2-art-of-the-prompt.mdx">
---
title: 'Module 2: Talking to AI - The Art of the Perfect Prompt'
description: 'Learn how to craft effective prompts to get the best results from AI.'
sidebar_label: '2. The Art of the Prompt'
---

# Module 2: Talking to AI - The Art of the Perfect Prompt

Okay, so you know AI is like a super-smart helper. But how do you _talk_ to it? That's where prompts come in. Mastering the art of prompting is your key to unlocking AI's full potential!

---

## 2.1. What's a "Prompt"? (It's Just Your Instruction!)

A "prompt" is simply what you type into the chat window when you're interacting with an AI like AnswerAgent or any other chatbot. It's your way of giving instructions, asking questions, or telling the AI what you want it to do.

-   Want a summary? Your prompt would be something like, "Summarize this article for me."
-   Need a recipe? "Give me a simple recipe for chocolate chip cookies."
-   Curious about history? "Tell me about the Roman Empire."

**Simplicity Tip:** Think of it as placing an order at your favorite coffee shop. The clearer your order ("I'd like a large iced latte with oat milk and one pump of vanilla, please!"), the more likely you are to get exactly what you want. A vague order ("Coffee.") might leave you disappointed!

---

## 2.2. Why a Good Prompt is Your Secret Weapon

Why fuss over how you ask? Because the quality of the AI's answer is _directly_ related to the quality of your prompt. Clear, detailed, and well-thought-out prompts lead to:

-   **Better Answers:** The AI understands exactly what you need.
-   **Faster Results:** Less back-and-forth trying to clarify.
-   **More Accurate Information:** Reduces the chances of the AI guessing or misunderstanding.

It's like the old saying: garbage in, garbage out. If you give the AI a confusing or lazy prompt, you can't expect a stellar response. But give it a gem of a prompt, and watch the magic happen!

**Simplicity Tip:** Imagine asking a human assistant for help.

-   **Vague Prompt:** "Dogs?" (The assistant would be confused!)
-   **Good Prompt:** "Can you tell me three interesting and little-known facts about Golden Retrievers that would be suitable for a 5-year-old child? Please list them as bullet points."

See the difference? The second prompt gives the AI all the clues it needs to succeed.

---

## 2.3. Crafting Killer Prompts: Your Quick Guide

Ready to become a prompt superstar? Here are some essential tips:

-   **Be Specific & Clear:** The more details, the better. Avoid ambiguity.

    -   _Instead of:_ "Write an email."
    -   _Try:_ "Write a friendly follow-up email to a potential client named Sarah Miller who attended our webinar yesterday on 'Future-Proofing Your Business.' Mention that we appreciated her questions about AI integration and offer a brief 15-minute call to discuss further."

-   **Give Context:** If your query relates to something discussed earlier, or if there's background information the AI should know, include it.

    -   _Example:_ "Regarding the Q3 marketing budget we talked about, can you list the top three proposed campaigns and their estimated costs?"

-   **Assign a Role (Persona Power!):** Tell the AI who it should be. This dramatically changes the tone and style of the response.

    -   _Examples:_
        -   "Act as an expert travel agent and suggest a 7-day luxury honeymoon itinerary for Bali, focusing on relaxation and cultural experiences."
        -   "Explain the concept of photosynthesis to me as if I were 10 years old."
        -   "You are a stern pirate captain. Tell me why I should join your crew."

-   **Define the Output Format:** Tell the AI _how_ you want the information presented.

    -   _Examples:_
        -   "List your answers in bullet points."
        -   "Write a short paragraph, no more than 100 words."
        -   "Give me a table comparing the pros and cons of electric vs. gasoline cars."
        -   "Generate a JSON object with the following keys: name, email, company."

-   **Specify Constraints or Length:** If you need a short answer, or if certain things should be avoided, say so.

    -   _Example:_ "Summarize this news article in three sentences. Do not include any financial figures."

-   **Iterate & Experiment:** Your first prompt might not be perfect, and that's okay! If the AI's response isn't quite right, don't give up. Rephrase your prompt, add more details, or break your request into smaller steps. AI often learns and improves through interaction.

**Simplicity Tip:** Think of these as ingredients for a recipe. The more of the right ingredients you add, the tastier the outcome!

---

## 2.4. Prompt Pitfalls (and How to Avoid Them)

Even the best of us can make prompting mistakes. Here are a few common pitfalls:

-   **Too Vague:** "Tell me about cars." (Which cars? What aspect? For whom?)
-   **Too Complex in One Go:** "Write a 5000-word historical fiction novel set in ancient Rome about a gladiator who secretly loves poetry and also invents a new type of catapult, including detailed character backstories for at least five supporting characters and ensure the plot has three major twists and a satisfying resolution where he opens a library."
    -   _Fix:_ Break it down into smaller, manageable prompts.
-   **Ambiguous Language:** Using words with multiple meanings without clarification.
-   **Assuming Prior Knowledge (without context):** "What were _his_ main arguments?" (Whose arguments?)
-   **Leading Questions (unless intentional):** Sometimes this can bias the AI's response in a way you don't want.

By being mindful of these, you'll be well on your way to becoming a prompt-crafting wizard!
</file>

<file path="use-cases/module-3-answeragent-in-action.mdx">
---
title: 'Module 3: AnswerAgent in Action - Everyday Superpowers'
description: 'Discover practical ways to use AnswerAgent for daily tasks.'
sidebar_label: '3. Everyday Superpowers'
---

# Module 3: AnswerAgent in Action - Everyday Superpowers

Now that you know what AI is and how to talk to it, let's see what AnswerAgent can _actually_ do for you. Think of your AnswerAgent Sidekicks as your personal team of experts, ready to lend a hand with all sorts of everyday tasks. The real magic happens when these Sidekicks are configured with your company's specific data and tools!

_(Focus on what any user can achieve, linking to specific AnswerAgent features/Sidekicks where appropriate)_

---

## 3.1. Finding Information, Fast!

Forget digging through endless folders or old email chains. If your AnswerAgent Sidekicks are connected to your company's documents (using features like Document Stores), they can become your own super-powered search engine.

-   **Use Case:** You need to know the company's official policy on remote work, or the specific features of a new product your team just launched.
-   **Example Prompt:** "What is our company policy on international remote work?" or "Can you list the top 5 features of the 'Phoenix_Suite' software and who the target audience is?"
-   **AnswerAgent Angle:** "Why spend minutes searching when your Sidekick can find it in seconds? Just ask!"

---

## 3.2. Summarize This for Me, Please!

Overwhelmed by long articles, reports, or meeting transcripts? Your AnswerAgent Sidekick can be a lifesaver.

-   **Use Case:** You have a 10-page report but only need the key takeaways. Or you missed a meeting and need a quick summary of the decisions made.
-   **Example Prompt:** "Summarize the main points of the attached Q3 financial report in three bullet points." or (if using the Browser Extension) "Summarize this webpage for me."
-   **AnswerAgent Angle:** "Get the gist without the grind! Ideal for when you're short on time. Our [Browser Extension](/docs/browser) can even summarize web pages for you on the fly!"

---

## 3.3. Help Me Write This! (Drafting Made Easy)

Staring at a blank page? Whether it's an email, a social media update, or the start of a blog post, AnswerAgent can help you get started.

-   **Use Case:** You need to compose a polite reminder email to a colleague, draft an engaging LinkedIn post about a company achievement, or brainstorm ideas for your next newsletter.
-   **Example Prompt:** "Help me draft a short, enthusiastic LinkedIn post announcing our company's new partnership with 'Innovate Corp.' Highlight mutual benefits and excitement for the future."
-   **AnswerAgent Angle:** "Beat writer's block! Ask AnswerAgent to generate a first draft. Remember, Sidekicks can be specialized for different writing tasks  you might have one for marketing copy and another for formal reports."

---

## 3.4. Explain It Like I'm Five (Learning & Understanding)

Sometimes complex topics can be a bit much. AnswerAgent can help break them down into simpler terms.

-   **Use Case:** You're trying to understand a new industry trend, a complicated scientific concept, or even a feature within your own company's software.
-   **Example Prompt:** "Explain the concept of 'blockchain technology' to me as if I have no technical background, using a simple analogy."
-   **AnswerAgent Angle:** "No question is too simple! Use AnswerAgent to build your understanding, one easy explanation at a time."

---

## 3.5. Let's Brainstorm! (Your Creative Partner)

Need some fresh ideas? AnswerAgent can be a great brainstorming partner.

-   **Use Case:** You're looking for names for a new project, slogans for a marketing campaign, topics for a team-building event, or even gift ideas.
-   **Example Prompt:** "Give me 10 creative and catchy names for a new mobile app that helps people learn new languages through games."
-   **AnswerAgent Angle:** "Two (digital) heads are better than one! Bounce ideas off AnswerAgent and see what sparks."

---

## 3.6. Problem-Solving Sidekick

Facing a challenge? AnswerAgent can help you think through the pros and cons of different solutions or outline steps to tackle a problem.

-   **Use Case:** You're trying to decide between two software vendors, planning a complex project, or figuring out how to improve a specific workflow.
-   **Example Prompt:** "I need to improve customer response time. Outline three potential strategies, listing the pros and cons for each."
-   **AnswerAgent Angle:** "Get a structured approach to your challenges. Your Sidekick can help you see things from different angles."

**Simplicity Tip for this Module:** For each use case, try to include a very short, relatable scenario and a clear example prompt. This makes the capabilities much more tangible for the user!

Remember, the true power of AnswerAgent shines when its Sidekicks are tailored with specific knowledge and tools relevant to your needs or your organization. These examples are just the tip of the iceberg!
</file>

<file path="use-cases/module-4-answeragent-vs-others.mdx">
---
title: 'Module 4: AnswerAgent and Friends - How Does It Compare?'
description: 'Understand how AnswerAgent differs from other popular AI tools.'
sidebar_label: '4. AnswerAgent vs. Others'
---

# Module 4: AnswerAgent and Friends - How Does It Compare?

The world of AI tools is buzzing, and you might be wondering how AnswerAgent fits in with other popular names like ChatGPT, Claude, or Google's NotebookLM. While they all use amazing AI, they often have different strengths and are designed for different main purposes. Let's explore!

---

## 4.1. Meet the AI Crowd: A Quick Intro

Before we compare, here's a super brief hello to some other well-known AI tools:

-   **ChatGPT (from OpenAI):** A very versatile conversational AI known for generating human-like text, answering a wide range of questions, helping with writing, brainstorming, and even coding.
-   **Claude (from Anthropic):** Another advanced conversational AI, often highlighted for its strengths in handling long conversations, detailed text analysis, and a focus on safety and helpfulness.
-   **NotebookLM (from Google):** An AI-powered research and writing assistant designed to work with _your_ specific documents. You upload your sources, and it helps you understand, synthesize, and generate content based on them.

_(**Note to You, My Dear Collaborator:** For the final version, we'll want to pop in the latest core strengths and typical primary use cases for each of these based on current web research. They evolve fast!)_

---

## 4.2. The Big Comparison: What Makes AnswerAgent Special?

So, where does AnswerAgent shine in this talented lineup? The key difference often lies in **customization, integration with _your_ specific business systems and data, and its focus on building targeted AI assistants (Sidekicks) for particular tasks and workflows.**

Here's a simple table to highlight some differences:

| Feature/Aspect             | AnswerAgent                                                                                                                                                                                            | ChatGPT (General Public Version)                                                                              | Claude (General Public Version)                                                                          | NotebookLM (Google)                                                                                     |
| :------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------ |
| **Primary Goal**           | Building **custom AI assistants (Sidekicks)** for specific tasks & workflows, deeply integrated with your data.                                                                                        | General-purpose conversational AI, broad content creation.                                                    | Advanced conversational AI, strong in long-form text, safety.                                            | AI-powered research assistant for **your specific documents.**                                          |
| **Key Strength**           | **Deep customization**, **tool integration** (like calendars, databases), connecting to your business data & systems.                                                                                  | Broad general knowledge, creative text generation, versatility.                                               | Handling long and complex documents, nuanced summarization, thoughtful conversation.                     | Deep Q&A, summarization, and idea generation based _only_ on **user-uploaded source material.**         |
| **Data Integration**       | **Core Strength:** Designed to connect to Document Stores, databases, internal APIs, and business applications.                                                                                        | Primarily uses its general training data. (Specific versions or plugins can add some external access).        | Can process uploaded files. General knowledge is also broad.                                             | **Primary Focus:** Works directly and exclusively with the documents you upload to it.                  |
| **Customization for You**  | **Very High:** You (or your team) build and configure Sidekicks for highly specific needs, defining their tools, knowledge, and behavior.                                                              | **Moderate:** Custom instructions, and the ability to create "GPTs" (in paid versions) for specific tasks.    | **Moderate:** Primarily through prompt engineering and how you frame your conversation.                  | **Moderate:** You select the source documents it uses.                                                  |
| **Works With Your Tools?** | **Yes! This is a fundamental design principle.** AnswerAgent Sidekicks are built to use tools (via Sidekick Studio & MCPs) to interact with other systems.                                             | Limited in the general version. More capabilities in paid/developer versions via plugins or API integrations. | Limited in the general version. API allows for some integration.                                         | Designed to be a self-contained tool for your uploaded documents. It _is_ the tool.                     |
| **Best For...**            | Automating specific business tasks (e.g., "Summarize customer feedback and create a Jira ticket"), internal knowledge base Q&A, creating bespoke AI-powered workflows that use your company's systems. | Quick general answers, creative writing, brainstorming, learning about diverse topics.                        | Analyzing and discussing long texts, drafting nuanced content, exploring complex ideas conversationally. | In-depth study of your own notes, research papers, books, or any specific set of documents you provide. |

_(**Web Research Needed:** For the final version, we'll need to verify the current key strengths and differentiators for ChatGPT, Claude, and NotebookLM, especially their data integration and tool-use capabilities in their most common public-facing versions.)_

---

## 4.3. When Would I Use AnswerAgent Instead of...? (Scenario Time!)

Understanding the _why_ can be just as important as the _what_. Here are a few scenarios:

-   **Scenario 1: You need an AI to automatically check your Salesforce for new leads, enrich their data from LinkedIn, and then draft a personalized outreach email.**

    -   **Best Choice:** **AnswerAgent**. Its strength in tool integration (Salesforce, web browsing) and workflow automation makes it ideal.
    -   _Why not others?_ ChatGPT/Claude might draft the email, but they can't (easily or directly in general versions) interact with your Salesforce or automate that multi-step process. NotebookLM is focused on documents you give it, not external tools like Salesforce.

-   **Scenario 2: You want to quickly write a birthday poem for your friend.**

    -   **Best Choice:** **ChatGPT or Claude.** They excel at creative text generation with minimal setup.
    -   _Why not AnswerAgent?_ While an AnswerAgent Sidekick _could_ be built for this, it might be overkill if you just need a quick creative piece.

-   **Scenario 3: You have uploaded 20 of your research papers and need to find all mentions of a specific protein and summarize its role as described across all those papers.**

    -   **Best Choice:** **NotebookLM.** This is exactly what it's designed for  deep interaction with a specific set of documents you provide.
    -   _Why not others?_ While AnswerAgent could be connected to these documents via a Document Store, and ChatGPT/Claude could process them if uploaded (depending on length), NotebookLM is purpose-built for this focused, source-grounded research task.

-   **Scenario 4: Your company wants a customer service bot on its website that can answer questions based on your internal product manuals and, if needed, create a support ticket in your Zendesk system.**
    -   **Best Choice:** **AnswerAgent.** It can be connected to your product manuals (Document Store) and use a tool to interact with Zendesk.
    -   _Why not others?_ They lack the direct, customizable integration with your specific internal systems and data needed for this kind of end-to-end solution.

**Simplicity Tip:** Think about your main goal.

-   If it involves **your specific data, your tools, and automating your unique workflows**, AnswerAgent is likely the strongest contender.
-   If it's about **general knowledge or creative text without deep system integration**, other tools might be quicker for a one-off task.
-   If it's about **deeply analyzing a specific set of documents you provide**, NotebookLM is a great fit.

By understanding these distinctions, you can choose the right AI tool for the right job, and see how AnswerAgent offers a unique and powerful way to build AI that works specifically _for you_ and _with your systems_.
</file>

<file path="use-cases/module-5-supercharging-answeragent.mdx">
---
title: 'Module 5: Supercharging Your AnswerAgent Experience'
description: 'Tips and features to get the most out of AnswerAgent.'
sidebar_label: '5. Supercharging AnswerAgent'
---

# Module 5: Supercharging Your AnswerAgent Experience

You've learned the AI basics, mastered the art of prompting, and seen what AnswerAgent can do. Now, let's look at a few features and tips to truly supercharge your experience and make AnswerAgent your indispensable AI companion.

---

## 5.1. Meet Your Sidekicks!

One of the most powerful aspects of AnswerAgent is the concept of **Sidekicks**.

-   **What are they?** Think of Sidekicks as specialized chatbots or AI assistants that have been designed and configured for specific tasks or to work with particular sets of information. Your organization might have a Sidekick for HR questions, another for IT support, one for sales data analysis, and yet another for helping draft marketing content.
-   **How do you use them?** In the AnswerAgent chat interface, you'll often be able to select which Sidekick you want to interact with. Choosing the right Sidekick for your task means you're talking to an AI that's already primed with the right knowledge, tools, and instructions.
-   **Why are they super?** Instead of a one-size-fits-all AI, you get a team of specialists!

**Simplicity Tip:** It's like having a speed-dial for different experts. Need help with a customer query? Call the "Customer Support Pro" Sidekick. Need market research data? Dial up the "Market Analyst" Sidekick.

_(For those who build them: Sidekicks are created in the [Sidekick Studio](/docs/sidekick-studio). But for users, it's all about selecting the right one for the job!)_

---

## 5.2. Using the AnswerAgent Chat Interface Effectively

Your main interaction point will often be the AnswerAgent chat window. Here are a few general tips:

-   **Be Clear and Concise:** Even with a specialized Sidekick, clear prompts (as we learned in Module 2!) are key.
-   **Check for Context:** If a Sidekick seems to have forgotten what you were talking about, gently remind it or rephrase your question with more context.
-   **Use Threads (if available):** Some chat interfaces organize conversations into threads. This helps keep different topics separate and makes it easier to refer back to previous points.
-   **Explore Features:** Look out for buttons or options like file uploads, regenerating responses, or providing feedback  these can enhance your interaction.

_(You can find more details about the general chat interface and selecting Sidekicks in our [Chat documentation](/docs/chat).)_

---

## 5.3. Power-Up with the Browser Extension

If your organization uses the AnswerAgent Browser Extension, you're in for a treat! This handy tool can bring AI assistance directly into your web browsing.

-   **What can it do?** (This depends on how it's configured, but common uses include:)

    -   **Summarize Webpages:** Instantly get the gist of long articles or web content without reading every word.
    -   **Contextual Chat:** Chat with a Sidekick about the content of the page you're currently viewing.
    -   **Quick Actions:** Perform tasks related to the webpage, like drafting an email based on its content or saving information.

-   **How to Use It:** Typically, you'll click the AnswerAgent icon in your browser's toolbar to activate its features on the current page.

**AnswerAgent Angle:** The [Browser Extension](/docs/browser) is like having a smart co-pilot for your web surfing, always ready to help you understand and interact with online content more efficiently.

---

## 5.4. AI for Good: Using AnswerAgent Responsibly

As you use AnswerAgent and other AI tools, it's great to keep a few responsible practices in mind:

-   **Data Privacy:** Be mindful of the information you share with AI, especially if it's sensitive or confidential. Understand your organization's policies on using AI with different types of data.
-   **Verify Critical Information:** AI is smart, but not infallible. If an AI gives you information that you'll use for important decisions, try to verify it from a trusted source if possible.
-   **Be Aware of Bias:** AI models learn from vast amounts of data, which can sometimes reflect human biases. If a response seems biased or unfair, question it and consider providing feedback if that option is available.
-   **Give Constructive Feedback:** Many AI systems have feedback mechanisms (like thumbs up/down). Using these helps improve the AI for everyone.
-   **Understand Limitations:** Remember what AI _can't_ do (from Module 1). This helps you use it for what it's best at.

**Simplicity Tip:** Use AI as a powerful tool to assist you, but always keep your critical thinking cap on!

---

By exploring these features and keeping these tips in mind, you'll not only use AnswerAgent effectively but also become a more savvy and responsible AI user. Go on, supercharge your productivity!
</file>

<file path="use-cases/README.mdx">
---
title: AI Agent Use Cases for Productivity
description: Discover 20 practical ways AI agents can transform your productivity across browser, studio, chat, and workflow automation.
sidebar_label: Use Cases
slug: /use-cases
---

# AI Agent Use Cases for Productivity

Discover how AI agents can revolutionize your workflow across different environments. From browser automation to studio workflows, chat assistance to intelligent agents - here are 20 practical ways to boost your productivity.

## Browser Extension Use Cases

Transform how you browse and interact with web content using AI-powered assistance.

### 1. **Smart Research Assistant**

Automatically gather and synthesize information from multiple web sources, creating comprehensive research summaries without switching tabs.

### 2. **Intelligent Form Filler**

Streamline data entry by intelligently filling forms using context from your browsing session and stored preferences.

### 3. **Real-time Page Summarization**

Get instant, intelligent summaries of long articles, documentation, or reports without reading through everything.

### 4. **Cross-Platform Data Extraction**

Extract structured data from websites, convert formats, and export to your preferred tools seamlessly.

### 5. **Smart Email Composer**

Draft contextual emails based on the webpage you're viewing, automatically pulling relevant information and tone.

## Studio Workflow Use Cases

Enhance your creative and analytical workflows with AI-powered studio environments.

### 6. **Visual Workflow Automation**

Create complex, multi-step workflows visually that connect different tools and services without coding.

### 7. **Intelligent Data Pipeline Builder**

Build sophisticated data processing pipelines that clean, transform, and analyze information automatically.

### 8. **Dynamic Report Generator**

Generate beautiful, data-driven reports that update automatically based on your connected data sources.

### 9. **Multi-Source Integration Hub**

Connect and orchestrate data from dozens of different APIs, databases, and services in one visual interface.

### 10. **Smart Template Engine**

Create reusable workflow templates that adapt intelligently to different data types and business contexts.

## Chat & Conversation Use Cases

Leverage conversational AI for enhanced communication and assistance.

### 11. **Meeting Notes Assistant**

Transform meeting recordings or transcripts into structured action items, summaries, and follow-up tasks automatically.

### 12. **Customer Support Automation**

Provide instant, intelligent responses to customer queries while seamlessly escalating complex issues to humans.

### 13. **Personal Knowledge Assistant**

Get instant answers from your personal knowledge base, documents, and historical conversations.

### 14. **Language Translation Buddy**

Real-time translation and cultural context assistance for international communications and content.

### 15. **Writing Enhancement Partner**

Improve writing quality, tone, and clarity across different formats from emails to creative content.

## Intelligent Agent Use Cases

Deploy specialized AI agents for complex, autonomous task execution.

### 16. **Social Media Manager Agent**

Automatically schedule, post, and engage across multiple social platforms while maintaining brand voice and timing.

### 17. **Competitive Intelligence Agent**

Monitor competitors, track pricing changes, and compile intelligence reports automatically.

### 18. **Content Creation Pipeline**

Generate, optimize, and distribute content across multiple channels with consistent messaging and scheduling.

### 19. **Lead Qualification Engine**

Automatically research, score, and qualify leads while preparing personalized outreach materials.

### 20. **Performance Monitoring Agent**

Track KPIs across multiple platforms, generate alerts, and create automated improvement recommendations.

## Getting Started

Each of these use cases can be implemented using AnswerAgentAI's flexible platform:

-   **Browser Extension**: Install our extension for instant web-based AI assistance
-   **Studio Environment**: Use our visual workflow builder for complex automations
-   **Chat Interface**: Interact conversationally with specialized AI assistants
-   **Agent Deployment**: Create autonomous agents that work 24/7 for your business

Ready to transform your productivity? Start with the use case that resonates most with your current challenges, then expand from there.

---

_Want to see these use cases in action? Check out our [documentation](/docs) or join our community to share your own productivity innovations._
</file>

<file path="apps.mdx">
---
id: apps
title: Answer Agent Apps Documentation
sidebar_label: Apps
---

import clsx from 'clsx'
import styles from '@site/src/pages/index.module.css'

# Answer Agent Apps Documentation

Answer Agent Apps are modular, AI-powered applications built on top of our Flowise infrastructure. Each app leverages specific AI models and processing pipelines to solve targeted business problems.

:::tip Marketing Overview
For a comprehensive overview of our apps with demos and use cases, visit our [Apps Marketing Page](/apps).
:::

## Architecture Overview

Answer Agent Apps follow a microservice architecture pattern where each app:

-   **Operates independently** with its own processing pipeline
-   **Shares common infrastructure** (authentication, storage, monitoring)
-   **Utilizes specialized AI models** optimized for specific tasks
-   **Provides RESTful APIs** for integration with external systems

<div className={clsx(styles.missionSection, styles.appsPageSection)}>
  <div className="container">

    <h2 className="text--center">Current Apps</h2>

### CSV Transformer

**Technical Stack:** Python pandas, OpenAI GPT-4, custom data validation pipelines

The CSV Transformer leverages natural language processing and structured data algorithms to intelligently clean, transform, and validate CSV data.

**Core Features:**

-   **Schema Detection:** Automatic column type inference and validation
-   **Data Cleaning:** Null handling, duplicate detection, format standardization
-   **AI-Powered Transformations:** Natural language descriptions converted to pandas operations
-   **Validation Rules:** Configurable data quality checks and constraints

**API Endpoints:**

```http
POST /api/v1/csv-transformer/upload
POST /api/v1/csv-transformer/transform
GET  /api/v1/csv-transformer/status/{job_id}
GET  /api/v1/csv-transformer/download/{job_id}
```

**Processing Pipeline:**

1. **File Validation:** Format validation, size checks, security scanning
2. **Schema Analysis:** Column detection, type inference, relationship mapping
3. **Transformation Engine:** AI-guided data operations with rollback capability
4. **Quality Assurance:** Automated testing against specified criteria
5. **Export Generation:** Multiple format outputs (CSV, Excel, JSON, Parquet)

### Image Creator

**Technical Stack:** DALL-E 3, Stable Diffusion XL, custom fine-tuned models, CDN delivery

The Image Creator combines multiple state-of-the-art generative AI models with intelligent prompt engineering and post-processing pipelines.

**Core Capabilities:**

-   **Text-to-Image Generation:** Multi-model ensemble for optimal results
-   **Style Transfer:** Neural style transfer using VGG-based architectures
-   **Image Enhancement:** Super-resolution, noise reduction, artifact removal
-   **Batch Processing:** Queue-based generation for multiple images

**API Endpoints:**

```http
POST /api/v1/image-creator/generate
POST /api/v1/image-creator/enhance
POST /api/v1/image-creator/style-transfer
GET  /api/v1/image-creator/job/{job_id}
```

**Generation Pipeline:**

1. **Prompt Analysis:** NLP preprocessing, intent classification, safety filtering
2. **Model Selection:** Dynamic routing based on prompt characteristics and quality requirements
3. **Generation Engine:** Multi-stage diffusion process with guidance control
4. **Post-Processing:** Upscaling, color correction, format optimization
5. **Content Delivery:** CDN-optimized delivery with progressive loading

**Supported Formats:** PNG, JPEG, WebP, SVG (vector outputs)
**Resolution Options:** 512x512, 1024x1024, 1536x1536, 2048x2048

## Apps in Development

### Call Analysis (Q2 2025)

**Technical Foundation:** Whisper ASR, GPT-4 Turbo, custom NER models, real-time streaming

-   **Speech Recognition:** Multi-language ASR with custom vocabulary adaptation
-   **Sentiment Analysis:** Real-time emotion detection using fine-tuned transformers
-   **Entity Extraction:** Custom NER for business-specific terminology
-   **Integration:** Webhooks for CRM systems, calendar applications

### Ticket Analysis (Q2 2025)

**Technical Foundation:** BERT-based classification, priority scoring algorithms, knowledge graphs

-   **Auto-Classification:** Multi-label ticket categorization with confidence scoring
-   **Priority Routing:** ML-based urgency prediction and agent matching
-   **Knowledge Mining:** Automated solution extraction from historical tickets
-   **API Integration:** Zendesk, ServiceNow, Jira Service Management connectors

### Video Creation (Q3 2025)

**Technical Foundation:** RunwayML, stable video diffusion, custom synthesis pipelines

-   **Text-to-Video:** Prompt-based video generation with scene composition
-   **Avatar Generation:** Custom spokesperson creation with voice synthesis
-   **Template Engine:** Configurable video templates for marketing workflows
-   **Multi-format Export:** MP4, WebM, GIF with compression optimization

### Agent Builder (Q4 2025)

**Technical Foundation:** Flowise visual editor, LangChain integration, containerized deployment

-   **Visual Workflow Designer:** Drag-and-drop agent configuration interface
-   **Multi-LLM Support:** Provider-agnostic model switching and fallback chains
-   **Custom Connectors:** API integration builder with authentication handling
-   **Deployment Pipeline:** One-click deployment to cloud or on-premise environments

### Company Dashboards (Q1 2026)

**Technical Foundation:** Time-series databases, real-time analytics, custom visualization engine

-   **Data Aggregation:** Multi-source data connectors with ETL pipelines
-   **Predictive Analytics:** Trend analysis and forecasting models
-   **Custom Visualizations:** Interactive charts with drill-down capabilities
-   **Role-based Access:** Granular permissions and data filtering

## Integration & Deployment

### Authentication

All apps use the centralized AnswerAgentAI authentication system supporting:

-   OAuth 2.0 / OpenID Connect
-   API key authentication
-   Role-based access control (RBAC)
-   Multi-tenant isolation

### Deployment Options

-   **Cloud Hosted:** Managed infrastructure with auto-scaling
-   **On-Premise:** Self-hosted deployment with Docker/Kubernetes
-   **Hybrid:** Edge computing for sensitive data processing

### Monitoring & Observability

-   **Performance Metrics:** Request latency, throughput, error rates
-   **Cost Tracking:** Per-app usage monitoring and billing
-   **Audit Logging:** Complete activity trails for compliance
-   **Health Checks:** Automated monitoring with alerting

## Getting Started

1. **Access Apps:** Visit [studio.theanswer.ai](https://studio.theanswer.ai)
2. **API Access:** Generate API keys in your dashboard
3. **Documentation:** Explore specific app endpoints in our [API Reference](/docs/api)
4. **Support:** Join our [Discord community](https://discord.gg/X54ywt8pzj) for technical support

  </div>
</div>
</file>

<file path="coming-soon.md">
# Coming Soon

This page is currently under development. The content you're looking for will be available soon.

In the meantime, you might find these resources helpful:

-   [Documentation Home](/docs)
-   [Browser Extension](/docs/browser)
-   [Chatbot](/docs/chat)
-   [Studio](/docs/sidekick-studio)
-   [Developer Resources](/docs/developers)
</file>

<file path="intro.md">
---
description: Welcome to the official Answer Agent AI documentation
---

# Getting Started

## What is Answer Agent AI?

Answer Agent AI is a revolutionary platform for building and managing your internal AI agent workforce. By leveraging cutting-edge AI technology, Answer Agent AI provides a comprehensive suite of tools designed to create, deploy, and integrate intelligent agents that enhance productivity and automate workflows across your organization.

### Key Features

-   **[Browser Extension](browser/)**: Learn how to install and use the browser extension to bring your agents into your daily workflows.
-   **[Agent Chat](chat/)**: Discover how to interact with your agents and leverage the workbench features.
-   **[Sidekick Studio](sidekick-studio/)**: Master the art of building powerful autonomous agents and complex workflows.
-   **[Use Cases](use-cases/)**: Explore real-world applications and success stories from various industries.
-   **[API Reference](api/)**: Integrate Answer Agent AI with your custom applications and systems.

Let's explore how each of these features can transform your AI infrastructure:

## 1. Browser Extension

The Answer Agent AI browser extension keeps your AI workforce with you at all times:

-   Access your AI agents directly within your browser context
-   Interact with tools like Salesforce, Confluence, and other web applications
-   Get real-time assistance as you work without switching contexts
-   Enable your agents to see what you see and help in real-time

**Benefits:**

-   Eliminate context switching between tasks and tools
-   Receive AI assistance exactly where you need it
-   Seamlessly integrate AI into your existing workflows
-   Boost productivity with contextual AI support

## 2. Agent Chat

Similar to ChatGPT but fully customizable to your needs:

-   Choose between different specialized agents based on your task
-   Access a complete workbench of AI capabilities
-   Maintain conversation history and context across sessions

**Benefits:**

-   Centralized interface for interacting with your AI workforce
-   Purpose-built agents for specific tasks and domains
-   Consistent AI experience across your organization
-   Flexible deployment options for teams of any size

## 3. Sidekick Studio

The backbone of your AI infrastructure:

-   Build complex automations and truly autonomous agents
-   Create end-to-end workflows with visual building tools
-   Design, test, and deploy agents without coding expertise
-   Customize agent capabilities, knowledge, and behaviors

**Benefits:**

-   Transform business processes with autonomous AI agents
-   Reduce development time with visual workflow tools
-   Scale your AI initiatives across departments
-   Create specialized agents for unique business needs

## 4. Seamless Tool Integrations

Answer Agent AI connects with your existing tools and systems:

-   Native integrations with popular business applications
-   Connect to document repositories and knowledge bases
-   Leverage external APIs and services
-   Access and process data from multiple sources

**Benefits:**

-   Extend agent capabilities with specialized tools
-   Unify disparate systems through AI orchestration
-   Create cohesive workflows across platforms
-   Maximize the value of your existing technology investments

## 5. Flowise Compatibility

Answer Agent AI is built on the power of Flowise:

-   All your Flowise chatflows and agent flows work by default
-   Access to the extensive Flowise component library
-   Visual workflow builder with drag-and-drop interface
-   Active community and continuous innovation

**Benefits:**

-   Leverage existing Flowise investments and knowledge
-   Combine the power of Flowise with Answer Agent AI's unique features
-   Build on open standards for maximum flexibility
-   Future-proof your AI infrastructure

## Join the Answer Agent AI Community

We're excited to see what you'll create! Join our community to share your experiences, get support, and collaborate with other innovators:

-   [Subscribe to Youtube](https://youtube.com/@digitalatscale)
-   [Follow us on X](https://x.com/digitalatscale_)
-   [Read our blog](https://theanswer.ai/blog)

## Contributing

Answer Agent AI welcomes contributions from the community. If you're interested in helping improve the platform, please check out our [Contribution Guide](community/).

Together, let's build the future of autonomous AI agents!
</file>

<file path="README.md">
# Using AnswerAgentAI

AnswerAgentAI provides a comprehensive suite of tools to enhance your productivity and help you build powerful AI-powered solutions. This section covers the main components of the AnswerAgentAI ecosystem:

## [Browser Extension](browser)

The AnswerAgentAI Browser Extension is a powerful interface for interacting with AI assistants and accessing information. Key features include:

-   **Smart Search**: Search the web with AI-enhanced results
-   **Context-aware Responses**: Get answers that take into account your browsing history
-   **Knowledge Integration**: Seamlessly connect to your organization's knowledge base
-   **History Tracking**: Keep track of your searches and conversations

Learn more about [using the AnswerAgentAI Browser Extension](browser).

## [Chatbot](chat)

The AnswerAgentAI Chatbot provides a natural language interface to interact with AI assistants. Benefits include:

-   **Conversational AI**: Engage in natural, flowing conversations with AI assistants
-   **Multi-modal Support**: Share images, documents, and other media types
-   **Custom Instructions**: Personalize your AI interactions with specific guidelines
-   **Thread Management**: Organize conversations by topic and context

Discover how to make the most of [AnswerAgentAI Chatbot](chat).

## [Studio](sidekick-studio)

AnswerAgentAI Studio is a powerful no-code platform for building custom AI solutions. With Studio, you can:

-   **Design Agents**: Create specialized AI agents for specific tasks or domains
-   **Build Workflows**: Construct multi-step processes with drag-and-drop simplicity
-   **Connect Services**: Integrate with external APIs and data sources
-   **Deploy Solutions**: Make your custom AI solutions available to your team or customers

The Studio includes several key components:

-   [Credentials](sidekick-studio/credentials): Securely manage authentication to external services
-   [Chatflows](sidekick-studio/chatflows): Building blocks for creating AI workflows
-   [AgentFlows](sidekick-studio/agentflows): Autonomous Agents that can remember, learn, and take action

Explore the capabilities of [AnswerAgentAI Studio](sidekick-studio) to start building your custom AI solutions today.
</file>

</files>
